

# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ë° í™˜ê²½ ì •ë³´
# ==============================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, classification_report, confusion_matrix, precision_recall_curve
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier, callback
import xgboost as xgb
import random
from imblearn.over_sampling import SMOTE
import datetime
import joblib  # ëª¨ë¸ ì €ì¥ì— ì‚¬ìš©
seed = 190
random.seed(seed)
np.random.seed(seed)

# íŒŒì¼ ì €ì¥ì„ ìœ„í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ê²½ë¡œ ì„¤ì •
timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')


# 2. ë°ì´í„° ë¡œë”©
############################################### 3377 222

path = './Study25/_data/dacon/cancer/'

train_csv = pd.read_csv(path+'train.csv',index_col=0)
test_csv =pd.read_csv(path+'test.csv', index_col=0)
submission_csv = pd.read_csv(path+'sample_submission.csv',index_col=0)

##################### ê²°ì¸¡ì¹˜ í™•ì¸ ####################
#print(train_csv.info())
#print(train_csv.isnull().sum()) #ê²°ì¸¡ì¹˜ ì—†ìŒ
#print(test_csv.isna().sum()) #ê²°ì¸¡ì¹˜ ì—†ìŒ
print(train_csv.describe())

print(train_csv.shape, test_csv.shape)  # (87159, 15) (46204, 14)
##################### train_csvì™€ test_csv ë¶„ë¦¬ ###############
# 1. êµ¬ë¶„ìš© ì¹¼ëŸ¼ ì¶”ê°€
train_csv['is_train'] = 1
test_csv['is_train'] = 0
print(train_csv.shape, test_csv.shape) # (87159, 16) (46204, 15)

############## ë²”ì£¼í˜• ë°ì´í„° ë¼ë²¨ì¸ì½”ë”© í•˜ê¸° ############
combined = pd.concat([train_csv,test_csv], axis=0)
print(combined)
print(combined.shape) #[133363 rows x 16 columns]

aaa = pd.get_dummies(combined, columns=['Gender','Country','Race','Family_Background','Radiation_History',
                                        'Iodine_Deficiency','Smoke','Weight_Risk','Diabetes',
                                        ], drop_first=True,
                                        dtype=int,
                                        )
print(aaa) # [133363 rows x 27 columns]
print(aaa.columns)
############## ìƒê´€ê³„ìˆ˜ ì‹œì‘ ############
print(aaa.corr())

plt.figure(figsize=(5,12))
sns.heatmap(aaa.corr(), annot=True, cmap='coolwarm',fmt='.2f', cbar=True)
plt.title("Cancerì™€ì˜ ìƒê´€ê³„ìˆ˜ íˆíŠ¸ë§µ")
plt.show
############## ìƒê´€ê³„ìˆ˜ ë ############
# ì§€ìš¸ ì¹¼ëŸ¼ë“¤
# [1] ['Age','Nodule_Size','TSH_Result','T4_Result','T3_Result'] # ìƒê´€, í”¼ì²˜ì„í¬í„´ìŠ¤
# [2] ['Smoke_Smoker','Weight_Risk_obese','Diabetes_Yes'] # ìƒê´€ê´€ê³„ëŠ” ì—†ëŠ”ë° í”¼ì²˜ì„í¬í„´ìŠ¤

# ê·¸ë˜ì„œ ìš°ì„  [1]ì˜ ì»¬ëŸ¼ì„ 5ê°œ ì‚­ì œí•œë‹¤.
drop_features = ["T3_Result","T4_Result","TSH_Result","Nodule_Size","Age"]
aaa = aaa.drop(columns=drop_features)

# print(aaa) #[133363 rows x 22 columns]

# 4. ë‹¤ì‹œ ë¶„ë¦¬
train_csv = aaa[aaa['is_train'] == 1].drop(columns='is_train')
test_csv = aaa[aaa['is_train'] == 0].drop(columns='is_train')

print(train_csv.shape, test_csv.shape) # (87159, 21) (46204, 21)
print(train_csv.columns)
print(test_csv['Cancer']) # ì „ë¶€ NaN
test_csv = test_csv.drop(['Cancer'],axis=1)
print(test_csv.shape) # (46204, 20)

############### xì™€ y ë¶„ë¦¬ #############
x = train_csv.drop(['Cancer'],axis=1) 
print(x) # [87159 rows x 20 columns]

y = train_csv['Cancer']
print(y.shape) # (87159,)

print(np.unique(y, return_counts=True)) # (array([0., 1.]), array([76700, 10459], dtype=int64))


x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.1,random_state=334,stratify=y)

print(np.unique(y_train, return_counts=True))
print(np.unique(y_test, return_counts=True))

print(x_train.shape, x_test.shape) # (78443, 20) (8716, 20)
print(y_train.shape, y_test.shape) # (78443,) (8716,)

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state= 190)
x_train, y_train = smote.fit_resample(x_train, y_train)

print(x_train.shape, y_train.shape) # (138060, 20) (138060,)
print(pd.Series(y_train).value_counts())
# 0.0    69030
# 1.0    69030

print(np.unique(y_train, return_counts=True)) #(array([0., 1.]), array([69030, 69030], dtype=int64))
print(np.unique(y_test, return_counts=True))

#2. ëª¨ë¸êµ¬ì„±
early_stop = callback.EarlyStopping(
    rounds=200,
    metric_name='logloss',     # ëª¨ë‹ˆí„°ë§í•  í‰ê°€ ì§€í‘œ /eval_metric ê³¼ ë™ì¼í•˜ê²Œ
    # data_name='validation_0',  # eval_setì˜ ì²« ë²ˆì§¸ ë°ì´í„° ì…‹
    save_best=True             # ìµœì  ëª¨ë¸ ì €ì¥ ì˜µì…˜
    # AttributeError: 'best_iteration' is only defined when early stopping is used.
)

# 1. ModelCheckpointë¥¼ ìœ„í•œ ì €ì¥ ê²½ë¡œ ì„¤ì •
# mcp_save_path = f'xgb_model_mcp_{timestamp}.json'

# mcp = callback.ModelCheckpoint(
#     filepath=mcp_save_path,
#     monitor='validation_0-logloss',
#     save_best_only=True,
#     maximize=False,
#     verbose=1 # ì €ì¥ë  ë•Œ ë©”ì‹œì§€ ì¶œë ¥
# )

model = XGBClassifier(
    n_estimators=10000,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.7,
    colsample_bytree=0.7,
    gamma=2,
    reg_alpha=0.1,
    reg_lambda=1,
    random_state=seed,
    objective='binary:logistic',
    eval_metric='logloss',
    use_label_encoder=False,
    callbacks=[early_stop],
)

# í›ˆë ¨
model.fit(
    x_train, y_train,
    eval_set=[(x_test, y_test)],
    verbose=10,
)

# âœ… ì œì¼ ì¢‹ì€ ëª¨ë¸ ì €ì¥
model.save_model(f'xgb_best_model_{timestamp}.json')


# ğŸ¯ (1) ëª¨ë¸ ì „ì²´ ì €ì¥ (ì¶”ì²œ)
joblib.dump(model, f'xgb_model_{timestamp}.pkl')  # ì €ì¥
# ë¶ˆëŸ¬ì˜¤ê¸°: model = joblib.load('xgb_model.pkl')


# 4. í‰ê°€, ì˜ˆì¸¡
results = model.score(x_test,y_test)
(print('ìµœì¢…ì ìˆ˜ :', results))

y_predict = model.predict(x_test)
print(y_predict[:10])
y_predict = np.round(y_predict)
print(y_predict[:10])

########### submission.csv íŒŒì¼ ë§Œë“¤ê¸° // count ì»¬ëŸ¼ ê°’ë§Œ ë„£ì–´ì£¼ê¸°
y_submit = model.predict(test_csv)
y_submit = np.round(y_submit)

submission_csv['Cancer'] = y_submit
print(submission_csv[:10])

filename = f"/Users/jaewoo000/Desktop/IBM x RedHat/Study25/_save/dacon_cancer/submission_{timestamp}.csv"
submission_csv.to_csv(filename)
print(f"Submission saved to: {filename}")

f1 = f1_score(y_test, y_predict)
print("f1 :", f1)

# # ==============================================
# # 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ë° í™˜ê²½ ì •ë³´
# # ==============================================
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import os

# from sklearn.model_selection import train_test_split, StratifiedKFold
# from sklearn.metrics import f1_score
# from xgboost import XGBClassifier
# from xgboost.callback import EarlyStopping
# import random
# from imblearn.over_sampling import SMOTE
# import datetime
# import joblib
# import optuna

# # Seed ê³ ì •
# seed = 190
# random.seed(seed)
# np.random.seed(seed)

# # íƒ€ì„ìŠ¤íƒ¬í”„
# timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')

# # ==============================================
# # 2. ë°ì´í„° ë¡œë”©
# # ==============================================
# path = './_data/dacon/cancer/'

# train_csv = pd.read_csv(path+'train.csv', index_col=0)
# test_csv = pd.read_csv(path+'test.csv', index_col=0)
# submission_csv = pd.read_csv(path+'sample_submission.csv', index_col=0)

# # train/test ê²°í•© í›„ ì „ì²˜ë¦¬
# train_csv['is_train'] = 1
# test_csv['is_train'] = 0
# combined = pd.concat([train_csv, test_csv], axis=0)

# # ì›-í•« ì¸ì½”ë”©
# combined = pd.get_dummies(combined, columns=[
#     'Gender','Country','Race','Family_Background','Radiation_History',
#     'Iodine_Deficiency','Smoke','Weight_Risk','Diabetes'
# ], drop_first=True, dtype=int)

# # ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±°
# drop_features = ["T3_Result","T4_Result","TSH_Result","Nodule_Size","Age"]
# combined = combined.drop(columns=drop_features)

# # train, test ì¬ë¶„ë¦¬
# train_csv = combined[combined['is_train'] == 1].drop(columns='is_train')
# test_csv = combined[combined['is_train'] == 0].drop(columns=['is_train','Cancer'])

# x = train_csv.drop(['Cancer'], axis=1)
# y = train_csv['Cancer']

# # ==============================================
# # 3. Optuna objective í•¨ìˆ˜ ì •ì˜
# # ==============================================
# def objective(trial):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 100, 3000),
#         'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.2, log=True),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'subsample': trial.suggest_float('subsample', 0.5, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
#         'gamma': trial.suggest_float('gamma', 0, 5),
#         'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
#         'reg_lambda': trial.suggest_float('reg_lambda', 0, 5),
#         'random_state': seed,
#         'objective': 'binary:logistic',
#         'eval_metric': 'logloss',
#         'use_label_encoder': False,
#     }

#     f1_scores = []
#     skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)

#     for train_idx, valid_idx in skf.split(x, y):
#         x_train_fold, x_valid_fold = x.iloc[train_idx], x.iloc[valid_idx]
#         y_train_fold, y_valid_fold = y.iloc[train_idx], y.iloc[valid_idx]

#         # SMOTE ì ìš©
#         smote = SMOTE(random_state=seed)
#         x_train_fold, y_train_fold = smote.fit_resample(x_train_fold, y_train_fold)

#         model = XGBClassifier(**params)

#         # EarlyStopping ì½œë°± ì •ì˜
#         early_stop = EarlyStopping(rounds=200, save_best=True)

#         model.fit(
#             x_train_fold, y_train_fold,
#             eval_set=[(x_valid_fold, y_valid_fold)],
#             callbacks=[early_stop],
#             verbose=False
#         )

#         y_pred = model.predict(x_valid_fold)
#         f1 = f1_score(y_valid_fold, y_pred)
#         f1_scores.append(f1)

#     return np.mean(f1_scores)

# # ==============================================
# # 4. Optuna ìµœì í™” ì‹¤í–‰
# # ==============================================
# study = optuna.create_study(direction='maximize')
# study.optimize(objective, n_trials=30)

# print("Best Trial:")
# trial = study.best_trial
# print("  f1_score: {}".format(trial.value))
# print("  Params: ")
# for key, value in trial.params.items():
#     print("    {}: {}".format(key, value))

# # ==============================================
# # 5. ìµœì¢… ëª¨ë¸ í•™ìŠµ (best params)
# # ==============================================
# best_params = trial.params

# # train_test_split + SMOTE for final training
# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=334, stratify=y)
# smote = SMOTE(random_state=seed)
# x_train, y_train = smote.fit_resample(x_train, y_train)

# # EarlyStopping ì½œë°± ì •ì˜
# early_stop_final = EarlyStopping(rounds=200, save_best=True)

# model = XGBClassifier(**best_params)

# model.fit(
#     x_train, y_train,
#     eval_set=[(x_test, y_test)],
#     callbacks=[early_stop_final],
#     verbose=10,
# )

# # ==============================================
# # 6. ëª¨ë¸ ì €ì¥
# # ==============================================
# model.save_model(f'xgb_best_model_{timestamp}_optuna.json')
# joblib.dump(model, f'xgb_model_{timestamp}_optuna.pkl')

# # ==============================================
# # 7. í‰ê°€ ë° submission ìƒì„±
# # ==============================================
# results = model.score(x_test, y_test)
# print('ìµœì¢…ì ìˆ˜ :', results)

# y_predict = model.predict(x_test)
# f1 = f1_score(y_test, y_predict)
# print("f1 :", f1)

# # submission
# y_submit = model.predict(test_csv)
# submission_csv['Cancer'] = np.round(y_submit)

# filename = f"/Users/jaewoo000/Desktop/IBM x RedHat/Study25/_save/dacon_cancer/submission_{timestamp}_optuna.csv"
# submission_csv.to_csv(filename)
# print(f"Submission saved to: {filename}")

