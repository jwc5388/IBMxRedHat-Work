import tensorflow as tf
import numpy as np
from keras.models import Sequential
from keras.layers import Conv1D, Dense, Flatten, Dropout, BatchNormalization
from keras.datasets import mnist
from keras.optimizers import Adam

# âœ… ì‹œë“œ ê³ ì •
np.random.seed(42)
tf.random.set_seed(42)

# âœ… ë¶„ì‚° ì „ëµ ì„ ì–¸ (ë©€í‹° GPU í•™ìŠµ)
strategy = tf.distribute.MirroredStrategy()
print("ğŸ§  Number of GPUs:", strategy.num_replicas_in_sync)

# âœ… ë°ì´í„°ì…‹ ë¡œë”©
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# âœ… Conv1D ì…ë ¥ì„ ìœ„í•œ Reshape: (batch, steps, features)
# ì—¬ê¸°ì„  ê°€ë¡œ 28í”½ì…€ì„ ì‹œê°„ì¶•(steps)ìœ¼ë¡œ ê°„ì£¼, ê° ì¤„(ì„¸ë¡œ)ì´ feature
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

x_train = x_train.reshape(-1, 28, 28)  # shape = (60000, 28, 28)
x_test = x_test.reshape(-1, 28, 28)    # shape = (10000, 28, 28)

# âœ… ëª¨ë¸ ì •ì˜ (strategy.scope ë‚´ë¶€ì—ì„œ)
with strategy.scope():
    model = Sequential([
        Conv1D(64, kernel_size=3, activation='relu', padding='same', input_shape=(28, 28)),
        BatchNormalization(),
        Dropout(0.3),

        Conv1D(128, kernel_size=3, activation='relu', padding='same'),
        BatchNormalization(),
        Dropout(0.3),

        Flatten(),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(10, activation='softmax')  # MNISTëŠ” 10ê°œ í´ë˜ìŠ¤
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

# âœ… ëª¨ë¸ í›ˆë ¨
model.fit(
    x_train, y_train,
    epochs=5,
    batch_size=512,
    validation_data=(x_test, y_test)
)
