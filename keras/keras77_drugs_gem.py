import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import KFold # train_test_split ëŒ€ì‹  KFold ì‚¬ìš©
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import Ridge
import xgboost as xgb
import lightgbm as lgb
import catboost as cat
from rdkit import Chem
from rdkit.Chem import Descriptors, AllChem, MACCSkeys, rdMolDescriptors # 3D ê¸°ìˆ ì ì¶”ê°€
import warnings
import copy
from datetime import datetime
import random

# --- ê¸°ë³¸ ì„¤ì • ---
seed = 42
random.seed(seed)
np.random.seed(seed)
warnings.filterwarnings('ignore')

# --- ê²½ë¡œ ì„¤ì • ---
BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') \
    else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
path = os.path.join(BASE_PATH, '_data/dacon/drugs/')

# --- ë°ì´í„° ë¡œë“œ ---
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
submission = pd.read_csv(path + 'sample_submission.csv')


# ==============================================================================
# [ë³€ê²½] 1. íŠ¹ì§• ê³µí•™ ê³ ë„í™”: 2D/3D/ë¬¼ë¦¬í™”í•™ì  ì†ì„±ì„ ëª¨ë‘ í¬í•¨í•˜ëŠ” í•¨ìˆ˜ë¡œ ë³€ê²½
# ==============================================================================
def get_all_descriptors(smiles, seed=42):
    """
    SMILES ë¬¸ìì—´ë¡œë¶€í„° 2D, 3D, ë¬¼ë¦¬í™”í•™ì  ê¸°ìˆ ìë¥¼ ëª¨ë‘ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    """
    # 2D/3D ê¸°ìˆ ì ê³„ì‚° ì‹œ ì˜¤ë¥˜ê°€ ë§ìœ¼ë¯€ë¡œ robustí•œ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìˆ˜
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None # ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜ í›„ í›„ì²˜ë¦¬

        # 1. 200ê°œì˜ ë¬¼ë¦¬í™”í•™ì  ê¸°ìˆ ì
        all_descriptors = [desc[1](mol) for desc in Descriptors._descList]

        # 2. Morgan Fingerprint
        morgan = [int(b) for b in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048).ToBitString()]

        # 3. MACCS Keys
        maccs = [int(b) for b in MACCSkeys.GenMACCSKeys(mol).ToBitString()]

        # 4. 3D ê¸°ìˆ ì (WHIM)
        mol_3d = Chem.AddHs(mol)
        AllChem.EmbedMolecule(mol_3d, randomSeed=seed, maxAttempts=1000, useRandomCoords=True)
        try:
            AllChem.MMFFOptimizeMolecule(mol_3d)
            whim_descriptors = list(rdMolDescriptors.GetWHIM(mol_3d))
        except Exception: # 3D êµ¬ì¡° ìµœì í™” ì‹¤íŒ¨ ì‹œ
            whim_descriptors = [0] * 114

        return all_descriptors + morgan + maccs + whim_descriptors
    except:
        return None

print("ê³ ë„í™”ëœ íŠ¹ì§• ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
# íŠ¹ì§• ì¶”ì¶œ ì ìš© (ì˜¤ë¥˜ ë°œìƒ ì‹œ Noneìœ¼ë¡œ ì±„ìš°ê³ , ì´í›„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´)
train['features'] = train['Canonical_Smiles'].apply(get_all_descriptors)
test['features'] = test['Canonical_Smiles'].apply(get_all_descriptors)

# íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨í•œ í–‰(None) ì²˜ë¦¬
train.dropna(subset=['features'], inplace=True)
test['features'].fillna(pd.Series([np.zeros(len(train['features'].iloc[0]))] * len(test)), inplace=True)

x_train_raw = np.array(train['features'].tolist(), dtype=np.float32)
x_test_raw = np.array(test['features'].tolist(), dtype=np.float32)
y_train_full = train['Inhibition'].values

# np.inf, np.nan ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
x_train_raw = np.nan_to_num(x_train_raw, nan=0.0, posinf=0.0, neginf=0.0)
x_test_raw = np.nan_to_num(x_test_raw, nan=0.0, posinf=0.0, neginf=0.0)

print(f"íŠ¹ì§• ì¶”ì¶œ ì™„ë£Œ. í•™ìŠµ ë°ì´í„° í˜•íƒœ: {x_train_raw.shape}")
# ==============================================================================


# --- í‰ê°€ì§€í‘œ ---
def rmse(y_true, y_pred):
    return np.sqrt(mean_squared_error(y_true, y_pred))

def normalized_rmse(y_true, y_pred):
    y_true_range = np.max(y_true) - np.min(y_true)
    if y_true_range == 0: return 0
    return np.sqrt(mean_squared_error(y_true, y_pred)) / y_true_range

def pearson_correlation(y_true, y_pred):
    return np.clip(np.corrcoef(y_true, y_pred)[0, 1], 0, 1)

def competition_score(y_true, y_pred):
    # NRMSE ê³„ì‚° ì‹œ ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²½ìš° ë°©ì§€
    y_true_range = np.max(y_true) - np.min(y_true)
    if y_true_range == 0: return pearson_correlation(y_true, y_pred)
    
    nrmse = np.sqrt(mean_squared_error(y_true, y_pred)) / y_true_range
    return 0.5 * (1 - min(nrmse, 1)) + 0.5 * pearson_correlation(y_true, y_pred)

# --- ëª¨ë¸ ì •ì˜ ---
# n_estimators(iterations)ëŠ” early_stoppingìœ¼ë¡œ ì¡°ì ˆë˜ë¯€ë¡œ ì¶©ë¶„íˆ í° ê°’ìœ¼ë¡œ ì„¤ì •
base_models = {
    "LightGBM": lgb.LGBMRegressor(n_estimators=2000, random_state=seed, n_jobs=-1,
                                 learning_rate=0.05, num_leaves=31, max_depth=7, verbose = -1),
    "XGBoost": xgb.XGBRegressor(n_estimators=2000, random_state=seed, tree_method='gpu_hist',
                                learning_rate=0.05, max_depth=6, subsample=0.8, colsample_bytree=0.8),
    "CatBoost": cat.CatBoostRegressor(iterations=2000, random_seed=seed, verbose=0,
                                      learning_rate=0.05, depth=6, l2_leaf_reg=3)
}


# ==============================================================================
# [ë³€ê²½] 2. K-Fold êµì°¨ ê²€ì¦ ë° ëª¨ë¸ ë¸”ë Œë”©ìœ¼ë¡œ í•™ìŠµ/ì˜ˆì¸¡ ë¡œì§ ë³€ê²½
# ==============================================================================
# K-Fold ì„¤ì •
kf = KFold(n_splits=5, shuffle=True, random_state=seed)

# Out-of-Fold(OOF) ì˜ˆì¸¡ê°’ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ì„ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™”
oof_preds = np.zeros((x_train_raw.shape[0], len(base_models)))
test_preds = np.zeros((x_test_raw.shape[0], len(base_models)))

# ëª¨ë¸ë³„ë¡œ K-Fold í•™ìŠµ ë° ì˜ˆì¸¡ ìˆ˜í–‰
for model_idx, (name, model) in enumerate(base_models.items()):
    print(f"\n===== {name} ëª¨ë¸ K-Fold í•™ìŠµ ë° ì˜ˆì¸¡ ì‹œì‘ =====")
    
    # ê° Foldì˜ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ì„ ì„ì‹œ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
    fold_test_preds = []
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(x_train_raw)):
        print(f"  Fold {fold+1}/{kf.n_splits} í•™ìŠµ ì¤‘...")
        
        # 1. ë°ì´í„° ë¶„í• 
        x_train_fold, x_val_fold = x_train_raw[train_idx], x_train_raw[val_idx]
        y_train_fold, y_val_fold = y_train_full[train_idx], y_train_full[val_idx]

        # 2. ìŠ¤ì¼€ì¼ë§ (ë§¤ Fold ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        scaler = StandardScaler()
        x_train_fold = scaler.fit_transform(x_train_fold)
        x_val_fold = scaler.transform(x_val_fold)
        x_test_scaled = scaler.transform(x_test_raw) # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼í•˜ê²Œ ë³€í™˜
        
        # 3. ëª¨ë¸ í•™ìŠµ (Early Stopping ì ìš©)
        m = copy.deepcopy(model)
        if name == "LightGBM":
            m.fit(x_train_fold, y_train_fold, eval_set=[(x_val_fold, y_val_fold)],
                  callbacks=[lgb.early_stopping(100, verbose=False)])
        elif name == "XGBoost":
             m.fit(x_train_fold, y_train_fold,
            eval_set=[(x_val_fold, y_val_fold)],
             early_stopping_rounds=100,
            verbose=False)
        elif name == "CatBoost":
             m.fit(x_train_fold, y_train_fold, eval_set=[(x_val_fold, y_val_fold)],
                  early_stopping_rounds=100, verbose=0)
        else:
            m.fit(x_train_fold, y_train_fold)
            
        # 4. ì˜ˆì¸¡
        # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡ (OOF)
        oof_preds[val_idx, model_idx] = m.predict(x_val_fold)
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡
        fold_test_preds.append(m.predict(x_test_scaled))
        
    # 5. í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ê°’ í‰ê· 
    # ê° Foldì—ì„œ ì˜ˆì¸¡í•œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ì˜ í‰ê· ì„ í•´ë‹¹ ëª¨ë¸ì˜ ìµœì¢… ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì‚¬ìš©
    test_preds[:, model_idx] = np.mean(fold_test_preds, axis=0)
    
    # ëª¨ë¸ë³„ OOF ì ìˆ˜ ì¶œë ¥
    model_oof_score = competition_score(y_train_full, oof_preds[:, model_idx])
    print(f"â†’ {name} ëª¨ë¸ OOF Score: {model_oof_score:.4f}")
    

# ==============================================================================
# [ì¶”ê°€] ëª¨ë¸ë³„ ìƒì„¸ OOF ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
# ==============================================================================
print("\nğŸ“Š ëª¨ë¸ë³„ ìµœì¢… OOF ì„±ëŠ¥ ë¹„êµ")
print("-" * 80)
for model_idx, name in enumerate(base_models.keys()):
    y_true = y_train_full
    y_pred = oof_preds[:, model_idx]
    
    _rmse = rmse(y_true, y_pred)
    _nrmse = normalized_rmse(y_true, y_pred)
    _pearson = pearson_correlation(y_true, y_pred)
    _score = competition_score(y_true, y_pred)
    
    print(f"{name:20} | RMSE: {_rmse:.4f} | NRMSE: {_nrmse:.4f} | Pearson: {_pearson:.4f} | Score: {_score:.4f}")
print("-" * 80)
# ==============================================================================

# --- ìµœì¢… ì˜ˆì¸¡: ëª¨ë¸ ì˜ˆì¸¡ê°’ ë¸”ë Œë”© ---
# ê° ëª¨ë¸ì˜ OOF ì ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš© (ì„±ëŠ¥ ì¢‹ì€ ëª¨ë¸ì— ë” í° ê°€ì¤‘ì¹˜)
oof_scores = np.array([competition_score(y_train_full, oof_preds[:, i]) for i in range(len(base_models))])
weights = oof_scores / oof_scores.sum()

print("\n--- ëª¨ë¸ë³„ ê°€ì¤‘ì¹˜ ---")
for name, weight in zip(base_models.keys(), weights):
    print(f"{name}: {weight:.4f}")

# ê°€ì¤‘ í‰ê· ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ê°’ ìƒì„±
final_predictions = np.sum(test_preds * weights, axis=1)

# Blended OOF Score ê³„ì‚°
blended_oof_preds = np.sum(oof_preds * weights, axis=1)
blended_score = competition_score(y_train_full, blended_oof_preds)
print(f"\nğŸ† ìµœì¢… Blended OOF Score: {blended_score:.4f}")
# ==============================================================================


# --- ì œì¶œ íŒŒì¼ ìƒì„± ---
submission['Inhibition'] = final_predictions
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f"submission_{timestamp}_score_{blended_score:.4f}.csv"
submission.to_csv(os.path.join(path, filename), index=False)
print(f"\nì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {filename}")


# --- ê²°ê³¼ ì‹œê°í™” (OOF ì˜ˆì¸¡ê°’ í™œìš©) ---
plt.figure(figsize=(10, 6))
plt.scatter(y_train_full, blended_oof_preds, alpha=0.5)
plt.plot([min(y_train_full), max(y_train_full)], [min(y_train_full), max(y_train_full)], 'r--')
plt.xlabel('ì‹¤ì œê°’ (Inhibition)')
plt.ylabel('OOF ì˜ˆì¸¡ê°’ (Blended)')
plt.title(f'Blended Model OOF Performance (Score: {blended_score:.4f})')
plt.grid(True)
plt.tight_layout()
plt.show()