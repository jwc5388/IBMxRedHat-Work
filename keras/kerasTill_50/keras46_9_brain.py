import numpy as np
import pandas as pd
import time
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.losses import BinaryCrossentropy
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from datetime import datetime


# path_train = '/workspace/TensorJae/Study25/_data/brain/train/'
# path_test = '/workspace/TensorJae/Study25/_data/brain/test'
np_path = '/workspace/TensorJae/Study25/_save/save_brain_npy/'
# âœ… ê²½ë¡œ ì„¤ì •
# np_path = '/workspace/TensorJae/Study25/_save/save_npy/'
path_test = '/workspace/TensorJae/Study25/_data/kaggle/cat_dog/test2/'
# sample_path = '/workspace/TensorJae/Study25/_data/kaggle/cat_dog/sample_submission.csv'
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
save_path = f'/workspace/TensorJae/Study25/_save/submission_brain_{timestamp}.csv'

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° + ì •ê·œí™”
start = time.time()
x = np.load(np_path + 'keras46_x_train.npy') / 255.0
y = np.load(np_path + 'keras46_y_train.npy')
test = np.load(np_path + 'keras_x_test.npy') / 255.0
x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=42)
print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")
print(x_train.shape, y_train.shape)

# âœ… ëª¨ë¸ êµ¬ì„±
model = Sequential()
model.add(Conv2D(128, (3,3), padding='same', activation='relu', input_shape=(150,150,1)))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Dropout(0.2))  # ğŸ”¹ ì•½í•˜ê²Œ

model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))

model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))  # ğŸ”¹ ì œì¼ ê¹Šì€ ì¸µë§Œ ì‚´ì§ ê°•í•˜ê²Œ

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# âœ… ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ì½œë°±
loss_fn = BinaryCrossentropy(label_smoothing=0.02)
optimizer = Adam(learning_rate=0.001)

model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])

es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, verbose=1)

# âœ… ëª¨ë¸ í•™ìŠµ
start = time.time()
hist = model.fit(
    x_train, y_train,
    epochs=200,
    batch_size=32,
    validation_data=(x_val, y_val),
    callbacks=[es, lr],
    verbose=1
)
print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")

# âœ… í‰ê°€
loss, acc = model.evaluate(x_train, y_train, verbose=0)
print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„° í‰ê°€ - Loss: {loss:.4f}, Accuracy: {acc:.4f}")

# âœ… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
# pred = model.predict(test, verbose=1)
# pred_prob = pred.reshape(-1)

# submission = pd.read_csv(sample_path)
# submission['label'] = pred_prob
# submission.to_csv(save_path, index=False)
# print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}")


# ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: 15.01ì´ˆ
# ìµœì¢… í›ˆë ¨ ë°ì´í„° í‰ê°€ - Loss: 0.6866, Accuracy: 0.4844