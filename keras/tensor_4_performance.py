# === General ===
import numpy as np
import time

# === Keras / TensorFlow ===
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv1D, Conv2D, LSTM, BatchNormalization
from keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf

# === Sklearn Metrics ===
from sklearn.metrics import (
    mean_squared_error,         # For RMSE
    r2_score,                   # For RÂ²
    roc_auc_score,              # For binary AUC
    f1_score,                   # For F1 score
    classification_report       # For classification report
)

# === Optional Scalers & Splits ===
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler


model = Sequential()

# âœ… íšŒê·€ìš© ëª¨ë¸ í‰ê°€
loss, mae = model.evaluate(x_test, y_test, verbose=0)
y_pred = model.predict(x_test)

# === R2 & RMSE ===
from sklearn.metrics import r2_score, mean_squared_error
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# === ì¶œë ¥ ===
print(f'âœ… Loss (MSE): {loss:.4f}')
print(f'âœ… MAE       : {mae:.4f}')
print(f'âœ… RMSE      : {rmse:.4f}')
print(f'âœ… RÂ² Score  : {r2:.4f}')
print(f'â±ï¸ Training Time: {end - start:.2f} sec')




# âœ… ì´ì§„ ë¶„ë¥˜ìš© ëª¨ë¸ í‰ê°€
loss, acc, auc = model.evaluate(x_test, y_test, verbose=0)
y_pred_prob = model.predict(x_test)

# === sklearn AUC ===
from sklearn.metrics import roc_auc_score
roc_auc = roc_auc_score(y_test, y_pred_prob)

# === ì¶œë ¥ ===
print(f'âœ… Binary Crossentropy Loss : {loss:.4f}')
print(f'âœ… Accuracy                 : {acc:.4f}')
print(f'âœ… Keras AUC                : {auc:.4f}')
print(f'âœ… Sklearn ROC AUC          : {roc_auc:.4f}')
print(f'â±ï¸ Training Time            : {end - start:.2f} sec')




# âœ… ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ í‰ê°€
loss, acc = model.evaluate(x_test, y_test, verbose=0)
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)  # if one-hot encoded

from sklearn.metrics import f1_score, classification_report
f1 = f1_score(y_true, y_pred_classes, average='weighted')

print(f'âœ… Categorical Crossentropy Loss: {loss:.4f}')
print(f'âœ… Accuracy                     : {acc:.4f}')
print(f'âœ… Weighted F1 Score            : {f1:.4f}')
print(f'â±ï¸ Training Time                : {end - start:.2f} sec')

# Optional
print("\nğŸ“„ Classification Report:\n")
print(classification_report(y_true, y_pred_classes))


# âœ… ë‹¤ì¤‘ ì¶œë ¥ íšŒê·€ ì˜ˆì¸¡
y_pred = model.predict(x_test)

from sklearn.metrics import mean_squared_error, r2_score
r2 = r2_score(y_test, y_pred, multioutput='uniform_average')  # ë˜ëŠ” 'raw_values'
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f'âœ… RMSE (avg across outputs): {rmse:.4f}')
print(f'âœ… RÂ² Score (avg)           : {r2:.4f}')


# âœ… Summary Table: Metrics by Task Type

# Task Type	Output Layer Example	Loss Function	Metrics (Keras & sklearn)
# íšŒê·€ (Regression)	Dense(1)	'mse', 'mae'	RMSE, MAE, RÂ²
# ì´ì§„ ë¶„ë¥˜ (Binary)	Dense(1, activation='sigmoid')	'binary_crossentropy'	Accuracy, AUC, ROC AUC
# ë‹¤ì¤‘ ë¶„ë¥˜ (Multiclass)	Dense(n_classes, activation='softmax')	'categorical_crossentropy'	Accuracy, F1-score, Top-k
# ë‹¤ì¤‘ ì¶œë ¥ íšŒê·€	Dense(n_outputs)	'mse', 'mae'	Per-output RMSE, RÂ²


# âœ… ê·¸ë˜ì„œ ì´ëŸ° ê²½ìš°ì—” ë‹¤ìŒì„ í•¨ê»˜ ë´ì•¼ ì •í™•í•´ìš”:

# Metric	ì‚¬ìš© ëª©ì 	ì¶”ì²œ ìƒí™©
# accuracy	ì „ì²´ ì˜ˆì¸¡ ì¤‘ ë§ì¶˜ ë¹„ìœ¨	í´ë˜ìŠ¤ ê· í˜•ì¼ ë•Œ OK
# AUC	í´ë˜ìŠ¤ êµ¬ë¶„ ëŠ¥ë ¥ (ê³¡ì„  ë©´ì )	ë¶ˆê· í˜• ë°ì´í„°ì¼ ë•Œ ê°•ë ¥ ì¶”ì²œ âœ…
# precision	ì˜ˆì¸¡í•œ ê¸ì • ì¤‘ ì‹¤ì œë¡œ ê¸ì •	False Positive ì¤„ì´ê³  ì‹¶ì„ ë•Œ
# recall	ì‹¤ì œ ê¸ì • ì¤‘ ëª¨ë¸ì´ ë§ì¶˜ ë¹„ìœ¨	False Negative ì¤„ì´ê³  ì‹¶ì„ ë•Œ
# f1_score	precision & recall ì¡°í™” í‰ê· 	ë‘˜ ë‹¤ ì¤‘ìš”í•  ë•Œ
# roc_auc_score	AUC ê³„ì‚° (sklearnìš©)	í‰ê°€ ê¸°ì¤€ìœ¼ë¡œ ìì£¼ ì“°ì„