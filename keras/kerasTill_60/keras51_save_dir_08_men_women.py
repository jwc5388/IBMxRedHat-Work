# import numpy as np
# import pandas as pd
# import time
# import tensorflow as tf
# from keras.models import Sequential
# from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense
# from keras.callbacks import EarlyStopping, ReduceLROnPlateau
# from keras.losses import BinaryCrossentropy
# from keras.optimizers import Adam
# from sklearn.model_selection import train_test_split
# from datetime import datetime


# # path_train = '/workspace/TensorJae/Study25/_data/brain/train/'
# # path_test = '/workspace/TensorJae/Study25/_data/brain/test'
# np_path = '/workspace/TensorJae/Study25/_save/keras46_men_women/'
# # âœ… ê²½ë¡œ ì„¤ì •
# # np_path = '/workspace/TensorJae/Study25/_save/save_npy/'
# # path_test = '/workspace/TensorJae/Study25/_data/kaggle/cat_dog/test2/'
# # sample_path = '/workspace/TensorJae/Study25/_data/kaggle/cat_dog/sample_submission.csv'
# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
# # save_path = f'/workspace/TensorJae/Study25/_save/submission_horse_{timestamp}.csv'

# # âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° + ì •ê·œí™”
# start = time.time()
# x = np.load(np_path + 'keras46_mw_x_train.npy') / 255.0
# y = np.load(np_path + 'keras46_mw_y_train.npy')
# # test = np.load(np_path + 'keras_x_test.npy') / 255.0
# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)
# print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")
# print(x_train.shape, y_train.shape)

# # âœ… ëª¨ë¸ êµ¬ì„±
# model = Sequential()
# model.add(Conv2D(128, (3,3), padding='same', activation='relu', input_shape=(150,150,3)))
# model.add(BatchNormalization())
# model.add(MaxPooling2D())
# model.add(Dropout(0.2))  # ğŸ”¹ ì•½í•˜ê²Œ

# model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
# model.add(BatchNormalization())
# model.add(Dropout(0.2))

# model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
# model.add(BatchNormalization())
# model.add(Dropout(0.2))  # ğŸ”¹ ì œì¼ ê¹Šì€ ì¸µë§Œ ì‚´ì§ ê°•í•˜ê²Œ

# model.add(Flatten())
# model.add(Dense(256, activation='relu'))
# model.add(Dropout(0.2))
# model.add(Dense(1, activation='sigmoid'))

# # âœ… ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ì½œë°±
# loss_fn = BinaryCrossentropy(label_smoothing=0.02)
# optimizer = Adam(learning_rate=0.001)

# model.compile(optimizer=optimizer, loss=loss_fn, metrics=['acc'])

# es = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)
# lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, verbose=1)

# # âœ… ëª¨ë¸ í•™ìŠµ
# start = time.time()
# hist = model.fit(
#     x_train, y_train,
#     epochs=200,
#     batch_size=32,
#     validation_data=(x_test, y_test),
#     callbacks=[es, lr],
#     verbose=1
# )
# print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")

# # âœ… í‰ê°€
# loss, acc = model.evaluate(x_train, y_train, verbose=0)
# print(f"ìµœì¢… í›ˆë ¨ ë°ì´í„° í‰ê°€ - Loss: {loss:.4f}, Accuracy: {acc:.4f}")

# # âœ… ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
# # pred = model.predict(test, verbose=1)
# # pred_prob = pred.reshape(-1)

# # submission = pd.read_csv(sample_path)
# # submission['label'] = pred_prob
# # submission.to_csv(save_path, index=False)
# # print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}")


# # Epoch 25: early stopping
# # ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: 177.36ì´ˆ
# # ìµœì¢… í›ˆë ¨ ë°ì´í„° í‰ê°€ - Loss: 0.5717, Accuracy: 0.6929

# # ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: 266.41ì´ˆ
# # ìµœì¢… í›ˆë ¨ ë°ì´í„° í‰ê°€ - Loss: 0.6832, Accuracy: 0.5723


from re import X
import numpy as np
import pandas as pd
import time
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense
from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from keras.losses import BinaryCrossentropy
from keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from datetime import datetime
from keras.preprocessing.image import ImageDataGenerator

# âœ… ê²½ë¡œ ë° ì„¤ì •
np_path = '/workspace/TensorJae/Study25/_save/keras46_men_women/'

# âœ… ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° + ì •ê·œí™”
start = time.time()
x = np.load(np_path + 'keras46_mw3_x_train.npy')
y = np.load(np_path + 'keras46_mw3_y_train.npy')
# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)
print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")
# print(x_train.shape, y_train.shape)

unique, counts = np.unique(y, return_counts=True)
print(dict(zip(unique, counts)))  


idx_class_0 = np.where(y == 0.0)[0]
idx_class_1 = np.where(y == 1.0)[0]


# 2. ì¦ê°•ì´ í•„ìš”í•œ ê°œìˆ˜ ê³„ì‚°
target_count = 2000
augment_0 = target_count - len(idx_class_0)  # 591ê°œ í•„ìš”
augment_1 = target_count - len(idx_class_1)  # 100ê°œ í•„ìš”

# 3. ì¦ê°•í•  ìƒ˜í”Œ ëœë¤ ì„ íƒ
randidx_0 = np.random.choice(idx_class_0, augment_0, replace=True)
randidx_1 = np.random.choice(idx_class_1, augment_1, replace=True)



x_aug_0 = x[randidx_0].copy()
y_aug_0 = y[randidx_0].copy()

x_aug_1 = x[randidx_1].copy()
y_aug_1 = y[randidx_1].copy()

# âœ… ë°ì´í„° ì¦ê°• ì„¤ì •
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# 5. ì¦ê°• ì‹¤í–‰
x_aug_0 = datagen.flow(x_aug_0, y_aug_0, batch_size=augment_0, shuffle=False, save_to_dir= '/workspace/TensorJae/Study25/_data/_save_img/08_men_women/0/', ).next()[0]
x_aug_1 = datagen.flow(x_aug_1, y_aug_1, batch_size=augment_1, shuffle=False,  save_to_dir= '/workspace/TensorJae/Study25/_data/_save_img/08_men_women/1/').next()[0]


exit()
# 6. í†µí•©
x = np.concatenate([x, x_aug_0, x_aug_1])
y = np.concatenate([y, y_aug_0, y_aug_1])

# 7. ê²°ê³¼ í™•ì¸
unique, counts = np.unique(y, return_counts=True)
print("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:", dict(zip(unique, counts)))



print(f"âœ… ì¦ê°• í›„: {x.shape}, {y.shape}") 


# âœ… í›ˆë ¨/ê²€ì¦ ë¶„í• 
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)
print(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ â±ï¸ {round(time.time() - start, 2)}ì´ˆ")
print(f"x_train: {x_train.shape}, y_train: {y_train.shape}")




# âœ… ëª¨ë¸ êµ¬ì„± (ì„±ëŠ¥ ê°œì„ ì•ˆ)
model = Sequential()
model.add(Conv2D(64, (3,3), padding='same', activation='relu', input_shape=(300,300,3)))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
model.add(BatchNormalization())
# model.add(Conv2D(256, (3,3), padding='same', activation='relu'))
# model.add(BatchNormalization())
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(256, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(1, activation='sigmoid'))

# âœ… ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì €, ì½œë°±
# loss_fn = BinaryCrossentropy(label_smoothing=0.02)
optimizer = Adam(learning_rate=0.0001) # âœ… í•™ìŠµë¥  ì¡°ì •

model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['acc'])

es = EarlyStopping(monitor='val_acc', mode='max', patience=10, restore_best_weights=True, verbose=1) # val_acc ëª¨ë‹ˆí„°ë§
# lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.5, patience=10, verbose=1) # val_acc ëª¨ë‹ˆí„°ë§

mcp_path = '/workspace/TensorJae/Study25/_save/keras46_men_women/'

mcp = ModelCheckpoint(
    monitor = 'val_loss',
    mode = 'auto',
    save_best_only= True,
    filepath = mcp_path + 'keras46_gender.h5'
)
# âœ… ëª¨ë¸ í•™ìŠµ (ë°ì´í„° ì¦ê°• ì ìš©)
start = time.time()
hist = model.fit(
    x_train, y_train,
    steps_per_epoch=len(x_train) // 32,
    epochs=200,
    validation_data=(x_test, y_test),
    callbacks=[es, mcp],
    verbose=1
)
print(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: {round(time.time() - start, 2)}ì´ˆ")

# âœ… ìµœì¢… í‰ê°€
loss, acc = model.evaluate(x_test, y_test, verbose=0)
print(f"ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ - Loss: {loss:.4f}, Accuracy: {acc:.4f}")


# 1. x_testì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
predictions_proba = model.predict(x_test)  # í™•ë¥ ê°’ (e.g., 0.98, 0.12, ...)
result_class = np.round(predictions_proba)     # ìµœì¢… í´ë˜ìŠ¤ (1 ë˜ëŠ” 0)

# 2. ì˜ˆì¸¡ ê²°ê³¼(result_class)ì™€ ì‹¤ì œ ì •ë‹µ(y_test)ì„ ë¹„êµ
print("\n===== ìƒìœ„ 20ê°œ ìƒ˜í”Œ ë¹„êµ =====")

# ë³´ê¸° ì¢‹ê²Œ 1ì°¨ì› ë°°ì—´ë¡œ ë§Œë“­ë‹ˆë‹¤.
print("ì˜ˆì¸¡ ê²°ê³¼ (Predictions):")
print(result_class[:20].flatten().astype(int)) 

print("\nì‹¤ì œ ì •ë‹µ (True Labels):")
print(y_test[:20].astype(int))

# result = np.round(model.predict(x_test))
# print(result[:20])
# print(y_train[:20])

# ëª¨ë¸ í•™ìŠµ ì™„ë£Œ. ì†Œìš”ì‹œê°„: 1554.95ì´ˆ
# ìµœì¢… í…ŒìŠ¤íŠ¸ ë°ì´í„° í‰ê°€ - Loss: 1.0410, Accuracy: 0.7125



# ===== ìƒìœ„ 20ê°œ ìƒ˜í”Œ ë¹„êµ =====
# ì˜ˆì¸¡ ê²°ê³¼ (Predictions):
# [1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 1]

# ì‹¤ì œ ì •ë‹µ (True Labels):
# [1 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 1 1]