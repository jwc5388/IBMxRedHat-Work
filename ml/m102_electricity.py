# -*- coding: utf-8 -*-
# Time-based CV + Fold-safe ì§‘ê³„ + ì£¼ê°„ ë™/ë¡¤ë§ + ìµœê·¼ ê°€ì¤‘ì¹˜ + ì‹œê°„ê¸°ë°˜ Optuna + ë³´ì •/í´ë¦¬í•‘
# seed bagging ì œì™¸

import os
import json
import random
import warnings
import datetime
import numpy as np
import pandas as pd
import optuna

from sklearn.linear_model import Ridge
from joblib import Parallel, delayed
from optuna.samplers import TPESampler

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf

warnings.filterwarnings("ignore")

# ==============================
# 0) ì‹œë“œ / ê²½ë¡œ
# ==============================
seed = 222
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
    else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
train = pd.read_csv(os.path.join(path, "train.csv"))
test = pd.read_csv(os.path.join(path, "test.csv"))
samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
bi = buildinginfo.copy() if 'buildinginfo' in globals() else buildinginfo.copy()

if bi is not None:
    for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
        if col in bi.columns:
            bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
    bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
    bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

    keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
    for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
        if c in bi.columns: keep_cols.append(c)
    bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

    train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['hour']      = df['ì¼ì‹œ'].dt.hour
    df['day']       = df['ì¼ì‹œ'].dt.day
    df['month']     = df['ì¼ì‹œ'].dt.month
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
    df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
    df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
    df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
    df['sin_month'] = np.sin(2*np.pi*df['month']/12)
    df['cos_month'] = np.cos(2*np.pi*df['month']/12)
    df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
    df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['DI'] = 0.0
    # ì›”ë§/ì›”ì´ˆ í”Œë˜ê·¸(ì‘ì€ ì´ë“)
    df['is_month_end'] = (df['day'] >= 28).astype(int)
    df['is_month_start'] = (df['day'] <= 3).astype(int)
    return df

train = add_time_features_kor(train)
test  = add_time_features_kor(test)

# === 2) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
            df[c] = 0.0
        return df
    grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
    stats = grp.agg(day_max_temperature='max',
                    day_mean_temperature='mean',
                    day_min_temperature='min').reset_index()
    df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
    return df

train = add_daily_temp_stats_kor(train)
test  = add_daily_temp_stats_kor(test)

# === 3) CDH / THI / WCT (train/test ë™ì¼)
def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        df['CDH'] = 0.0
        return df
    def _cdh_1d(x):
        cs = np.cumsum(x - 26)
        return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
    parts = []
    for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
        arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
        cdh = _cdh_1d(arr)
        parts.append(pd.Series(cdh, index=g.index))
    df['CDH'] = pd.concat(parts).sort_index()
    return df

def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['THI'] = 0.0
    if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
        df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
    else:
        df['WCT'] = 0.0
    return df

train = add_CDH_kor(train)
test  = add_CDH_kor(test)
train = add_THI_WCT_kor(train)
test  = add_THI_WCT_kor(test)

# === 4) ì‹œê°„ ê¸°ë°˜ ì£¼ê°„ ë™/ë¡¤ë§: trainì€ shiftë¡œ, testëŠ” trainì˜ ê³¼ê±°ì‹œì  ë§¤í•‘
def add_weekly_lags_train(df_train: pd.DataFrame):
    df = df_train.sort_values(['ê±´ë¬¼ë²ˆí˜¸','ì¼ì‹œ']).copy()
    df['lag168'] = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].shift(168)
    df['rm168']  = (df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
                    .transform(lambda s: s.shift(1).rolling(168, min_periods=24).mean()))
    med = df.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].transform('median')
    df['lag168'] = df['lag168'].fillna(med)
    df['rm168']  = df['rm168'].fillna(med)
    return df

def add_weekly_lags_test(df_train_with_lag: pd.DataFrame, df_test: pd.DataFrame):
    # testì˜ (ê±´ë¬¼, t)ì˜ lag168 = trainì˜ (ê±´ë¬¼, t-168h) ê°’ì„ lookup
    train_map = df_train_with_lag.set_index(['ê±´ë¬¼ë²ˆí˜¸','ì¼ì‹œ'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
    # ë¡¤ë§ í‰ê· ë„ t-1 ì‹œì  ë¡¤ë§ì„ trainì—ì„œ lookup (ë³´ìˆ˜ì )
    train_rm_map = df_train_with_lag.set_index(['ê±´ë¬¼ë²ˆí˜¸','ì¼ì‹œ'])['rm168'] if 'rm168' in df_train_with_lag.columns else None

    df_te = df_test.copy()
    t_minus_168 = df_te['ì¼ì‹œ'] - pd.to_timedelta(168, unit='h')
    key = list(zip(df_te['ê±´ë¬¼ë²ˆí˜¸'].values, t_minus_168.values))
    lag_vals = train_map.reindex(key).values
    df_te['lag168'] = lag_vals

    if train_rm_map is not None:
        # rm168ì€ t-1 ì‹œì ì˜ ë¡¤ë§í‰ê· ì„ ëŒ€ì‹  ì‚¬ìš©(ì—„ê²©í•œ ëˆ„ìˆ˜ ë°©ì§€)
        t_minus_1 = df_te['ì¼ì‹œ'] - pd.to_timedelta(1, unit='h')
        key_rm = list(zip(df_te['ê±´ë¬¼ë²ˆí˜¸'].values, t_minus_1.values))
        rm_vals = train_rm_map.reindex(key_rm).values
        df_te['rm168'] = rm_vals

    # ê²°ì¸¡ ëŒ€ì²´: ê±´ë¬¼ë³„ train ì¤‘ì•™ê°’
    med_map = df_train_with_lag.groupby('ê±´ë¬¼ë²ˆí˜¸')['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].median().to_dict()
    df_te['lag168'] = df_te.apply(lambda r: med_map.get(r['ê±´ë¬¼ë²ˆí˜¸'], 0.0) if pd.isna(r['lag168']) else r['lag168'], axis=1)
    if 'rm168' in df_te.columns:
        df_te['rm168'] = df_te.apply(lambda r: med_map.get(r['ê±´ë¬¼ë²ˆí˜¸'], 0.0) if pd.isna(r['rm168']) else r['rm168'], axis=1)
    else:
        df_te['rm168'] = df_te['lag168']

    return df_te

train = add_weekly_lags_train(train)
test  = add_weekly_lags_test(train, test)

# === 5) íŠ¹ì§• í›„ë³´
feature_candidates = [
    'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
    'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
    'hour','day','month','dayofweek','is_weekend','is_working_hours',
    'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
    'DI','day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
    'CDH','THI','WCT',
    'is_month_end','is_month_start',
    # fold-safeë¡œ ëŒ€ì²´í•  ì˜ˆì •ì¸ ìë¦¬í‘œì‹œì(ë¨¸ì§€ ì‹œ ìƒì„±)
    # 'day_hour_mean','day_hour_std','expected_solar'
    # ì£¼ê°„ ë™/ë¡¤ë§
    'lag168','rm168'
]

# ê³µí†µ ì»¬ëŸ¼ë§Œ ì¼ë‹¨ í™•ë³´(ì´í›„ fold-safe ë¨¸ì§€ë¡œ ì¶”ê°€ ì»¬ëŸ¼ ë“¤ì–´ì˜´)
features_base = [c for c in feature_candidates if c in train.columns and c in test.columns]

target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
assert target in train.columns, "trainì— targetì´ ì—†ìŠµë‹ˆë‹¤."

# ------------------------------
# SMAPE (log domain ì…ë ¥)
# ------------------------------
def smape_exp(y_true_log, y_pred_log):
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# ------------------------------
# ì‹œê°„ê¸°ë°˜ í´ë“œ ìƒì„±(ìë™): ëì—ì„œ 9ì¼ì„ 3ë“±ë¶„(ê° 3ì¼ ê²€ì¦), ê·¸ ì´ì „ ì „ì²´ë¥¼ ê° foldì˜ trainìœ¼ë¡œ ì‚¬ìš©
# ------------------------------
def make_time_folds(df_b):
    df_b = df_b.sort_values('ì¼ì‹œ')
    days = np.sort(df_b['ì¼ì‹œ'].dt.floor('D').unique())
    if len(days) < 15:
        # ë°ì´í„°ê°€ ì§§ìœ¼ë©´ ë§ˆì§€ë§‰ 6ì¼ì„ 3ê°œ ì°½(2ì¼ì”©)ìœ¼ë¡œ
        val_len = 2
        total_val_days = min(6, len(days))
    else:
        val_len = 3
        total_val_days = min(9, len(days))
    last_days = days[-total_val_days:]
    folds = []
    for i in range(0, total_val_days, val_len):
        val_days = last_days[i:i+val_len]
        if len(val_days) < val_len: break
        vs, ve = val_days[0], val_days[-1] + np.timedelta64(23, 'h')  # í¬í•¨ ëì‹œê°
        tr_mask = df_b['ì¼ì‹œ'] < vs
        va_mask = (df_b['ì¼ì‹œ'] >= vs) & (df_b['ì¼ì‹œ'] <= ve)
        tr_idx = df_b.index[tr_mask].to_numpy()
        va_idx = df_b.index[va_mask].to_numpy()
        if len(tr_idx) and len(va_idx):
            folds.append((tr_idx, va_idx))
    return folds

# ------------------------------
# Fold-safe ì§‘ê³„: ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ & expected_solar
# ------------------------------
def fold_safe_hour_stats(df_train_fold):
    pm = (df_train_fold
          .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])[target]
          .agg(['mean','std']).reset_index()
          .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
    return pm

def fold_safe_expected_solar(df_train_fold):
    if 'ì¼ì‚¬(MJ/m2)' in df_train_fold.columns:
        solar_proxy = (df_train_fold.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
                       .mean().reset_index()
                       .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'}))
    else:
        solar_proxy = pd.DataFrame({'month':[], 'hour':[], 'expected_solar':[]})
    return solar_proxy

def merge_fold_safe(df_any, pm, solar_proxy):
    d = df_any.copy()
    # ê¸°ì¡´ ì—´ ì œê±° í›„ ì¬ë¨¸ì§€(ì¶©ëŒ ë°©ì§€)
    d = d.drop(columns=['day_hour_mean','day_hour_std','expected_solar'], errors='ignore')
    d = d.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
    if solar_proxy.shape[0] > 0:
        d = d.merge(solar_proxy, on=['month','hour'], how='left')
    else:
        d['expected_solar'] = 0.0
    d[['day_hour_mean','day_hour_std','expected_solar']] = d[['day_hour_mean','day_hour_std','expected_solar']].fillna(0.0)
    return d

# ------------------------------
# ìµœê·¼ ê°€ì¤‘ì¹˜
# ------------------------------
def make_recency_weight(df_b, idx, decay=0.005):
    tmax = df_b['ì¼ì‹œ'].max()
    dt_h = (tmax - df_b.loc[idx, 'ì¼ì‹œ']).dt.total_seconds() / 3600.0
    return np.exp(-decay * dt_h)

# ------------------------------
# ì˜ˆì¸¡ í›„ ë³´ì • & í´ë¦¬í•‘
# ------------------------------
def build_calib_table(df_va, pred_log):
    df = df_va.copy()
    df['pred'] = np.expm1(pred_log)
    df['ratio'] = (df[target] + 1e-6) / (df['pred'] + 1e-6)
    tab = df.groupby(['hour','dayofweek'])['ratio'].median().to_dict()
    return tab

def apply_calib(df_te, pred_log, tab):
    base = np.expm1(pred_log)
    ratios = df_te.apply(lambda r: tab.get((r['hour'], r['dayofweek']), 1.0), axis=1).values
    return base * ratios

def per_building_clip(df_b, y_pred):
    lo = df_b[target].quantile(0.01) if target in df_b.columns else 0.0
    hi = df_b[target].quantile(0.99) if target in df_b.columns else np.percentile(y_pred, 99)
    return np.clip(y_pred, max(0.0, lo*0.5), hi*1.2)

# ------------------------------
# ì‹œê°„ê¸°ë°˜ Optuna íŠœë‹(ê±´ë¬¼ë³„ 1íšŒ ì €ì¥/ë¡œë“œ)
# ------------------------------
def tune_xgb_timecv(trial, df_b, features, seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth":    trial.suggest_int("max_depth", 3, 8),
        "learning_rate":trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "subsample":    trial.suggest_float("subsample", 0.7, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
        "objective": "reg:squarederror",
        "eval_metric": "mae",
        "random_state": seed,
        "early_stopping_rounds": 50,
    }
    folds = make_time_folds(df_b)
    smapes = []
    for tr_idx, va_idx in folds:
        df_tr = df_b.loc[tr_idx].copy()
        df_va = df_b.loc[va_idx].copy()
        pm = fold_safe_hour_stats(df_tr)
        solar_proxy = fold_safe_expected_solar(df_tr)
        df_tr = merge_fold_safe(df_tr, pm, solar_proxy)
        df_va = merge_fold_safe(df_va, pm, solar_proxy)

        X_tr = df_tr[features].values
        y_tr = np.log1p(df_tr[target].values.astype(float))
        X_va = df_va[features].values
        y_va = np.log1p(df_va[target].values.astype(float))

        w_tr = make_recency_weight(df_tr, df_tr.index)
        model = XGBRegressor(**params)
        model.fit(X_tr, y_tr, sample_weight=w_tr, eval_set=[(X_va, y_va)], verbose=False)
        pred = model.predict(X_va)
        smapes.append(smape_exp(y_va, pred))
    return float(np.mean(smapes))

def tune_lgb_timecv(trial, df_b, features, seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth":    trial.suggest_int("max_depth", 3, 8),
        "learning_rate":trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "subsample":    trial.suggest_float("subsample", 0.7, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.7, 1.0),
        "objective": "mae",
        "random_state": seed,
    }
    folds = make_time_folds(df_b)
    smapes = []
    for tr_idx, va_idx in folds:
        df_tr = df_b.loc[tr_idx].copy()
        df_va = df_b.loc[va_idx].copy()
        pm = fold_safe_hour_stats(df_tr)
        solar_proxy = fold_safe_expected_solar(df_tr)
        df_tr = merge_fold_safe(df_tr, pm, solar_proxy)
        df_va = merge_fold_safe(df_va, pm, solar_proxy)

        X_tr = df_tr[features].values
        y_tr = np.log1p(df_tr[target].values.astype(float))
        X_va = df_va[features].values
        y_va = np.log1p(df_va[target].values.astype(float))

        w_tr = make_recency_weight(df_tr, df_tr.index)
        model = LGBMRegressor(**params)
        model.fit(X_tr, y_tr, sample_weight=w_tr,
                  eval_set=[(X_va, y_va)],
                  callbacks=[lgb.early_stopping(50, verbose=False)])
        pred = model.predict(X_va)
        smapes.append(smape_exp(y_va, pred))
    return float(np.mean(smapes))

def tune_cat_timecv(trial, df_b, features, seed):
    params = {
        "iterations":   trial.suggest_int("iterations", 300, 1000),
        "depth":        trial.suggest_int("depth", 3, 8),
        "learning_rate":trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
        "l2_leaf_reg":  trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
        "random_seed":  seed,
        "loss_function":"MAE",
        "verbose": 0,
    }
    folds = make_time_folds(df_b)
    smapes = []
    for tr_idx, va_idx in folds:
        df_tr = df_b.loc[tr_idx].copy()
        df_va = df_b.loc[va_idx].copy()
        pm = fold_safe_hour_stats(df_tr)
        solar_proxy = fold_safe_expected_solar(df_tr)
        df_tr = merge_fold_safe(df_tr, pm, solar_proxy)
        df_va = merge_fold_safe(df_va, pm, solar_proxy)

        X_tr = df_tr[features].values
        y_tr = np.log1p(df_tr[target].values.astype(float))
        X_va = df_va[features].values
        y_va = np.log1p(df_va[target].values.astype(float))

        w_tr = make_recency_weight(df_tr, df_tr.index)
        model = CatBoostRegressor(**params)
        model.fit(X_tr, y_tr, sample_weight=w_tr,
                  eval_set=(X_va, y_va), early_stopping_rounds=50, verbose=0)
        pred = model.predict(X_va)
        smapes.append(smape_exp(y_va, pred))
    return float(np.mean(smapes))

def get_or_tune_params_once_timecv(bno, df_b, features, param_dir):
    """ê±´ë¬¼ë‹¹ 1íšŒë§Œ ì‹œê°„ê¸°ë°˜ CVë¡œ íŠœë‹í•˜ê³  JSON ì €ì¥/ë¡œë“œ"""
    os.makedirs(param_dir, exist_ok=True)
    paths = {
        "xgb": os.path.join(param_dir, f"{bno}_xgb_time.json"),
        "lgb": os.path.join(param_dir, f"{bno}_lgb_time.json"),
        "cat": os.path.join(param_dir, f"{bno}_cat_time.json"),
    }
    params = {}
    # XGB
    if os.path.exists(paths["xgb"]):
        with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_xgb_timecv(t, df_b, features, seed), n_trials=30)
        params["xgb"] = st.best_params
        with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)
    # LGB
    if os.path.exists(paths["lgb"]):
        with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_lgb_timecv(t, df_b, features, seed), n_trials=30)
        params["lgb"] = st.best_params
        with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)
    # CAT
    if os.path.exists(paths["cat"]):
        with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_cat_timecv(t, df_b, features, seed), n_trials=30)
        params["cat"] = st.best_params
        with open(paths["cat"], "w") as f: json.dump(params["cat"], f)
    return params

# ------------------------------
# ë©”íƒ€(Ridge) íŠœë‹ë„ ì‹œê°„ê¸°ë°˜
# ------------------------------
def objective_ridge_time(trial, oof_tr, oof_val, y_tr, y_val):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    ridge.fit(oof_tr, y_tr)
    preds = ridge.predict(oof_val)
    return smape_exp(y_val, preds)

# ------------------------------
# ê±´ë¬¼ ë‹¨ìœ„ ì²˜ë¦¬ (ì‹œê°„ê¸°ë°˜ CV)
# ------------------------------
def process_building_timecv(bno):
    print(f"ğŸ¢ building {bno} Time-CV...")
    param_dir = os.path.join(path, "optuna_params_time")
    os.makedirs(param_dir, exist_ok=True)

    tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
    te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

    # ë¹Œë”©ë³„ ê³µí†µ íŠ¹ì§• ì„¸íŠ¸(ë¨¸ì§€ ì „ ê¸°ë³¸)
    features = features_base.copy()

    # ì‹œê°„ê¸°ë°˜ íŠœë‹(ê±´ë¬¼ë‹¹ 1íšŒ)
    best_params = get_or_tune_params_once_timecv(bno, tr_b, features, param_dir)

    folds = make_time_folds(tr_b)
    test_preds, val_smapes = [], []

    # Ridgeë„ ê±´ë¬¼ë‹¹ 1íšŒë§Œ: ì²« foldì—ì„œ íŠœë‹ â†’ ì €ì¥, ì´í›„ ë¡œë“œ
    ridge_key = f"{bno}_ridge_time"
    ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
    ridge_params_cached = None
    if os.path.exists(ridge_path):
        with open(ridge_path, "r") as f:
            ridge_params_cached = json.load(f)

    for fold, (tr_idx, va_idx) in enumerate(folds, 1):
        print(f" - fold {fold}")
        df_tr = tr_b.loc[tr_idx].copy()
        df_va = tr_b.loc[va_idx].copy()
        df_te = te_b.copy()

        # Fold-safe ì§‘ê³„ ìƒì„±
        pm = fold_safe_hour_stats(df_tr)
        solar_proxy = fold_safe_expected_solar(df_tr)

        # Fold-safe ë¨¸ì§€
        df_tr = merge_fold_safe(df_tr, pm, solar_proxy)
        df_va = merge_fold_safe(df_va, pm, solar_proxy)
        df_te = merge_fold_safe(df_te, pm, solar_proxy)

        # íŠ¹ì§• ì»¬ëŸ¼(ë¨¸ì§€ í›„ ì¶”ê°€ëœ ì—´ í¬í•¨)
        cols_now = features + [c for c in ['day_hour_mean','day_hour_std','expected_solar'] if c in df_tr.columns and c not in features]
        X_tr = df_tr[cols_now].values
        y_tr = np.log1p(df_tr[target].values.astype(float))
        X_va = df_va[cols_now].values
        y_va = np.log1p(df_va[target].values.astype(float))
        X_te = df_te[cols_now].values

        # ìµœê·¼ ê°€ì¤‘ì¹˜
        w_tr = make_recency_weight(df_tr, df_tr.index)

        # Base ëª¨ë¸ í•™ìŠµ
        xgb = XGBRegressor(**best_params["xgb"])
        xgb.fit(X_tr, y_tr, sample_weight=w_tr, eval_set=[(X_va, y_va)], verbose=False)

        lgbm = LGBMRegressor(**best_params["lgb"])
        lgbm.fit(X_tr, y_tr, sample_weight=w_tr,
                 eval_set=[(X_va, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])

        cat = CatBoostRegressor(**best_params["cat"])
        cat.fit(X_tr, y_tr, sample_weight=w_tr,
                eval_set=(X_va, y_va), early_stopping_rounds=50, verbose=0)

        # ìŠ¤íƒœí‚¹ ì…ë ¥
        oof_tr = np.vstack([xgb.predict(X_tr), lgbm.predict(X_tr), cat.predict(X_tr)]).T
        oof_va = np.vstack([xgb.predict(X_va), lgbm.predict(X_va), cat.predict(X_va)]).T
        oof_te = np.vstack([xgb.predict(X_te), lgbm.predict(X_te), cat.predict(X_te)]).T

        # Ridge ë©”íƒ€: ê±´ë¬¼ë‹¹ 1íšŒ íŠœë‹/ì €ì¥
        if ridge_params_cached is None and fold == 1:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: objective_ridge_time(t, oof_tr, oof_va, y_tr, y_va), n_trials=30)
            ridge_params_cached = st.best_params
            with open(ridge_path, "w") as f:
                json.dump(ridge_params_cached, f)

        meta = Ridge(alpha=ridge_params_cached["alpha"])
        meta.fit(oof_tr, y_tr)

        va_pred_log = meta.predict(oof_va)
        te_pred_log = meta.predict(oof_te)

        # ê²€ì¦ ì„±ëŠ¥
        fold_smape = smape_exp(y_va, va_pred_log)

        # ì˜ˆì¸¡ í›„ ë³´ì •(ê²€ì¦ìœ¼ë¡œ í…Œì´ë¸”) â†’ í…ŒìŠ¤íŠ¸ì— ì ìš©
        calib_tab = build_calib_table(df_va, va_pred_log)
        te_pred = apply_calib(df_te, te_pred_log, calib_tab)

        # í¼ì„¼íƒ€ì¼ í´ë¦¬í•‘
        te_pred = per_building_clip(tr_b, te_pred)

        val_smapes.append(fold_smape)
        test_preds.append(te_pred)

    avg_test_pred = np.mean(test_preds, axis=0)
    avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
    return avg_test_pred.tolist(), avg_smape

# ==============================
# ë³‘ë ¬ ì‹¤í–‰
# ==============================
results = Parallel(n_jobs=-1, backend="loky")(
    delayed(process_building_timecv)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
)

final_preds, val_smapes = [], []
for preds, sm in results:
    final_preds.extend(preds)
    val_smapes.append(sm)

samplesub["answer"] = final_preds
today = datetime.datetime.now().strftime("%Y%m%d")
avg_smape = float(np.mean(val_smapes))
filename = f"submission_timecv_foldsafe_weeklylag_recency_calibclip_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")