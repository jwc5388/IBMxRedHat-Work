

####current best !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
######!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

# exit()


# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import KFold, TimeSeriesSplit  # â† ë³€ê²½: TSS ì¶”ê°€
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ
# # ==============================
# seed = 2025
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìë¡œ ìºìŠ¤íŒ…
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     # ì„¤ë¹„ ìœ ë¬´ í”Œë˜ê·¸
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['DI'] = 0.0
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH / THI / WCT (train/test ë™ì¼)
# def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         cs = np.cumsum(x - 26)
#         return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
#     parts = []
#     for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['THI'] = 0.0
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_kor(train)
# test  = add_CDH_kor(test)
# train = add_THI_WCT_kor(train)
# test  = add_THI_WCT_kor(test)

# # === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     pm = (train
#           .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
#           .agg(['mean','std'])
#           .reset_index()
#           .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
#     train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
#     test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
# else:
#     train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
#     test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# # === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
# feature_candidates = [
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     'DI','expected_solar',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     'CDH','THI','WCT',
#     'day_hour_mean','day_hour_std'
# ]
# features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# # 2) target ëª…ì‹œ
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # 3) ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
# X = train[features].values
# y = np.log1p(train[target].values.astype(float))
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']  # ë‚´ë¶€/ì™¸ë¶€ ë¶„í• ì— ì°¸ê³  ê°€ëŠ¥

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y shape: {y.shape}")

# # ------------------------------
# # SMAPE
# # ------------------------------
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# # ------------------------------
# # ë‚´ë¶€ íŠœë‹: TimeSeriesSplit(n_splits=3)ë¡œ êµì²´ (ì‹œê³„ì—´ ë³´ì „)
# # ------------------------------
# def tune_xgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#         "early_stopping_rounds": 50,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = XGBRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_lgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_cat_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = CatBoostRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def get_or_tune_params_once(bno, X_full, y_full, order_index, param_dir):
#     """ê±´ë¬¼ë‹¹ 1íšŒë§Œ íŠœë‹í•˜ê³  JSON ì €ì¥/ë¡œë“œ
#        ë³€ê²½: ë‚´ë¶€ ê²€ì¦ì„ TimeSeriesSplitìœ¼ë¡œ(ì‹œê³„ì—´ ìˆœì„œ=order_indexì— ë§ì¶° ì •ë ¬)"""
#     os.makedirs(param_dir, exist_ok=True)
#     paths = {
#         "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
#         "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
#         "cat": os.path.join(param_dir, f"{bno}_cat.json"),
#     }
#     params = {}

#     # ì‹œê³„ì—´ ì •ë ¬
#     X_sorted = X_full[order_index]
#     y_sorted = y_full[order_index]

#     # XGB
#     if os.path.exists(paths["xgb"]):
#         with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["xgb"] = st.best_params
#         with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)

#     # LGB
#     if os.path.exists(paths["lgb"]):
#         with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["lgb"] = st.best_params
#         with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)

#     # CAT
#     if os.path.exists(paths["cat"]):
#         with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["cat"] = st.best_params
#         with open(paths["cat"], "w") as f: json.dump(params["cat"], f)

#     return params

# # ------------------------------
# # Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜ìœ¼ë¡œ ê±´ë¬¼ë‹¹ 1íšŒ
# # ------------------------------
# def objective_ridge_on_oof(trial, oof_meta, y_full):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     # ê°„ë‹¨íˆ 5-Fold CVë¡œ OOF ë©”íƒ€ ìµœì í™”
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full[va_idx], preds))
#     return float(np.mean(scores))

# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params")
#     os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X_full = tr_b[features].values
#     y_full = np.log1p(tr_b[target].values.astype(float))
#     X_test = te_b[features].values

#     # ì‹œê³„ì—´ ì •ë ¬ ì¸ë±ìŠ¤ (ë‚´ë¶€ íŠœë‹ìš©)
#     order = np.argsort(tr_b['ì¼ì‹œ'].values)

#     # ê±´ë¬¼ë‹¹ 1íšŒ Optuna íŠœë‹(TimeSeriesSplit) í›„ íŒŒë¼ë¯¸í„° ë¡œë“œ
#     best_params = get_or_tune_params_once(bno, X_full, y_full, order, param_dir)

#     # ì™¸ë¶€ KFold(ê·¸ëŒ€ë¡œ ìœ ì§€)
#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     # ----- ì§„ì§œ OOF ìŠ¤íƒœí‚¹ ì¤€ë¹„ -----
#     n_train_b = len(tr_b)
#     n_test_b  = len(te_b)
#     base_models = ["xgb", "lgb", "cat"]
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)  # ê° í–‰=í›ˆë ¨í–‰, ì—´=ë² ì´ìŠ¤ëª¨ë¸
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     # í´ë“œ ë£¨í”„: base í•™ìŠµ â†’ ê²€ì¦ì…‹ ì˜ˆì¸¡ì„ OOFì— ì±„ìš°ê³ , í…ŒìŠ¤íŠ¸ëŠ” í´ë“œ í‰ê· ì„ ëˆ„ì 
#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr, y_va = y_full[tr_idx], y_full[va_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         # ë² ì´ìŠ¤ ëª¨ë¸ë“¤ í•™ìŠµ (íŠœë‹ íŒŒë¼ë¯¸í„° ì‚¬ìš©, ESëŠ” ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
#         xgb = XGBRegressor(**best_params["xgb"])
#         xgb.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)

#         lgbm = LGBMRegressor(**best_params["lgb"])
#         lgbm.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)],
#                  callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat = CatBoostRegressor(**best_params["cat"])
#         cat.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va),
#                 early_stopping_rounds=50, verbose=0)

#         # ê²€ì¦ì…‹ OOF ì˜ˆì¸¡ ì €ì¥
#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)

#         # í…ŒìŠ¤íŠ¸ ë©”íƒ€ ì…ë ¥ ëˆ„ì (í´ë“œ í‰ê· )
#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)

#     test_meta = test_meta_accum / kf.get_n_splits()

#     # ----- ë©”íƒ€(Ridge) ê±´ë¬¼ë‹¹ 1íšŒ íŠœë‹/í•™ìŠµ: OOF í–‰ë ¬ ê¸°ë°˜ -----
#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")

#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full), n_trials=30)
#         ridge_params = st.best_params
#         with open(ridge_path, "w") as f:
#             json.dump(ridge_params, f)

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full)

#     # OOF ì„±ëŠ¥(ì „ì²´ ê¸°ì¤€) ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
#     oof_pred = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full, oof_pred))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")




















# import pandas as pd
# import numpy as np
# import os
# from datetime import datetime

# # ===== 0) ê²½ë¡œ =====
# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# PATH = os.path.join(BASE_PATH, "_data/dacon/electricity/")
# OUT_DIR = os.path.join(PATH, "holiday_dates")
# os.makedirs(OUT_DIR, exist_ok=True)

# # ===== 1) íœ´ë¬´ì¼ íƒì§€ í•¨ìˆ˜ =====
# def detect_holidays_simple(file_path, name):
#     df = pd.read_csv(file_path)
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H', errors='coerce')
#     df['date'] = df['ì¼ì‹œ'].dt.date
#     df['dow'] = df['ì¼ì‹œ'].dt.dayofweek  # 0=ì›”, 6=ì¼
    
#     # 1) ì£¼ë§
#     # weekend_days = set(df.loc[df['dow'] >= 5, 'date'])

#     # 2) í•œêµ­ ê³µíœ´ì¼
#     try:
#         import holidays
#         years = sorted({d.year for d in df['date']})
#         hol = holidays.KR(years=years)
#         official_holidays = {pd.Timestamp(d).date() for d in hol.keys()}
#     except ImportError:
#         official_holidays = set()

#     # í•©ì¹˜ê¸°
#     all_holidays = sorted(official_holidays)

#     # CSV ì €ì¥
#     out_path = os.path.join(OUT_DIR, f'holiday_dates_{name}.csv')
#     pd.DataFrame({'date': all_holidays}).to_csv(out_path, index=False)
#     print(f"[{name}] íœ´ë¬´/ê³µíœ´ì¼ í›„ë³´ {len(all_holidays)}ê°œ ì €ì¥ â†’ {out_path}")

# # ===== 2) ì‹¤í–‰ =====
# detect_holidays_simple(os.path.join(PATH, "train.csv"), "train")
# detect_holidays_simple(os.path.join(PATH, "test.csv"), "test")
# # exit()




















## íœ´ë¬´ ì¶”ì •ì¼ë§Œ ì¶”ê°€

# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import KFold, TimeSeriesSplit  # â† ë‚´ë¶€ íŠœë‹ìš©
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# # -------------------------
# # NEW: ê³µíœ´ì¼ ë¼ì´ë¸ŒëŸ¬ë¦¬
# # -------------------------
# from datetime import timedelta
# import holidays

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ / ì˜µì…˜
# # ==============================
# seed = 2025
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# # ---- NEW: íœ´ì¼/íœ´ë¬´ ì²˜ë¦¬ ìŠ¤ìœ„ì¹˜
# DROP_HOLIDAY = False           # Trueë©´ trainì—ì„œ ê³µíœ´ì¼ ì œê±°
# DROP_CLOSURE = False           # Trueë©´ trainì—ì„œ íœ´ë¬´ ì¶”ì •ì¼ ì œê±°
# WEIGHT_HOLIDAY = 0.6           # ê³µíœ´ì¼ ê°€ì¤‘ì¹˜
# WEIGHT_CLOSURE = 0.3          # íœ´ë¬´ ì¶”ì •ì¼ ê°€ì¤‘ì¹˜

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìë¡œ ìºìŠ¤íŒ…
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     # ì„¤ë¹„ ìœ ë¬´ í”Œë˜ê·¸
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['DI'] = 0.0
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH / THI / WCT
# def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         cs = np.cumsum(x - 26)
#         return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
#     parts = []
#     for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['THI'] = 0.0
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_kor(train)
# test  = add_CDH_kor(test)
# train = add_THI_WCT_kor(train)
# test  = add_THI_WCT_kor(test)

# # -------------------------
# # NEW: ìº˜ë¦°ë” í”Œë˜ê·¸ + íœ´ë¬´ ì¶”ì •ì¼ ë¼ë²¨
# # -------------------------
# def add_calendar_flags(df, country='KR'):
#     df = df.copy()
#     df['date'] = df['ì¼ì‹œ'].dt.date
#     years = sorted({d.year for d in df['ì¼ì‹œ']})
#     hol = holidays.country_holidays(country=country, years=years)
#     is_holiday = df['date'].apply(lambda d: d in hol).astype(int)
#     prev_is_holiday = df['date'].apply(lambda d: (d - timedelta(days=1)) in hol).astype(int)
#     next_is_holiday = df['date'].apply(lambda d: (d + timedelta(days=1)) in hol).astype(int)
#     df['is_holiday'] = is_holiday
#     df['is_holiday_prev'] = prev_is_holiday
#     df['is_holiday_next'] = next_is_holiday
#     return df

# def tag_closure_like_days(df, target_col='ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'):
#     df = df.copy()
#     if target_col not in df.columns:
#         df['is_closure_like_day'] = 0
#         return df
#     df['date'] = df['ì¼ì‹œ'].dt.date
#     daily = (df.groupby(['ê±´ë¬¼ë²ˆí˜¸','date'])[target_col]
#                .sum().rename('day_total').reset_index())
#     tmp = df[['ê±´ë¬¼ë²ˆí˜¸','date','dayofweek']].drop_duplicates()
#     daily = daily.merge(tmp, on=['ê±´ë¬¼ë²ˆí˜¸','date'], how='left')
#     base = (daily.groupby(['ê±´ë¬¼ë²ˆí˜¸','dayofweek'])['day_total']
#                   .median().rename('base_median').reset_index())
#     p10  = (daily.groupby(['ê±´ë¬¼ë²ˆí˜¸'])['day_total']
#                   .quantile(0.10).rename('p10').reset_index())
#     daily = (daily.merge(base, on=['ê±´ë¬¼ë²ˆí˜¸','dayofweek'], how='left')
#                   .merge(p10, on='ê±´ë¬¼ë²ˆí˜¸', how='left'))
#     daily['closure_like'] = ((daily['day_total'] < 0.4*daily['base_median']) |
#                              (daily['day_total'] <= daily['p10'])).astype(int)
#     df = df.merge(daily[['ê±´ë¬¼ë²ˆí˜¸','date','closure_like']],
#                   on=['ê±´ë¬¼ë²ˆí˜¸','date'], how='left')
#     df['is_closure_like_day'] = df['closure_like'].fillna(0).astype(int)
#     df.drop(columns=['closure_like'], inplace=True)
#     return df

# # ì ìš©
# train = add_calendar_flags(train, country='KR')
# test  = add_calendar_flags(test,  country='KR')
# train = tag_closure_like_days(train, target_col='ì „ë ¥ì†Œë¹„ëŸ‰(kWh)')
# test  = tag_closure_like_days(test,  target_col='ì „ë ¥ì†Œë¹„ëŸ‰(kWh)')

# # === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     pm = (train
#           .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
#           .agg(['mean','std'])
#           .reset_index()
#           .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
#     train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
#     test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
# else:
#     train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
#     test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# # === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # -------------------------
# # NEW: íœ´ì¼/íœ´ë¬´ ë“œë¡­ ë˜ëŠ” ê°€ì¤‘ì¹˜ ë¶€ì—¬
# # -------------------------
# if DROP_HOLIDAY:
#     train = train.loc[train['is_holiday'] == 0].reset_index(drop=True)
# if DROP_CLOSURE:
#     train = train.loc[train['is_closure_like_day'] == 0].reset_index(drop=True)

# train['_sample_weight'] = 1.0
# if not DROP_HOLIDAY:
#     train.loc[train['is_holiday'] == 1, '_sample_weight'] = WEIGHT_HOLIDAY
# if not DROP_CLOSURE:
#     train.loc[train['is_closure_like_day'] == 1, '_sample_weight'] = WEIGHT_CLOSURE

# # === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
# feature_candidates = [
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     'DI','expected_solar',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     'CDH','THI','WCT',
#     'day_hour_mean','day_hour_std',
#     # NEW: ìº˜ë¦°ë”/íœ´ë¬´ í”Œë˜ê·¸
#     'is_holiday','is_holiday_prev','is_holiday_next','is_closure_like_day'
# ]
# features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# # 2) target
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # 3) ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
# X = train[features].values
# y = np.log1p(train[target].values.astype(float))
# W = train.get('_sample_weight', pd.Series(1.0, index=train.index)).values  # NEW
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y shape: {y.shape}")

# # ------------------------------
# # SMAPE
# # ------------------------------
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# # ------------------------------
# # ë‚´ë¶€ íŠœë‹: TimeSeriesSplit(n_splits=3)
# # ------------------------------
# def tune_xgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#         "early_stopping_rounds": 50,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = XGBRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_lgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_cat_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = CatBoostRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def get_or_tune_params_once(bno, X_full, y_full, order_index, param_dir):
#     os.makedirs(param_dir, exist_ok=True)
#     paths = {
#         "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
#         "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
#         "cat": os.path.join(param_dir, f"{bno}_cat.json"),
#     }
#     params = {}

#     X_sorted = X_full[order_index]
#     y_sorted = y_full[order_index]

#     # XGB
#     if os.path.exists(paths["xgb"]):
#         with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["xgb"] = st.best_params
#         with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)

#     # LGB
#     if os.path.exists(paths["lgb"]):
#         with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["lgb"] = st.best_params
#         with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)

#     # CAT
#     if os.path.exists(paths["cat"]):
#         with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["cat"] = st.best_params
#         with open(paths["cat"], "w") as f: json.dump(params["cat"], f)

#     return params

# # ------------------------------
# # Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜
# # ------------------------------
# def objective_ridge_on_oof(trial, oof_meta, y_full):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full[va_idx], preds))
#     return float(np.mean(scores))

# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params")
#     os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X_full = tr_b[features].values
#     y_full = np.log1p(tr_b[target].values.astype(float))
#     W_full = tr_b.get('_sample_weight', pd.Series(1.0, index=tr_b.index)).values  # NEW
#     X_test = te_b[features].values

#     order = np.argsort(tr_b['ì¼ì‹œ'].values)
#     best_params = get_or_tune_params_once(bno, X_full, y_full, order, param_dir)

#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     n_train_b = len(tr_b)
#     n_test_b  = len(te_b)
#     base_models = ["xgb", "lgb", "cat"]
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr, y_va = y_full[tr_idx], y_full[va_idx]
#         w_tr = W_full[tr_idx]  # NEW

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         xgb = XGBRegressor(**best_params["xgb"])
#         xgb.fit(X_tr_s, y_tr, sample_weight=w_tr, eval_set=[(X_va_s, y_va)], verbose=False)

#         lgbm = LGBMRegressor(**best_params["lgb"])
#         lgbm.fit(X_tr_s, y_tr, sample_weight=w_tr,
#                  eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat = CatBoostRegressor(**best_params["cat"])
#         cat.fit(X_tr_s, y_tr, sample_weight=w_tr,
#                 eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)

#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)

#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)

#     test_meta = test_meta_accum / kf.get_n_splits()

#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")

#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full), n_trials=30)
#         ridge_params = st.best_params
#         with open(ridge_path, "w") as f:
#             json.dump(ridge_params, f)

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full)

#     oof_pred = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full, oof_pred))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_cal_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")

# exit()








### ë‹¤ ì¶”ê°€í•˜ê³ , ê³µíœ´ì¼ ì²˜ë¦¬ê¹Œì§€ ëœê±°



# # exit()
# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import KFold, TimeSeriesSplit
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# from datetime import timedelta
# import holidays

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ / ì˜µì…˜
# # ==============================
# seed = 2025
# random.seed(seed); np.random.seed(seed); tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# # íœ´ì¼/íœ´ë¬´ ì²˜ë¦¬ ìŠ¤ìœ„ì¹˜ (ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ë°”ê¿”ì¨)
# DROP_HOLIDAY  = True     # Trueë©´ trainì—ì„œ ê³µíœ´ì¼ ì œê±°
# DROP_CLOSURE  = True     # Trueë©´ trainì—ì„œ íœ´ë¬´ ì¶”ì •ì¼ ì œê±°
# WEIGHT_HOLIDAY = 0.7     # ê³µíœ´ì¼ ê°€ì¤‘ì¹˜ (DROP_HOLIDAY=Falseì¼ ë•Œë§Œ ì ìš©)
# WEIGHT_CLOSURE = 0.5     # íœ´ë¬´ ì¶”ì •ì¼ ê°€ì¤‘ì¹˜ (DROP_CLOSURE=Falseì¼ ë•Œë§Œ)

# # ==============================
# # ì•ˆì „ ë¡œê·¸ ë³€í™˜ ìœ í‹¸ (ìŒìˆ˜ ë°©ì§€)
# # ==============================
# def log1p_pos(a):
#     a = np.asarray(a, dtype=float)
#     return np.log1p(np.maximum(a, 0.0))

# # ==============================
# # ë°ì´í„° ë¡œë“œ
# # ==============================
# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test  = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === building_info ë³‘í•©
# bi = buildinginfo.copy() if 'buildinginfo' in globals() else None
# if bi is not None:
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = (bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = (bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')
#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi,  on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # ==============================
# # 1) ì‹œê°„ íŒŒìƒ
# # ==============================
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['DI'] = 0.0
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # 2) expected_solar
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#                         .mean().reset_index().rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'}))
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0; test['expected_solar'] = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # 3) ì¼ë³„ ì˜¨ë„ í†µê³„
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train); test = add_daily_temp_stats_kor(test)

# # 4) CDH / THI / WCT
# def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0; return df
#     def _cdh_1d(x):
#         cs = np.cumsum(x - 26)
#         return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
#     parts = []
#     for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         parts.append(pd.Series(_cdh_1d(arr), index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['THI'] = 0.0
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_kor(train); test = add_CDH_kor(test)
# train = add_THI_WCT_kor(train); test = add_THI_WCT_kor(test)

# # -------------------------
# # ìº˜ë¦°ë” í”Œë˜ê·¸ + íœ´ë¬´ ì¶”ì •ì¼
# # -------------------------
# def add_calendar_flags(df, country='KR'):
#     df = df.copy()
#     df['date'] = df['ì¼ì‹œ'].dt.date
#     years = sorted({d.year for d in df['ì¼ì‹œ']})
#     hol = holidays.country_holidays(country=country, years=years)
#     df['is_holiday']       = df['date'].apply(lambda d: d in hol).astype(int)
#     df['is_holiday_prev']  = df['date'].apply(lambda d: (d - timedelta(days=1)) in hol).astype(int)
#     df['is_holiday_next']  = df['date'].apply(lambda d: (d + timedelta(days=1)) in hol).astype(int)
#     return df

# def tag_closure_like_days(df, target_col='ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'):
#     df = df.copy()
#     if target_col not in df.columns:
#         df['is_closure_like_day'] = 0; return df
#     df['date'] = df['ì¼ì‹œ'].dt.date
#     daily = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','date'])[target_col].sum().rename('day_total').reset_index()
#     tmp = df[['ê±´ë¬¼ë²ˆí˜¸','date','dayofweek']].drop_duplicates()
#     daily = daily.merge(tmp, on=['ê±´ë¬¼ë²ˆí˜¸','date'], how='left')
#     base = daily.groupby(['ê±´ë¬¼ë²ˆí˜¸','dayofweek'])['day_total'].median().rename('base_median').reset_index()
#     p10  = daily.groupby(['ê±´ë¬¼ë²ˆí˜¸'])['day_total'].quantile(0.10).rename('p10').reset_index()
#     daily = daily.merge(base, on=['ê±´ë¬¼ë²ˆí˜¸','dayofweek'], how='left').merge(p10, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     daily['closure_like'] = ((daily['day_total'] < 0.4*daily['base_median']) |
#                              (daily['day_total'] <= daily['p10'])).astype(int)
#     df = df.merge(daily[['ê±´ë¬¼ë²ˆí˜¸','date','closure_like']], on=['ê±´ë¬¼ë²ˆí˜¸','date'], how='left')
#     df['is_closure_like_day'] = daily_val = df['closure_like'].fillna(0).astype(int)
#     df.drop(columns=['closure_like'], inplace=True)
#     return df

# train = add_calendar_flags(train, 'KR'); test = add_calendar_flags(test, 'KR')
# train = tag_closure_like_days(train, 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)')
# test  = tag_closure_like_days(test,  'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)')  # testì—ëŠ” 0ìœ¼ë¡œë§Œ ì±„ì›Œì§(íƒ€ê¹ƒ ì—†ìŒ)

# # 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     pm = (train.groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
#                .agg(['mean','std']).reset_index()
#                .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
#     train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
#     test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
# else:
#     train['day_hour_mean']=0.0; train['day_hour_std']=0.0
#     test['day_hour_mean']=0.0;  test['day_hour_std']=0.0

# # 6) 0 kWh ì œê±°(ì„ íƒ)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # íœ´ì¼/íœ´ë¬´ ë“œë¡­ ë˜ëŠ” ê°€ì¤‘ì¹˜
# if DROP_HOLIDAY:
#     train = train.loc[train['is_holiday'] == 0].reset_index(drop=True)
# if DROP_CLOSURE:
#     train = train.loc[train['is_closure_like_day'] == 0].reset_index(drop=True)

# train['_sample_weight'] = 1.0
# if not DROP_HOLIDAY:
#     train.loc[train['is_holiday'] == 1, '_sample_weight'] = WEIGHT_HOLIDAY
# if not DROP_CLOSURE:
#     train.loc[train['is_closure_like_day'] == 1, '_sample_weight'] = WEIGHT_CLOSURE

# # ë²”ì£¼í˜• ì¸ì½”ë”©
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat:i for i,cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # ==============================
# # ìµœì¢… í”¼ì²˜/íƒ€ê¹ƒ
# # ==============================
# feature_candidates = [
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     'DI','expected_solar',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     'CDH','THI','WCT',
#     'day_hour_mean','day_hour_std',
#     'is_holiday','is_holiday_prev','is_holiday_next','is_closure_like_day'
# ]
# features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# X = train[features].values
# y_raw = train[target].values.astype(float)
# y_log = log1p_pos(y_raw)
# W = train.get('_sample_weight', pd.Series(1.0, index=train.index)).values
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y_log shape: {y_log.shape}")

# # ==============================
# # SMAPE (ë¡œê·¸ ìŠ¤ì¼€ì¼ìš©)
# # ==============================
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# # ==============================
# # íŠœë‹ í•¨ìˆ˜ë“¤ (TSS)
# # ==============================
# def tune_xgb_tss(trial, X_full_sorted, y_full_log, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#         "early_stopping_rounds": 50,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores=[]
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_log[tr_idx], y_full_log[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = XGBRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_lgb_tss(trial, X_full_sorted, y_full_log, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores=[]
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_log[tr_idx], y_full_log[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_cat_tss(trial, X_full_sorted, y_full_log, seed=seed):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores=[]
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_log[tr_idx], y_full_log[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = CatBoostRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# # TweedieëŠ” raw íƒ€ê¹ƒ ê¸°ì¤€ìœ¼ë¡œ íŠœë‹ â†’ í‰ê°€/ìŠ¤íƒœí‚¹ì€ log1p_posë¡œ ë³€í™˜
# def tune_lgb_tweedie_tss(trial, X_full_sorted, y_full_raw, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "num_leaves": trial.suggest_int("num_leaves", 31, 256),
#         "min_child_samples": trial.suggest_int("min_child_samples", 20, 150),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
#         "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 1.0, log=True),
#         "tweedie_variance_power": trial.suggest_float("tweedie_variance_power", 1.1, 1.9),
#         "objective": "tweedie",
#         "metric": "mae",
#         "random_state": seed,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores=[]
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr_raw, y_va_raw = y_full_raw[tr_idx], y_full_raw[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred_raw = model.predict(X_va_s)
#         y_va_log  = log1p_pos(y_va_raw)
#         pred_log  = log1p_pos(pred_raw)
#         scores.append(smape_exp(y_va_log, pred_log))
#     return float(np.mean(scores))

# # ==============================
# # íŒŒë¼ë¯¸í„° ì €ì¥/ë¡œë“œ (ê±´ë¬¼ë‹¹ 1íšŒ)
# # ==============================
# def get_or_tune_params_once(bno, X_full, y_full_log, order_index, param_dir):
#     os.makedirs(param_dir, exist_ok=True)
#     paths = {
#         "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
#         "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
#         "cat": os.path.join(param_dir, f"{bno}_cat.json"),
#         "twd": os.path.join(param_dir, f"{bno}_twd.json"),
#     }
#     params = {}

#     X_sorted = X_full[order_index]
#     y_sorted_log = y_full_log[order_index]
#     y_sorted_raw = np.expm1(y_sorted_log)

#     # XGB
#     if os.path.exists(paths["xgb"]):
#         with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted_log), n_trials=30)
#         params["xgb"] = st.best_params; open(paths["xgb"], "w").write(json.dumps(params["xgb"]))

#     # LGB (MAE)
#     if os.path.exists(paths["lgb"]):
#         with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted_log), n_trials=30)
#         params["lgb"] = st.best_params; open(paths["lgb"], "w").write(json.dumps(params["lgb"]))

#     # CAT
#     if os.path.exists(paths["cat"]):
#         with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted_log), n_trials=30)
#         params["cat"] = st.best_params; open(paths["cat"], "w").write(json.dumps(params["cat"]))

#     # LGB Tweedie (raw target)
#     if os.path.exists(paths["twd"]):
#         with open(paths["twd"], "r") as f: params["twd"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tweedie_tss(t, X_sorted, y_sorted_raw), n_trials=30)
#         params["twd"] = st.best_params; open(paths["twd"], "w").write(json.dumps(params["twd"]))

#     return params

# # ==============================
# # ë©”íƒ€ íŠœë‹ (Ridge)
# # ==============================
# def objective_ridge_on_oof(trial, oof_meta, y_full_log):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full_log[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full_log[va_idx], preds))
#     return float(np.mean(scores))

# # ==============================
# # ê±´ë¬¼ë³„ í•™ìŠµ
# # ==============================
# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params"); os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X_full = tr_b[features].values
#     y_full_raw = tr_b[target].values.astype(float)
#     y_full_log = log1p_pos(y_full_raw)
#     W_full = tr_b.get('_sample_weight', pd.Series(1.0, index=tr_b.index)).values
#     X_test = te_b[features].values

#     order = np.argsort(tr_b['ì¼ì‹œ'].values)
#     best_params = get_or_tune_params_once(bno, X_full, y_full_log, order, param_dir)

#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     n_train_b = len(tr_b)
#     n_test_b  = len(te_b)
#     base_models = ["xgb", "lgb", "cat", "twd"]
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr_log, y_va_log = y_full_log[tr_idx], y_full_log[va_idx]
#         y_tr_raw, y_va_raw = y_full_raw[tr_idx], y_full_raw[va_idx]
#         w_tr = W_full[tr_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr); X_va_s = sc.transform(X_va); X_te_s = sc.transform(X_test)

#         # XGB (log target)
#         xgb = XGBRegressor(**best_params["xgb"])
#         xgb.fit(X_tr_s, y_tr_log, sample_weight=w_tr, eval_set=[(X_va_s, y_va_log)], verbose=False)

#         # LGB (log target)
#         lgbm = LGBMRegressor(**best_params["lgb"])
#         lgbm.fit(X_tr_s, y_tr_log, sample_weight=w_tr,
#                  eval_set=[(X_va_s, y_va_log)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         # CAT (log target)
#         cat = CatBoostRegressor(**best_params["cat"])
#         cat.fit(X_tr_s, y_tr_log, sample_weight=w_tr,
#                 eval_set=(X_va_s, y_va_log), early_stopping_rounds=50, verbose=0)

#         # Tweedie (raw target)
#         twd_params = best_params["twd"].copy()
#         twd_params.update({"objective":"tweedie","metric":"mae","random_state":seed})
#         twd = LGBMRegressor(**twd_params)
#         twd.fit(X_tr_s, y_tr_raw, sample_weight=w_tr,
#                 eval_set=[(X_va_s, y_va_raw)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         # OOF (log scale)
#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)
#         oof_meta[va_idx, 3] = log1p_pos(twd.predict(X_va_s))

#         # TEST meta ëˆ„ì  (log scale)
#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)
#         test_meta_accum[:, 3] += log1p_pos(twd.predict(X_te_s))

#     test_meta = test_meta_accum / kf.get_n_splits()

#     # ë©”íƒ€ íŠœë‹/í•™ìŠµ
#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full_log), n_trials=30)
#         ridge_params = st.best_params; open(ridge_path, "w").write(json.dumps(ridge_params))

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full_log)

#     oof_pred_log = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full_log, oof_pred_log))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_tweedie_fix_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")







############ì§„ì§œ í˜„ì¬ ìµœê³ 

# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import KFold, TimeSeriesSplit  # â† ë³€ê²½: TSS ì¶”ê°€
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ
# # ==============================
# seed = 2025
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['DI'] = 0.0
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH / THI / WCT (train/test ë™ì¼)
# def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         cs = np.cumsum(x - 26)
#         return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
#     parts = []
#     for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['THI'] = 0.0
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_kor(train)
# test  = add_CDH_kor(test)
# train = add_THI_WCT_kor(train)
# test  = add_THI_WCT_kor(test)

# # === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     pm = (train
#           .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
#           .agg(['mean','std'])
#           .reset_index()
#           .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
#     train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
#     test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
# else:
#     train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
#     test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# # === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
# feature_candidates = [
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     'DI','expected_solar',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     'CDH','THI','WCT',
#     'day_hour_mean','day_hour_std'
# ]
# features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# # 2) target ëª…ì‹œ
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # 3) ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ ë°ì´í„° (ê¸°ì¡´ ìœ ì§€)
# X = train[features].values
# y_log = np.log1p(train[target].values.astype(float))   # ê¸°ì¡´ ë² ì´ìŠ¤ ëª¨ë¸ìš© (ë¡œê·¸ íƒ€ê¹ƒ)
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']  # ë‚´ë¶€/ì™¸ë¶€ ë¶„í• ì— ì°¸ê³  ê°€ëŠ¥

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y_log shape: {y_log.shape}")

# # ------------------------------
# # SMAPE
# # ------------------------------
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# # ========== Tweedie ì „ìš© ìœ í‹¸ & íŠœë‹ ì¶”ê°€ (NEW) ==========
# def log1p_pos(arr):
#     """ìŒìˆ˜ ì•ˆì „ ë¡œê·¸ ë³€í™˜ (Tweedie ì˜ˆì¸¡ì„ ìŠ¤íƒœí‚¹ìš© ë¡œê·¸ë¡œ ë³€í™˜)"""
#     return np.log1p(np.clip(arr, a_min=0, a_max=None))

# def tune_lgb_tweedie_tss(trial, X_full_sorted, y_full_sorted_raw, seed=seed):
#     """TweedieëŠ” ì›ì‹œ íƒ€ê¹ƒìœ¼ë¡œ í•™ìŠµ â†’ ì˜ˆì¸¡ì„ log1pë¡œ ë³€í™˜í•´ í‰ê°€"""
#     params = {
#         "objective": "tweedie",
#         "metric": "mae",
#         "boosting_type": "gbdt",
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "num_leaves": trial.suggest_int("num_leaves", 64, 512),
#         "min_child_samples": trial.suggest_int("min_child_samples", 20, 150),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
#         "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 1.0, log=True),
#         "tweedie_variance_power": trial.suggest_float("tweedie_variance_power", 1.1, 1.9),
#         "random_state": seed,
#         "verbosity": -1,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr_raw, y_va_raw = y_full_sorted_raw[tr_idx], y_full_sorted_raw[va_idx]

#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)

#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)],
#                   callbacks=[lgb.early_stopping(50, verbose=False)])

#         pred_raw = model.predict(X_va_s)
#         y_va_log = log1p_pos(y_va_raw)
#         pred_log = log1p_pos(pred_raw)
#         scores.append(smape_exp(y_va_log, pred_log))
#     return float(np.mean(scores))

# def get_or_tune_tweedie_once(bno, X_full, y_full_raw, order_index, param_dir):
#     """Tweedie ì „ìš© íŒŒë¼ë¯¸í„° (ê¸°ì¡´ í•¨ìˆ˜ì™€ ë¶„ë¦¬, ê¸°ì¡´ ë¡œì§ ë³´ì¡´)"""
#     os.makedirs(param_dir, exist_ok=True)
#     path_twd = os.path.join(param_dir, f"{bno}_twd.json")

#     # ì‹œê³„ì—´ ì •ë ¬
#     X_sorted = X_full[order_index]
#     y_sorted_raw = y_full_raw[order_index]

#     if os.path.exists(path_twd):
#         with open(path_twd, "r") as f:
#             return json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tweedie_tss(t, X_sorted, y_sorted_raw), n_trials=30)
#         best = st.best_params
#         with open(path_twd, "w") as f:
#             json.dump(best, f)
#         return best
# # ========================================================

# # ------------------------------
# # ê¸°ì¡´ íŠœë‹ í•¨ìˆ˜ë“¤ (XGB/LGB/CAT) - ì›ë³¸ ìœ ì§€
# # ------------------------------
# def tune_xgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#         "early_stopping_rounds": 50,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = XGBRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_lgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_cat_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = CatBoostRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def get_or_tune_params_once(bno, X_full, y_full, order_index, param_dir):
#     """ê±´ë¬¼ë‹¹ 1íšŒë§Œ íŠœë‹í•˜ê³  JSON ì €ì¥/ë¡œë“œ (ê¸°ì¡´ 3ëª¨ë¸)"""
#     os.makedirs(param_dir, exist_ok=True)
#     paths = {
#         "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
#         "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
#         "cat": os.path.join(param_dir, f"{bno}_cat.json"),
#     }
#     params = {}

#     # ì‹œê³„ì—´ ì •ë ¬
#     X_sorted = X_full[order_index]
#     y_sorted = y_full[order_index]

#     # XGB
#     if os.path.exists(paths["xgb"]):
#         with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["xgb"] = st.best_params
#         with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)

#     # LGB
#     if os.path.exists(paths["lgb"]):
#         with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["lgb"] = st.best_params
#         with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)

#     # CAT
#     if os.path.exists(paths["cat"]):
#         with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["cat"] = st.best_params
#         with open(paths["cat"], "w") as f: json.dump(params["cat"], f)

#     return params

# # ------------------------------
# # Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜ìœ¼ë¡œ ê±´ë¬¼ë‹¹ 1íšŒ
# # ------------------------------
# def objective_ridge_on_oof(trial, oof_meta, y_full):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full[va_idx], preds))
#     return float(np.mean(scores))

# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params")
#     os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X_full = tr_b[features].values
#     y_full_log = np.log1p(tr_b[target].values.astype(float))  # ê¸°ì¡´ 3ëª¨ë¸ìš©
#     y_full_raw = tr_b[target].values.astype(float)            # Tweedieìš©
#     X_test = te_b[features].values

#     # ì‹œê³„ì—´ ì •ë ¬ ì¸ë±ìŠ¤ (ë‚´ë¶€ íŠœë‹ìš©)
#     order = np.argsort(tr_b['ì¼ì‹œ'].values)

#     # ê¸°ì¡´ 3ëª¨ë¸ íŒŒë¼ë¯¸í„°
#     best_params = get_or_tune_params_once(bno, X_full, y_full_log, order, param_dir)
#     # Tweedie íŒŒë¼ë¯¸í„° (NEW)
#     best_twd = get_or_tune_tweedie_once(bno, X_full, y_full_raw, order, param_dir)

#     # ì™¸ë¶€ KFold(ê·¸ëŒ€ë¡œ ìœ ì§€)
#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     # ----- ì§„ì§œ OOF ìŠ¤íƒœí‚¹ ì¤€ë¹„ -----
#     n_train_b = len(tr_b)
#     n_test_b  = len(te_b)
#     base_models = ["xgb", "lgb", "cat", "twd"]   # â† Tweedie ì¶”ê°€
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)  # ê° í–‰=í›ˆë ¨í–‰, ì—´=ë² ì´ìŠ¤ëª¨ë¸
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     # í´ë“œ ë£¨í”„
#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr_log, y_va_log = y_full_log[tr_idx], y_full_log[va_idx]
#         y_tr_raw, y_va_raw = y_full_raw[tr_idx], y_full_raw[va_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         # XGB (log íƒ€ê¹ƒ)
#         xgb = XGBRegressor(**best_params["xgb"])
#         xgb.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)], verbose=False)

#         # LGB (log íƒ€ê¹ƒ, objective=MAE)
#         lgbm = LGBMRegressor(**best_params["lgb"])
#         lgbm.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)],
#                  callbacks=[lgb.early_stopping(50, verbose=False)])

#         # CAT (log íƒ€ê¹ƒ)
#         cat = CatBoostRegressor(**best_params["cat"])
#         cat.fit(X_tr_s, y_tr_log, eval_set=(X_va_s, y_va_log),
#                 early_stopping_rounds=50, verbose=0)

#         # Tweedie (ì›ì‹œ íƒ€ê¹ƒ)  â† NEW
#         twd = LGBMRegressor(**best_twd)
#         twd.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)],
#                 callbacks=[lgb.early_stopping(50, verbose=False)])

#         # ê²€ì¦ì…‹ OOF ì˜ˆì¸¡ ì €ì¥ (ëª¨ë‘ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ í†µì¼)
#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)

#         pred_raw_va_twd = twd.predict(X_va_s)
#         oof_meta[va_idx, 3] = log1p_pos(pred_raw_va_twd)   # â† ë³€í™˜

#         # í…ŒìŠ¤íŠ¸ ë©”íƒ€ ì…ë ¥ ëˆ„ì (í´ë“œ í‰ê· ) - ë¡œê·¸ ìŠ¤ì¼€ì¼
#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)

#         pred_raw_te_twd = twd.predict(X_te_s)
#         test_meta_accum[:, 3] += log1p_pos(pred_raw_te_twd)  # â† ë³€í™˜

#     test_meta = test_meta_accum / kf.get_n_splits()

#     # ----- ë©”íƒ€(Ridge) ê±´ë¬¼ë‹¹ 1íšŒ íŠœë‹/í•™ìŠµ: OOF í–‰ë ¬ ê¸°ë°˜ -----
#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")

#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full_log), n_trials=30)
#         ridge_params = st.best_params
#         with open(ridge_path, "w") as f:
#             json.dump(ridge_params, f)

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full_log)

#     # OOF ì„±ëŠ¥(ì „ì²´ ê¸°ì¤€) ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
#     oof_pred = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full_log, oof_pred))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")










####################ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ current BEST ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ



# -*- coding: utf-8 -*-
import os
import json
import random
import warnings
import datetime
import numpy as np
import pandas as pd
import optuna

from sklearn.model_selection import KFold, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from joblib import Parallel, delayed
from optuna.samplers import TPESampler

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf

warnings.filterwarnings("ignore")

# ==============================
# 0) ì‹œë“œ / ê²½ë¡œ
# ==============================
seed = 2025
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
    else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
train = pd.read_csv(os.path.join(path, "train.csv"))
test = pd.read_csv(os.path.join(path, "test.csv"))
samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
if 'buildinginfo' in globals():
    bi = buildinginfo.copy()
else:
    bi = None

if bi is not None:
    for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
        if col in bi.columns:
            bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
    bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
    bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

    keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
    for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
        if c in bi.columns: keep_cols.append(c)
    bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

    train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['hour']      = df['ì¼ì‹œ'].dt.hour
    df['day']       = df['ì¼ì‹œ'].dt.day
    df['month']     = df['ì¼ì‹œ'].dt.month
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
    df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
    df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
    df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
    df['sin_month'] = np.sin(2*np.pi*df['month']/12)
    df['cos_month'] = np.cos(2*np.pi*df['month']/12)
    df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
    df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['DI'] = 0.0
    return df

train = add_time_features_kor(train)
test  = add_time_features_kor(test)

# === 1-ì¶”ê°€) í•œêµ­ ê³µíœ´ì¼ í”¼ì²˜ (ëŒ€ì²´íœ´ì¼/ì„ ê±°ì¼ í¬í•¨)
try:
    import holidays
    def add_kr_holidays(df):
        df = df.copy()
        kr_hol = holidays.KR()
        d = df['ì¼ì‹œ'].dt.date
        df['is_holiday'] = d.map(lambda x: int(x in kr_hol))
        prev_d = (df['ì¼ì‹œ'] - pd.Timedelta(days=1)).dt.date
        next_d = (df['ì¼ì‹œ'] + pd.Timedelta(days=1)).dt.date
        df['is_pre_holiday']  = prev_d.map(lambda x: int(x in kr_hol))
        df['is_post_holiday'] = next_d.map(lambda x: int(x in kr_hol))
        daily = df.groupby(df['ì¼ì‹œ'].dt.date)['is_holiday'].max()
        daily_roll7 = daily.rolling(7, min_periods=1).sum()
        df['holiday_7d_count'] = df['ì¼ì‹œ'].dt.date.map(daily_roll7)
        dow = df['dayofweek']
        df['is_bridge_day'] = (((dow==4) & (df['is_post_holiday']==1)) | ((dow==0) & (df['is_pre_holiday']==1))).astype(int)
        return df
except Exception:
    def add_kr_holidays(df):
        df = df.copy()
        for c in ['is_holiday','is_pre_holiday','is_post_holiday','holiday_7d_count','is_bridge_day']:
            df[c] = 0
        return df

train = add_kr_holidays(train)
test  = add_kr_holidays(test)

# === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
if 'ì¼ì‚¬(MJ/m2)' in train.columns:
    solar_proxy = (
        train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
             .mean().reset_index()
             .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
    )
    train = train.merge(solar_proxy, on=['month','hour'], how='left')
    test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
else:
    train['expected_solar'] = 0.0
    test['expected_solar']  = 0.0

train['expected_solar'] = train['expected_solar'].fillna(0)
test['expected_solar']  = test['expected_solar'].fillna(0)

# === 3) ì¼ë³„ ì˜¨ë„ í†µê³„
def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
            df[c] = 0.0
        return df
    grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
    stats = grp.agg(day_max_temperature='max',
                    day_mean_temperature='mean',
                    day_min_temperature='min').reset_index()
    df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
    return df

train = add_daily_temp_stats_kor(train)
test  = add_daily_temp_stats_kor(test)

# === 4) CDH / THI / WCT
def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        df['CDH'] = 0.0
        return df
    def _cdh_1d(x):
        cs = np.cumsum(x - 26)
        return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
    parts = []
    for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
        arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
        cdh = _cdh_1d(arr)
        parts.append(pd.Series(cdh, index=g.index))
    df['CDH'] = pd.concat(parts).sort_index()
    return df

def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['THI'] = 0.0
    if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
        df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
    else:
        df['WCT'] = 0.0
    return df

train = add_CDH_kor(train)
test  = add_CDH_kor(test)
train = add_THI_WCT_kor(train)
test  = add_THI_WCT_kor(test)

# === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„(ì „ì²´ train ì§‘ê³„) - íŠœë‹/ë² ì´ìŠ¤ ì°¸ê³ ìš©
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    pm = (train
          .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
          .agg(['mean','std'])
          .reset_index()
          .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
    train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
    test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
else:
    train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
    test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# === 6) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”©
if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
    both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
    cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
    train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
    test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# ------------------------------
# Feature Set
# ------------------------------
feature_candidates = [
    # building_info
    'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
    # weather/raw
    'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
    # time parts & cycles
    'hour','day','month','dayofweek','is_weekend','is_working_hours',
    'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
    # engineered
    'DI','expected_solar',
    'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
    'CDH','THI','WCT',
    # target stats (ì „ì—­ ì§‘ê³„ - í´ë“œì—ì„œ ë®ì–´ì”€)
    'day_hour_mean','day_hour_std',
    # holidays
    'is_holiday','is_pre_holiday','is_post_holiday','holiday_7d_count','is_bridge_day'
]
features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# # íƒ€ê¹ƒ í†µê³„ ëˆ„ì„¤ ì°¨ë‹¨ìš© ì»¬ëŸ¼(í´ë“œë³„ ì¬ê³„ì‚° ê°’)ì„ featuresì— ë³´ì¥
# for c in ["hour_mean","hour_std","day_hour_mean","day_hour_std","day_hour_median","month_hour_mean"]:
#     if c not in features:
#         features.append(c)

# Target
target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
if target not in train.columns:
    raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ
X = train[features].values
y_log = np.log1p(train[target].values.astype(float))
X_test_raw = test[features].values

print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
print(f"[í™•ì¸] target: {target}")
print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y_log shape: {y_log.shape}")

# ì „ì²˜ë¦¬ ì •í•©ì„± ì ê²€
print("len(test) =", len(test))
print("len(samplesub) =", len(samplesub))
print("ê±´ë¬¼ ìˆ˜ train vs test:", train["ê±´ë¬¼ë²ˆí˜¸"].nunique(), test["ê±´ë¬¼ë²ˆí˜¸"].nunique())
counts = test.groupby("ê±´ë¬¼ë²ˆí˜¸").size()
bad = counts[counts != 168]
if len(bad):
    print("âš ï¸ 168ì´ ì•„ë‹Œ ê±´ë¬¼ ë°œê²¬:\n", bad)
assert len(test) == len(samplesub), f"test:{len(test)} sample:{len(samplesub)}"

# ------------------------------
# SMAPE helpers
# ------------------------------
def smape_exp(y_true_log, y_pred_log):
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

def smape(y, yhat):
    return np.mean(200*np.abs(yhat - y)/(np.abs(yhat)+np.abs(y)+1e-6))

# ========== Tweedie ì „ìš© ìœ í‹¸ & íŠœë‹ ==========
def log1p_pos(arr):
    return np.log1p(np.clip(arr, a_min=0, a_max=None))

def tune_lgb_tweedie_tss(trial, X_full_sorted, y_full_sorted_raw, seed=seed):
    params = {
        "objective": "tweedie",
        "metric": "mae",
        "boosting_type": "gbdt",
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "num_leaves": trial.suggest_int("num_leaves", 64, 512),
        "min_child_samples": trial.suggest_int("min_child_samples", 20, 150),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
        "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 1.0, log=True),
        "tweedie_variance_power": trial.suggest_float("tweedie_variance_power", 1.1, 1.9),
        "random_state": seed,
        "verbosity": -1,
    }
    tss = TimeSeriesSplit(n_splits=3)
    scores = []
    for tr_idx, va_idx in tss.split(X_full_sorted):
        X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
        y_tr_raw, y_va_raw = y_full_sorted_raw[tr_idx], y_full_sorted_raw[va_idx]

        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)

        model = LGBMRegressor(**params)
        model.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)],
                  callbacks=[lgb.early_stopping(50, verbose=False)])

        pred_raw = model.predict(X_va_s)
        y_va_log = log1p_pos(y_va_raw)
        pred_log = log1p_pos(pred_raw)
        scores.append(smape_exp(y_va_log, pred_log))
    return float(np.mean(scores))

def get_or_tune_tweedie_once(bno, X_full, y_full_raw, order_index, param_dir):
    os.makedirs(param_dir, exist_ok=True)
    path_twd = os.path.join(param_dir, f"{bno}_twd.json")
    X_sorted = X_full[order_index]
    y_sorted_raw = y_full_raw[order_index]
    if os.path.exists(path_twd):
        with open(path_twd, "r") as f:
            return json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_lgb_tweedie_tss(t, X_sorted, y_sorted_raw), n_trials=30)
        best = st.best_params
        with open(path_twd, "w") as f:
            json.dump(best, f)
        return best

# ------------------------------
# ê¸°ì¡´ íŠœë‹ í•¨ìˆ˜ë“¤ (XGB/LGB/CAT)
# ------------------------------
def tune_xgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "eval_metric": "mae",
        "random_state": seed,
        "objective": "reg:squarederror",
        "early_stopping_rounds": 50,
    }
    tss = TimeSeriesSplit(n_splits=3)
    scores = []
    for tr_idx, va_idx in tss.split(X_full_sorted):
        X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
        y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = XGBRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def tune_lgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "random_state": seed,
        "objective": "mae",
    }
    tss = TimeSeriesSplit(n_splits=3)
    scores = []
    for tr_idx, va_idx in tss.split(X_full_sorted):
        X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
        y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = LGBMRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def tune_cat_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
    params = {
        "iterations": trial.suggest_int("iterations", 300, 1000),
        "depth": trial.suggest_int("depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
        "random_seed": seed,
        "loss_function": "MAE",
        "verbose": 0,
    }
    tss = TimeSeriesSplit(n_splits=3)
    scores = []
    for tr_idx, va_idx in tss.split(X_full_sorted):
        X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
        y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = CatBoostRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def get_or_tune_params_once(bno, X_full, y_full, order_index, param_dir):
    os.makedirs(param_dir, exist_ok=True)
    paths = {
        "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
        "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
        "cat": os.path.join(param_dir, f"{bno}_cat.json"),
    }
    params = {}
    X_sorted = X_full[order_index]
    y_sorted = y_full[order_index]

    if os.path.exists(paths["xgb"]):
        with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted), n_trials=30)
        params["xgb"] = st.best_params
        with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)

    if os.path.exists(paths["lgb"]):
        with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted), n_trials=30)
        params["lgb"] = st.best_params
        with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)

    if os.path.exists(paths["cat"]):
        with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted), n_trials=30)
        params["cat"] = st.best_params
        with open(paths["cat"], "w") as f: json.dump(params["cat"], f)

    return params

# ------------------------------
# Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜
# ------------------------------
def objective_ridge_on_oof(trial, oof_meta, y_full):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    kf = KFold(n_splits=5, shuffle=True, random_state=seed)
    scores = []
    for tr_idx, va_idx in kf.split(oof_meta):
        ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
        preds = ridge.predict(oof_meta[va_idx])
        scores.append(smape_exp(y_full[va_idx], preds))
    return float(np.mean(scores))

# ------------------------------
# [PATCH-1] íƒ€ê¹ƒí†µê³„(ëˆ„ì„¤ ì°¨ë‹¨) ìœ í‹¸
# ------------------------------
def build_target_stats_fold(base_df, idx, target):
    base = base_df.iloc[idx]

    # ê±´ë¬¼Ã—hour í‰ê· /í‘œì¤€í¸ì°¨ë¥¼ í•œ ë²ˆì— ìƒì„±
    g1 = (base
          .groupby(["ê±´ë¬¼ë²ˆí˜¸","hour"])[target]
          .agg(hour_mean="mean", hour_std="std")
          .reset_index())

    # ê±´ë¬¼Ã—hourÃ—dow í‰ê· /í‘œì¤€í¸ì°¨/ì¤‘ì•™ê°’
    g2 = base.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"])[target]
    d_mean = g2.mean().rename("day_hour_mean").reset_index()
    d_std  = g2.std().rename("day_hour_std").reset_index()
    d_med  = g2.median().rename("day_hour_median").reset_index()

    # ê±´ë¬¼Ã—hourÃ—month í‰ê· 
    g3 = (base
          .groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","month"])[target]
          .mean()
          .rename("month_hour_mean")
          .reset_index())

    return g1, d_mean, d_std, d_med, g3

def merge_target_stats(df, stats):
    g1, d_mean, d_std, d_med, g3 = stats
    out = df.merge(g1, on=["ê±´ë¬¼ë²ˆí˜¸","hour"], how="left")  # hour_mean + hour_std ë‘˜ ë‹¤ ë¶™ìŒ
    out = out.merge(d_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
    out = out.merge(d_std,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
    out = out.merge(d_med,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
    out = out.merge(g3,     on=["ê±´ë¬¼ë²ˆí˜¸","hour","month"],    how="left")
    return out

# ------------------------------
# ê±´ë¬¼ ë‹¨ìœ„ í•™ìŠµ/ì˜ˆì¸¡
# ------------------------------
def process_building_kfold(bno):
    print(f"ğŸ¢ building {bno} KFold...")
    param_dir = os.path.join(path, "optuna_params")
    os.makedirs(param_dir, exist_ok=True)

    tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
    te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

    X_full = tr_b[features].values
    y_full_log = np.log1p(tr_b[target].values.astype(float))
    y_full_raw = tr_b[target].values.astype(float)
    X_test = te_b[features].values

    # ì‹œê³„ì—´ ì •ë ¬ ì¸ë±ìŠ¤ (íŠœë‹ìš©)
    order = np.argsort(tr_b['ì¼ì‹œ'].values)

    # ë² ì´ìŠ¤ ëª¨ë¸ íŒŒë¼ë¯¸í„° (ê±´ë¬¼ë‹¹ 1íšŒ)
    best_params = get_or_tune_params_once(bno, X_full, y_full_log, order, param_dir)
    best_twd = get_or_tune_tweedie_once(bno, X_full, y_full_raw, order, param_dir)

    # ì™¸ë¶€ KFold
    kf = KFold(n_splits=8, shuffle=True, random_state=seed)

    base_models = ["xgb", "lgb", "cat", "twd"]
    n_train_b = len(tr_b); n_test_b = len(te_b)
    oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)
    test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

    # í´ë“œ ë£¨í”„
    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
        print(f" - fold {fold}")

        # [PATCH-1] í´ë“œë³„ íƒ€ê¹ƒí†µê³„ ì¬ê³„ì‚°â†’ë¨¸ì§€ (ëˆ„ì„¤ ì°¨ë‹¨)
        stats = build_target_stats_fold(tr_b, tr_idx, target)
        tr_fold = merge_target_stats(tr_b.iloc[tr_idx].copy(), stats)
        va_fold = merge_target_stats(tr_b.iloc[va_idx].copy(), stats)
        te_fold = merge_target_stats(te_b.copy(),               stats)

        # ê²°ì¸¡ ë³´ì •
        fill_cols = ["hour_mean","hour_std","day_hour_mean","day_hour_std","day_hour_median","month_hour_mean"]

        # 1) ìš°ì„  ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ëª¨ì•„ì„œ ì „ì—­ í‰ê·  ê³„ì‚°
        present = [c for c in fill_cols if c in tr_fold.columns]
        if len(present) == 0:
            # ê·¹ë‹¨ì ìœ¼ë¡œ ì•„ë¬´ê²ƒë„ ì—†ì„ ë•Œ ëŒ€ë¹„(ê±°ì˜ ì—†ìŒ): 0.0 ì‚¬ìš©
            glob_mean = 0.0
        else:
            glob_mean = float(pd.concat([tr_fold[present]], axis=1).stack().mean())

        # 2) ì—†ëŠ” ì»¬ëŸ¼ì€ ì¦‰ì‹œ ìƒì„± í›„ ë³´ì •, ìˆëŠ” ì»¬ëŸ¼ì€ ê²°ì¸¡ë§Œ ì±„ìš°ê¸°
        for df_ in (tr_fold, va_fold, te_fold):
            for c in fill_cols:
                if c not in df_.columns:
                    df_[c] = glob_mean
                else:
                    df_[c] = df_[c].fillna(glob_mean)

        # í–‰ë ¬ êµ¬ì„±

        # [ì¶”ê°€] í´ë“œì—ì„œ ë°©ê¸ˆ ìƒì„±ëœ í†µê³„ ì»¬ëŸ¼ë“¤ë§Œ ì„ íƒì ìœ¼ë¡œ í™•ì¥
        fold_only_cols = ["hour_mean","hour_std","day_hour_mean","day_hour_std","day_hour_median","month_hour_mean"]
        features_fold = features + [c for c in fold_only_cols if c in tr_fold.columns]
        X_tr = tr_fold[features].values
        X_va = va_fold[features].values
        X_te = te_fold[features].values
        y_tr_log, y_va_log = np.log1p(tr_fold[target].values.astype(float)), np.log1p(va_fold[target].values.astype(float))
        y_tr_raw, y_va_raw = tr_fold[target].values.astype(float), va_fold[target].values.astype(float)

        sc = StandardScaler()
        X_tr_s = sc.fit_transform(X_tr)
        X_va_s = sc.transform(X_va)
        X_te_s = sc.transform(X_te)

        # XGB (log íƒ€ê¹ƒ)
        xgb = XGBRegressor(**best_params["xgb"])
        xgb.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)], verbose=False)

        # LGB (log íƒ€ê¹ƒ)
        lgbm = LGBMRegressor(**best_params["lgb"])
        lgbm.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)], callbacks=[lgb.early_stopping(50, verbose=False)])

        # CAT (log íƒ€ê¹ƒ)
        cat = CatBoostRegressor(**best_params["cat"])
        cat.fit(X_tr_s, y_tr_log, eval_set=(X_va_s, y_va_log), early_stopping_rounds=50, verbose=0)

        # Tweedie (ì›ì‹œ íƒ€ê¹ƒ)
        twd = LGBMRegressor(**best_twd)
        twd.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)], callbacks=[lgb.early_stopping(50, verbose=False)])

        # OOF ì €ì¥(ëª¨ë‘ ë¡œê·¸ ìŠ¤ì¼€ì¼ í†µì¼)
        oof_meta[va_idx, 0] = xgb.predict(X_va_s)
        oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
        oof_meta[va_idx, 2] = cat.predict(X_va_s)
        pred_raw_va_twd = twd.predict(X_va_s)
        oof_meta[va_idx, 3] = log1p_pos(pred_raw_va_twd)

        # í…ŒìŠ¤íŠ¸ ë©”íƒ€ ëˆ„ì 
        test_meta_accum[:, 0] += xgb.predict(X_te_s)
        test_meta_accum[:, 1] += lgbm.predict(X_te_s)
        test_meta_accum[:, 2] += cat.predict(X_te_s)
        pred_raw_te_twd = twd.predict(X_te_s)
        test_meta_accum[:, 3] += log1p_pos(pred_raw_te_twd)

    test_meta = test_meta_accum / kf.get_n_splits()

    # ----- ë©”íƒ€(Ridge) íŠœë‹/í•™ìŠµ
    ridge_key = f"{bno}_ridge"
    ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
    if os.path.exists(ridge_path):
        with open(ridge_path, "r") as f:
            ridge_params = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full_log), n_trials=30)
        ridge_params = st.best_params
        with open(ridge_path, "w") as f:
            json.dump(ridge_params, f)

    meta = Ridge(alpha=ridge_params["alpha"])
    meta.fit(oof_meta, y_full_log)

    # ----- OOF ì„±ëŠ¥, Smearing ë³´ì •, SMAPE ì¹¼ë¦¬ë¸Œë ˆì´ì…˜
    oof_pred_log = meta.predict(oof_meta)
    avg_smape = float(smape_exp(y_full_log, oof_pred_log))

    # [PATCH-3] Smearing
    resid = y_full_log - oof_pred_log
    S = float(np.mean(np.exp(resid)))

    # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ (ë¡œê·¸â†’ì›ë³µ + Smearing)
    te_pred_log = meta.predict(test_meta)
    te_pred = np.expm1(te_pred_log) * S

    # [PATCH-4] ë‹¨ì¡° ì¹¼ë¦¬ë¸Œë ˆì´ì…˜ g(p)=aÂ·p^b (OOF ê¸°ë°˜)
    y_oof = np.expm1(y_full_log)
    p_oof = np.expm1(oof_pred_log) * S
    a_grid = np.linspace(0.8, 1.2, 21)
    b_grid = np.linspace(0.85, 1.15, 31)
    best = (1.0, 1.0, smape(y_oof, p_oof))
    for a in a_grid:
        for b in b_grid:
            s = smape(y_oof, a*(p_oof**b))
            if s < best[2]:
                best = (a, b, s)
    a_opt, b_opt, _ = best
    te_pred = a_opt * (te_pred ** b_opt)

    return te_pred.tolist(), avg_smape

# ==============================
# 12) ë³‘ë ¬ ì‹¤í–‰ (test ê±´ë¬¼ ê¸°ì¤€) + ìˆœì„œ ë§¤í•‘
# ==============================
bld_list = list(np.sort(test["ê±´ë¬¼ë²ˆí˜¸"].unique()))
results = Parallel(n_jobs=-1, backend="loky")(
    delayed(process_building_kfold)(bno) for bno in bld_list
)

preds_full = np.zeros(len(test), dtype=float)
val_smapes = []
for bno, (preds, sm) in zip(bld_list, results):
    idx = (test["ê±´ë¬¼ë²ˆí˜¸"] == bno).values
    assert idx.sum() == len(preds), f"building {bno}: test rows={idx.sum()}, preds={len(preds)}"
    preds_full[idx] = preds
    if not np.isnan(sm):
        val_smapes.append(sm)

assert len(preds_full) == len(samplesub), f"final preds:{len(preds_full)}, sample:{len(samplesub)}"
samplesub["answer"] = preds_full

today = datetime.datetime.now().strftime("%Y%m%d")
avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
filename = f"submission_stack_PATCHED_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")


























# ### ë‚´ ì˜µíŠœë‚˜ íŒŒë¼ë¯¸í„°ë¡œ ëŒë¦¬ëŠ” ì „ì²˜ë¦¬ 4ë“±êº¼ ì¶”ê°€ëœ ì½”ë“œ #########
# # -*- coding: utf-8 -*-
# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import KFold, TimeSeriesSplit
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ
# # ==============================
# seed = 2025
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 1.5) ê¸°ìƒ ê²°ì¸¡ ë‹¤ë‹¨ê³„ ë³´ê°„ (ê±´ë¬¼Ã—ì›”Ã—ì‹œ â†’ ê±´ë¬¼Ã—ì‹œ â†’ ì‹œ â†’ ì „ì²´)
# def fill_weather_missing(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if "ê°•ìˆ˜ëŸ‰(mm)" in df.columns:
#         df["ê°•ìˆ˜ëŸ‰(mm)"] = df["ê°•ìˆ˜ëŸ‰(mm)"].fillna(0)
#     for col in ["í’ì†(m/s)", "ìŠµë„(%)"]:
#         if col not in df.columns: 
#             continue
#         lvl1 = df.groupby(["ê±´ë¬¼ë²ˆí˜¸","month","hour"])[col].transform("mean")
#         x = df[col].copy().fillna(lvl1)

#         mask = x.isna()
#         if mask.any():
#             lvl2 = df.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour"])[col].transform("mean")
#             x = x.where(~mask, lvl2)

#         x = pd.Series(x, index=df.index); mask = x.isna()
#         if mask.any():
#             lvl3 = df.groupby(["hour"])[col].transform("mean")
#             x = x.where(~mask, lvl3)

#         df[col] = pd.Series(x, index=df.index).fillna(df[col].mean())
#     return df

# train = fill_weather_missing(train)
# test  = fill_weather_missing(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ merge)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH êµì • / THI êµì •+êµ¬ê°„í™” / WCT ë‹¨ìœ„ ë³´ì •
# def add_CDH_fixed(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         z  = x - 26.0
#         cs = np.concatenate([[0.0], np.cumsum(z)])
#         win = 12
#         head = np.cumsum(z)[:min(win-1, len(z))]
#         if len(z) >= win:
#             tail = cs[win:] - cs[:-win]
#             return np.concatenate([head, tail])
#         else:
#             return head
#     parts = []
#     for _, g in df.groupby('ê±´ë¬¼ë²ˆí˜¸', sort=False):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_binned_and_WCT(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     # THI (ì„­ì”¨ ê¸°ë°˜) + êµ¬ê°„í™”
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         thi = t - (0.55 - 0.0055*h) * (t - 14.5)
#         df['DI']  = thi                          # ì—°ì†ê°’(ì°¸ê³ ìš©)
#         df['THI'] = pd.cut(thi, [0,68,75,80,200], labels=[1,2,3,4]).astype(int)
#     else:
#         df['DI'] = 0.0
#         df['THI'] = 1
#     # WCT (í’ì† m/s â†’ km/h)
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = (df['í’ì†(m/s)']*3.6).clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_fixed(train)
# test  = add_CDH_fixed(test)
# train = add_THI_binned_and_WCT(train)
# test  = add_THI_binned_and_WCT(test)

# # === 5) íƒ€ê¹ƒ í†µê³„ í™•ì¥ (ê±´ë¬¼Ã—hour, ê±´ë¬¼Ã—hourÃ—ìš”ì¼(median í¬í•¨), ê±´ë¬¼Ã—hourÃ—month)
# def add_target_stats(train_df, test_df, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"):
#     tr = train_df.copy(); te = test_df.copy()

#     g1 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour"])[target]
#     h_mean = g1.mean().rename("hour_mean").reset_index()
#     h_std  = g1.std().rename("hour_std").reset_index()

#     g2 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"])[target]
#     d_mean = g2.mean().rename("day_hour_mean").reset_index()
#     d_std  = g2.std().rename("day_hour_std").reset_index()
#     d_med  = g2.median().rename("day_hour_median").reset_index()

#     g3 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","month"])[target]
#     m_mean = g3.mean().rename("month_hour_mean").reset_index()

#     def merge_all(df):
#         df = df.merge(h_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour"], how="left")
#         df = df.merge(h_std,  on=["ê±´ë¬¼ë²ˆí˜¸","hour"], how="left")
#         df = df.merge(d_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(d_std,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(d_med,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(m_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour","month"],  how="left")
#         return df

#     tr = merge_all(tr)
#     te = merge_all(te)

#     # NaN ë³´ì •(í¬ì†Œ ì¡°í•© ëŒ€ë¹„)
#     fill_cols = ["hour_mean","hour_std","day_hour_mean","day_hour_std","day_hour_median","month_hour_mean"]
#     for c in fill_cols:
#         glob = float(tr[c].mean())
#         tr[c] = tr[c].fillna(glob)
#         te[c] = te[c].fillna(glob)

#     return tr, te

# train, test = add_target_stats(train, test, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)")

# # === 6) KMeans êµ°ì§‘ + êµ°ì§‘Ã—ì‹œê°„ í†µê³„
# from sklearn.cluster import KMeans
# def add_cluster_features(train_df, test_df, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)", n_clusters=5, seed=42):
#     tmp = train_df.copy()
#     tmp["is_weekend_tmp"] = (tmp["dayofweek"] >= 5).astype(int)
#     wk = tmp[tmp["is_weekend_tmp"]==0].pivot_table(values=target, index="ê±´ë¬¼ë²ˆí˜¸", columns="hour", aggfunc="mean")
#     we = tmp[tmp["is_weekend_tmp"]==1].pivot_table(values=target, index="ê±´ë¬¼ë²ˆí˜¸", columns="hour", aggfunc="mean")
#     patt = pd.merge(wk, we, left_index=True, right_index=True, how="left").fillna(method="ffill").fillna(method="bfill")

#     kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init="auto")
#     labs = kmeans.fit_predict(patt.values)
#     cl = pd.DataFrame({"ê±´ë¬¼ë²ˆí˜¸": patt.index, "cluster": labs})

#     tr = train_df.merge(cl, on="ê±´ë¬¼ë²ˆí˜¸", how="left")
#     te = test_df.merge(cl,  on="ê±´ë¬¼ë²ˆí˜¸", how="left")

#     grp = tr.groupby(["cluster","hour","dayofweek"])[target].mean().rename("cluster_day_hour_mean").reset_index()
#     tr = tr.merge(grp, on=["cluster","hour","dayofweek"], how="left")
#     te = te.merge(grp,  on=["cluster","hour","dayofweek"],  how="left")

#     return tr, te

# train, test = add_cluster_features(train, test, n_clusters=5, seed=seed)

# # === 7) ê¸°ì˜¨/ìŠµë„ ì´ë™í‰ê· (4ì¼Â·7ì¼) - testëŠ” train-tail ë¶™ì—¬ ê²½ê³„ ì•ˆì •í™”
# def add_rollings(train_df, test_df, windows=(96,168)):
#     tr = train_df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()
#     te = test_df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()

#     for w in windows:
#         tr[f"ê¸°ì˜¨_{w//24}ì¼_ì´ë™í‰ê· "] = tr.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].transform(lambda s: s.rolling(w,1).mean())
#         tr[f"ìŠµë„_{w//24}ì¼_ì´ë™í‰ê· "] = tr.groupby("ê±´ë¬¼ë²ˆí˜¸")["ìŠµë„(%)"].transform(lambda s: s.rolling(w,1).mean())

#     out=[]
#     for b, te_part in te.groupby("ê±´ë¬¼ë²ˆí˜¸"):
#         tr_part = tr[tr["ê±´ë¬¼ë²ˆí˜¸"]==b]
#         cat = pd.concat([tr_part, te_part]).sort_values("ì¼ì‹œ")
#         for w in windows:
#             cat[f"ê¸°ì˜¨_{w//24}ì¼_ì´ë™í‰ê· "] = cat["ê¸°ì˜¨(Â°C)"].rolling(w,1).mean()
#             cat[f"ìŠµë„_{w//24}ì¼_ì´ë™í‰ê· "] = cat["ìŠµë„(%)"].rolling(w,1).mean()
#         out.append(cat.loc[te_part.index, :])
#     te = pd.concat(out).sort_index()
#     return tr, te

# train, test = add_rollings(train, test, windows=(96,168))

# # === 8) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±° (ì› ì½”ë“œ ìœ ì§€)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # === 9) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # ------------------------------
# # Feature Set
# # ------------------------------
# # ê¸°ì¡´ í›„ë³´ + í™•ì¥ í†µê³„/êµ°ì§‘/ë¡¤ë§ê¹Œì§€ í¬í•¨
# feature_candidates = [
#     # building_info
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     # weather/raw
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     # time parts & cycles
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     # engineered
#     'expected_solar','DI','THI','WCT','CDH',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     # target stats (ê¸°ì¡´ + í™•ì¥)
#     'day_hour_mean','day_hour_std',           # ê¸°ì¡´(ìš”ì¼ ê¸°ì¤€)
#     'hour_mean','hour_std','day_hour_median','month_hour_mean',  # í™•ì¥
#     # cluster
#     'cluster','cluster_day_hour_mean',
#     # rollings
#     'ê¸°ì˜¨_4ì¼_ì´ë™í‰ê· ','ìŠµë„_4ì¼_ì´ë™í‰ê· ','ê¸°ì˜¨_7ì¼_ì´ë™í‰ê· ','ìŠµë„_7ì¼_ì´ë™í‰ê· '
# ]
# # train/test ê³µí†µ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
# features = [c for c in feature_candidates if (c in train.columns and c in test.columns)]

# # Target
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ
# X = train[features].values
# y_log = np.log1p(train[target].values.astype(float))   # ê¸°ì¡´ 3ëª¨ë¸ìš© (ë¡œê·¸ íƒ€ê¹ƒ)
# y_raw = train[target].values.astype(float)             # Tweedieìš©
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y_log shape: {y_log.shape}")

# # ------------------------------
# # SMAPE helpers
# # ------------------------------
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# def log1p_pos(arr):
#     return np.log1p(np.clip(arr, a_min=0, a_max=None))

# # ========== Tweedie ì „ìš© íŠœë‹ ==========
# def tune_lgb_tweedie_tss(trial, X_full_sorted, y_full_sorted_raw, seed=seed):
#     params = {
#         "objective": "tweedie",
#         "metric": "mae",
#         "boosting_type": "gbdt",
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "num_leaves": trial.suggest_int("num_leaves", 64, 512),
#         "min_child_samples": trial.suggest_int("min_child_samples", 20, 150),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
#         "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 1.0, log=True),
#         "tweedie_variance_power": trial.suggest_float("tweedie_variance_power", 1.1, 1.9),
#         "random_state": seed,
#         "verbosity": -1,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr_raw, y_va_raw = y_full_sorted_raw[tr_idx], y_full_sorted_raw[va_idx]

#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)

#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)],
#                   callbacks=[lgb.early_stopping(50, verbose=False)])

#         pred_raw = model.predict(X_va_s)
#         y_va_log = log1p_pos(y_va_raw)
#         pred_log = log1p_pos(pred_raw)
#         scores.append(smape_exp(y_va_log, pred_log))
#     return float(np.mean(scores))

# def get_or_tune_tweedie_once(bno, X_full, y_full_raw, order_index, param_dir):
#     os.makedirs(param_dir, exist_ok=True)
#     path_twd = os.path.join(param_dir, f"{bno}_twd.json")

#     X_sorted = X_full[order_index]
#     y_sorted_raw = y_full_raw[order_index]

#     if os.path.exists(path_twd):
#         with open(path_twd, "r") as f:
#             return json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tweedie_tss(t, X_sorted, y_sorted_raw), n_trials=30)
#         best = st.best_params
#         with open(path_twd, "w") as f:
#             json.dump(best, f)
#         return best

# # ------------------------------
# # ê¸°ì¡´ íŠœë‹ í•¨ìˆ˜ë“¤ (XGB/LGB/CAT)
# # ------------------------------
# def tune_xgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#         "early_stopping_rounds": 50,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = XGBRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_lgb_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = LGBMRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def tune_cat_tss(trial, X_full_sorted, y_full_sorted, seed=seed):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     tss = TimeSeriesSplit(n_splits=3)
#     scores = []
#     for tr_idx, va_idx in tss.split(X_full_sorted):
#         X_tr, X_va = X_full_sorted[tr_idx], X_full_sorted[va_idx]
#         y_tr, y_va = y_full_sorted[tr_idx], y_full_sorted[va_idx]
#         sc = StandardScaler().fit(X_tr)
#         X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
#         model = CatBoostRegressor(**params)
#         model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
#         pred = model.predict(X_va_s)
#         scores.append(smape_exp(y_va, pred))
#     return float(np.mean(scores))

# def get_or_tune_params_once(bno, X_full, y_full, order_index, param_dir):
#     os.makedirs(param_dir, exist_ok=True)
#     paths = {
#         "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
#         "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
#         "cat": os.path.join(param_dir, f"{bno}_cat.json"),
#     }
#     params = {}

#     X_sorted = X_full[order_index]
#     y_sorted = y_full[order_index]

#     # XGB
#     if os.path.exists(paths["xgb"]):
#         with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_xgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["xgb"] = st.best_params
#         with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)

#     # LGB
#     if os.path.exists(paths["lgb"]):
#         with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_lgb_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["lgb"] = st.best_params
#         with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)

#     # CAT
#     if os.path.exists(paths["cat"]):
#         with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: tune_cat_tss(t, X_sorted, y_sorted), n_trials=30)
#         params["cat"] = st.best_params
#         with open(paths["cat"], "w") as f: json.dump(params["cat"], f)

#     return params

# # ------------------------------
# # Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜
# # ------------------------------
# def objective_ridge_on_oof(trial, oof_meta, y_full):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full[va_idx], preds))
#     return float(np.mean(scores))

# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params")
#     os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X_full = tr_b[features].values
#     y_full_log = np.log1p(tr_b[target].values.astype(float))  # ê¸°ì¡´ 3ëª¨ë¸ìš©
#     y_full_raw = tr_b[target].values.astype(float)            # Tweedieìš©
#     X_test = te_b[features].values

#     order = np.argsort(tr_b['ì¼ì‹œ'].values)

#     best_params = get_or_tune_params_once(bno, X_full, y_full_log, order, param_dir)
#     best_twd = get_or_tune_tweedie_once(bno, X_full, y_full_raw, order, param_dir)

#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     base_models = ["xgb", "lgb", "cat", "twd"]
#     n_train_b = len(tr_b); n_test_b  = len(te_b)
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr_log, y_va_log = y_full_log[tr_idx], y_full_log[va_idx]
#         y_tr_raw, y_va_raw = y_full_raw[tr_idx], y_full_raw[va_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         xgb = XGBRegressor(**best_params["xgb"])
#         xgb.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)], verbose=False)

#         lgbm = LGBMRegressor(**best_params["lgb"])
#         lgbm.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)],
#                  callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat = CatBoostRegressor(**best_params["cat"])
#         cat.fit(X_tr_s, y_tr_log, eval_set=(X_va_s, y_va_log),
#                 early_stopping_rounds=50, verbose=0)

#         twd = LGBMRegressor(**best_twd)
#         twd.fit(X_tr_s, y_tr_raw, eval_set=[(X_va_s, y_va_raw)],
#                 callbacks=[lgb.early_stopping(50, verbose=False)])

#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)
#         oof_meta[va_idx, 3] = log1p_pos(twd.predict(X_va_s))

#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)
#         test_meta_accum[:, 3] += log1p_pos(twd.predict(X_te_s))

#     test_meta = test_meta_accum / kf.get_n_splits()

#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")

#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full_log), n_trials=30)
#         ridge_params = st.best_params
#         with open(ridge_path, "w") as f:
#             json.dump(ridge_params, f)

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full_log)

#     oof_pred = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full_log, oof_pred))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")




# # -*- coding: utf-8 -*-
# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna  # Ridge(ë©”íƒ€)ë§Œ íŠœë‹ ìš©ë„

# from sklearn.model_selection import KFold
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ
# # ==============================
# seed = 2025
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 1.5) ê¸°ìƒ ê²°ì¸¡ ë‹¤ë‹¨ê³„ ë³´ê°„ (ê±´ë¬¼Ã—ì›”Ã—ì‹œ â†’ ê±´ë¬¼Ã—ì‹œ â†’ ì‹œ â†’ ì „ì²´)
# def fill_weather_missing(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if "ê°•ìˆ˜ëŸ‰(mm)" in df.columns:
#         df["ê°•ìˆ˜ëŸ‰(mm)"] = df["ê°•ìˆ˜ëŸ‰(mm)"].fillna(0)
#     for col in ["í’ì†(m/s)", "ìŠµë„(%)"]:
#         if col not in df.columns:
#             continue
#         lvl1 = df.groupby(["ê±´ë¬¼ë²ˆí˜¸","month","hour"])[col].transform("mean")
#         x = df[col].copy().fillna(lvl1)

#         mask = x.isna()
#         if mask.any():
#             lvl2 = df.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour"])[col].transform("mean")
#             x = x.where(~mask, lvl2)

#         x = pd.Series(x, index=df.index); mask = x.isna()
#         if mask.any():
#             lvl3 = df.groupby(["hour"])[col].transform("mean")
#             x = x.where(~mask, lvl3)

#         df[col] = pd.Series(x, index=df.index).fillna(df[col].mean())
#     return df

# train = fill_weather_missing(train)
# test  = fill_weather_missing(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ merge)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH / THI(êµ¬ê°„í™”) / WCT
# def add_CDH_fixed(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         z  = x - 26.0
#         cs = np.concatenate([[0.0], np.cumsum(z)])
#         win = 12
#         head = np.cumsum(z)[:min(win-1, len(z))]
#         if len(z) >= win:
#             tail = cs[win:] - cs[:-win]
#             return np.concatenate([head, tail])
#         else:
#             return head
#     parts = []
#     for _, g in df.groupby('ê±´ë¬¼ë²ˆí˜¸', sort=False):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_binned_and_WCT(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         thi = t - (0.55 - 0.0055*h) * (t - 14.5)
#         df['DI']  = thi
#         df['THI'] = pd.cut(thi, [0,68,75,80,200], labels=[1,2,3,4]).astype(int)
#     else:
#         df['DI'] = 0.0
#         df['THI'] = 1
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = (df['í’ì†(m/s)']*3.6).clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_fixed(train)
# test  = add_CDH_fixed(test)
# train = add_THI_binned_and_WCT(train)
# test  = add_THI_binned_and_WCT(test)

# # === 5) íƒ€ê¹ƒ í†µê³„ (ê±´ë¬¼Ã—hour / ìš”ì¼ / month)
# def add_target_stats(train_df, test_df, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)"):
#     tr = train_df.copy(); te = test_df.copy()

#     g1 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour"])[target]
#     h_mean = g1.mean().rename("hour_mean").reset_index()
#     h_std  = g1.std().rename("hour_std").reset_index()

#     g2 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"])[target]
#     d_mean = g2.mean().rename("day_hour_mean").reset_index()
#     d_std  = g2.std().rename("day_hour_std").reset_index()
#     d_med  = g2.median().rename("day_hour_median").reset_index()

#     g3 = tr.groupby(["ê±´ë¬¼ë²ˆí˜¸","hour","month"])[target]
#     m_mean = g3.mean().rename("month_hour_mean").reset_index()

#     def merge_all(df):
#         df = df.merge(h_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour"], how="left")
#         df = df.merge(h_std,  on=["ê±´ë¬¼ë²ˆí˜¸","hour"], how="left")
#         df = df.merge(d_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(d_std,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(d_med,  on=["ê±´ë¬¼ë²ˆí˜¸","hour","dayofweek"], how="left")
#         df = df.merge(m_mean, on=["ê±´ë¬¼ë²ˆí˜¸","hour","month"],  how="left")
#         return df

#     tr = merge_all(tr)
#     te = merge_all(te)

#     fill_cols = ["hour_mean","hour_std","day_hour_mean","day_hour_std","day_hour_median","month_hour_mean"]
#     for c in fill_cols:
#         glob = float(tr[c].mean())
#         tr[c] = tr[c].fillna(glob)
#         te[c] = te[c].fillna(glob)

#     return tr, te

# train, test = add_target_stats(train, test, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)")

# # === 6) KMeans êµ°ì§‘ + êµ°ì§‘Ã—ì‹œê°„ í†µê³„
# from sklearn.cluster import KMeans
# def add_cluster_features(train_df, test_df, target="ì „ë ¥ì†Œë¹„ëŸ‰(kWh)", n_clusters=5, seed=42):
#     tmp = train_df.copy()
#     tmp["is_weekend_tmp"] = (tmp["dayofweek"] >= 5).astype(int)
#     wk = tmp[tmp["is_weekend_tmp"]==0].pivot_table(values=target, index="ê±´ë¬¼ë²ˆí˜¸", columns="hour", aggfunc="mean")
#     we = tmp[tmp["is_weekend_tmp"]==1].pivot_table(values=target, index="ê±´ë¬¼ë²ˆí˜¸", columns="hour", aggfunc="mean")
#     patt = pd.merge(wk, we, left_index=True, right_index=True, how="left").fillna(method="ffill").fillna(method="bfill")

#     kmeans = KMeans(n_clusters=n_clusters, random_state=seed, n_init="auto")
#     labs = kmeans.fit_predict(patt.values)
#     cl = pd.DataFrame({"ê±´ë¬¼ë²ˆí˜¸": patt.index, "cluster": labs})

#     tr = train_df.merge(cl, on="ê±´ë¬¼ë²ˆí˜¸", how="left")
#     te = test_df.merge(cl,  on="ê±´ë¬¼ë²ˆí˜¸", how="left")

#     grp = tr.groupby(["cluster","hour","dayofweek"])[target].mean().rename("cluster_day_hour_mean").reset_index()
#     tr = tr.merge(grp, on=["cluster","hour","dayofweek"], how="left")
#     te = te.merge(grp,  on=["cluster","hour","dayofweek"],  how="left")

#     return tr, te

# train, test = add_cluster_features(train, test, n_clusters=5, seed=seed)

# # === 7) ê¸°ì˜¨/ìŠµë„ ì´ë™í‰ê· (4ì¼Â·7ì¼) - ì•ˆì „ ìŠ¬ë¼ì´ì‹± ë²„ì „
# def add_rollings(train_df, test_df, windows=(96,168)):
#     tr = train_df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()
#     te = test_df.sort_values(["ê±´ë¬¼ë²ˆí˜¸","ì¼ì‹œ"]).copy()

#     for w in windows:
#         tr[f"ê¸°ì˜¨_{w//24}ì¼_ì´ë™í‰ê· "] = tr.groupby("ê±´ë¬¼ë²ˆí˜¸")["ê¸°ì˜¨(Â°C)"].transform(lambda s: s.rolling(w,1).mean())
#         tr[f"ìŠµë„_{w//24}ì¼_ì´ë™í‰ê· "] = tr.groupby("ê±´ë¬¼ë²ˆí˜¸")["ìŠµë„(%)"].transform(lambda s: s.rolling(w,1).mean())

#     out = []
#     for b, te_part in te.groupby("ê±´ë¬¼ë²ˆí˜¸"):
#         tr_part = tr[tr["ê±´ë¬¼ë²ˆí˜¸"] == b]
#         # ì¸ë±ìŠ¤ ì¶©ëŒ ë°©ì§€ + ìœ„ì¹˜ê¸°ë°˜ ìŠ¬ë¼ì´ì‹±
#         cat = pd.concat([tr_part, te_part], axis=0, ignore_index=True)
#         for w in windows:
#             cat[f"ê¸°ì˜¨_{w//24}ì¼_ì´ë™í‰ê· "] = cat["ê¸°ì˜¨(Â°C)"].rolling(w,1).mean()
#             cat[f"ìŠµë„_{w//24}ì¼_ì´ë™í‰ê· "] = cat["ìŠµë„(%)"].rolling(w,1).mean()
#         tail = cat.iloc[-len(te_part):].copy()
#         # ì›ë˜ te_part ì¸ë±ìŠ¤ë¡œ ë³µêµ¬í•´ ì „ì²´ te ì •ë ¬ ìœ ì§€
#         tail.index = te_part.index
#         out.append(tail)

#     te_fixed = pd.concat(out, axis=0).sort_index()
#     return tr, te_fixed

# train, test = add_rollings(train, test, windows=(96,168))

# # === 8) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # === 9) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# # ------------------------------
# # Feature Set
# # ------------------------------
# feature_candidates = [
#     # building_info
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     # weather/raw
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     # time parts & cycles
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     # engineered
#     'expected_solar','DI','THI','WCT','CDH',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     # target stats
#     'day_hour_mean','day_hour_std','hour_mean','hour_std','day_hour_median','month_hour_mean',
#     # cluster
#     'cluster','cluster_day_hour_mean',
#     # rollings
#     'ê¸°ì˜¨_4ì¼_ì´ë™í‰ê· ','ìŠµë„_4ì¼_ì´ë™í‰ê· ','ê¸°ì˜¨_7ì¼_ì´ë™í‰ê· ','ìŠµë„_7ì¼_ì´ë™í‰ê· '
# ]
# features = [c for c in feature_candidates if (c in train.columns and c in test.columns)]

# # Target
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ
# X = train[features].values
# y_log = np.log1p(train[target].values.astype(float))   # ë² ì´ìŠ¤ ëª¨ë¸ìš© (ë¡œê·¸ íƒ€ê¹ƒ)
# X_test_raw = test[features].values
# ts = train['ì¼ì‹œ']

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y_log shape: {y_log.shape}")

# # ì „ì²˜ë¦¬ ì •í•©ì„± ì ê²€
# print("len(test) =", len(test))
# print("len(samplesub) =", len(samplesub))
# print("ê±´ë¬¼ ìˆ˜ train vs test:", train["ê±´ë¬¼ë²ˆí˜¸"].nunique(), test["ê±´ë¬¼ë²ˆí˜¸"].nunique())
# # ê±´ë¬¼ë³„ í–‰ ìˆ˜(ëª¨ë‘ 168ì´ì–´ì•¼ ì •ìƒ)
# counts = test.groupby("ê±´ë¬¼ë²ˆí˜¸").size()
# bad = counts[counts != 168]
# if len(bad):
#     print("âš ï¸ 168ì´ ì•„ë‹Œ ê±´ë¬¼ ë°œê²¬:\n", bad)
# # ê¸¸ì´ ì‚¬ì „ì ê²€ (ì—¬ê¸°ì„œ ë°”ë¡œ ì‹¤íŒ¨ì‹œí‚¤ë©´ ì›ì¸ ì¶”ì  ì‰¬ì›€)
# assert len(test) == len(samplesub), f"test:{len(test)} sample:{len(samplesub)}"

# # ------------------------------
# # SMAPE helpers
# # ------------------------------
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# # ------------------------------
# # ê³ ì • í•˜ì´í¼íŒŒë¼ë¯¸í„°
# # ------------------------------
# xgb_params = dict(
#     random_state=seed,
#     n_estimators=4682,
#     max_leaves=101,
#     min_child_weight=7.581207558322951,
#     learning_rate=0.08979034933474227,
#     subsample=0.8905280196300354,
#     colsample_bylevel=1.0,
#     colsample_bytree=0.9523645407001878,
#     reg_alpha=0.006919296411231538,
#     reg_lambda=0.0998936254543762,
#     n_jobs=-1,
# )
# lgbm_params = dict(
#     random_state=seed,
#     n_estimators=15000,
#     num_leaves=8,
#     min_child_samples=12,
#     learning_rate=0.17010396907527026,
#     colsample_bytree=0.9605563464803123,
#     reg_alpha=0.1110993344544235,
#     reg_lambda=0.7948637803974561,
#     verbose=-1,
#     n_jobs=-1,
# )
# cat_params = dict(
#     learning_rate=0.14059048492476106,
#     loss_function="RMSE",
#     random_seed=seed,
#     verbose=False,
#     n_estimators=10000,
#     early_stopping_rounds=100,
#     objective="MAE",
# )

# # ------------------------------
# # Ridge íŠœë‹(ë©”íƒ€) - OOF í–‰ë ¬ ê¸°ë°˜
# # ------------------------------
# def objective_ridge_on_oof(trial, oof_meta, y_full):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
#     scores = []
#     for tr_idx, va_idx in kf.split(oof_meta):
#         ridge.fit(oof_meta[tr_idx], y_full[tr_idx])
#         preds = ridge.predict(oof_meta[va_idx])
#         scores.append(smape_exp(y_full[va_idx], preds))
#     return float(np.mean(scores))

# def process_building_kfold(bno):
#     print(f"ğŸ¢ building {bno} KFold...")

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     # testê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
#     if len(te_b) == 0:
#         return [], np.nan

#     X_full = tr_b[features].values
#     y_full_log = np.log1p(tr_b[target].values.astype(float))
#     X_test = te_b[features].values

#     # ì™¸ë¶€ KFold
#     kf = KFold(n_splits=8, shuffle=True, random_state=seed)

#     base_models = ["xgb", "lgb", "cat"]
#     n_train_b = len(tr_b); n_test_b  = len(te_b)
#     oof_meta = np.zeros((n_train_b, len(base_models)), dtype=float)
#     test_meta_accum = np.zeros((n_test_b, len(base_models)), dtype=float)

#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X_full[tr_idx], X_full[va_idx]
#         y_tr_log, y_va_log = y_full_log[tr_idx], y_full_log[va_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         # ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ í•™ìŠµ
#         xgb = XGBRegressor(**xgb_params)
#         xgb.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)], verbose=False)

#         lgbm = LGBMRegressor(**lgbm_params)
#         lgbm.fit(X_tr_s, y_tr_log, eval_set=[(X_va_s, y_va_log)],
#                  callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat = CatBoostRegressor(**cat_params)
#         cat.fit(X_tr_s, y_tr_log, eval_set=(X_va_s, y_va_log),
#                 early_stopping_rounds=100, verbose=False)

#         # OOF ì €ì¥
#         oof_meta[va_idx, 0] = xgb.predict(X_va_s)
#         oof_meta[va_idx, 1] = lgbm.predict(X_va_s)
#         oof_meta[va_idx, 2] = cat.predict(X_va_s)

#         # Test ë©”íƒ€ ëˆ„ì 
#         test_meta_accum[:, 0] += xgb.predict(X_te_s)
#         test_meta_accum[:, 1] += lgbm.predict(X_te_s)
#         test_meta_accum[:, 2] += cat.predict(X_te_s)

#     test_meta = test_meta_accum / kf.get_n_splits()

#     # ----- ë©”íƒ€(Ridge) ê±´ë¬¼ë‹¹ íŠœë‹/í•™ìŠµ -----
#     param_dir = os.path.join(path, "optuna_params_fixed")
#     os.makedirs(param_dir, exist_ok=True)
#     ridge_key = f"{bno}_ridge"
#     ridge_path = os.path.join(param_dir, f"{ridge_key}.json")

#     if os.path.exists(ridge_path):
#         with open(ridge_path, "r") as f:
#             ridge_params = json.load(f)
#     else:
#         st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#         st.optimize(lambda t: objective_ridge_on_oof(t, oof_meta, y_full_log), n_trials=30)
#         ridge_params = st.best_params
#         with open(ridge_path, "w") as f:
#             json.dump(ridge_params, f)

#     meta = Ridge(alpha=ridge_params["alpha"])
#     meta.fit(oof_meta, y_full_log)

#     # OOF ì„±ëŠ¥ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡
#     oof_pred = meta.predict(oof_meta)
#     avg_smape = float(smape_exp(y_full_log, oof_pred))

#     te_pred_log = meta.predict(test_meta)
#     te_pred = np.expm1(te_pred_log)

#     return te_pred.tolist(), avg_smape

# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰ (test ê±´ë¬¼ ê¸°ì¤€) + ìˆœì„œ ë§¤í•‘
# # ==============================
# bld_list = list(np.sort(test["ê±´ë¬¼ë²ˆí˜¸"].unique()))
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in bld_list
# )

# # test ì›ë˜ ìˆœì„œì— ì§ì ‘ ë§¤í•‘
# preds_full = np.zeros(len(test), dtype=float)
# val_smapes = []
# for bno, (preds, sm) in zip(bld_list, results):
#     idx = (test["ê±´ë¬¼ë²ˆí˜¸"] == bno).values
#     # ê±´ë¬¼ë³„ ê¸¸ì´ ì •í•© ì²´í¬ (ê° ê±´ë¬¼ 168í–‰ ê°€ì •)
#     assert idx.sum() == len(preds), f"building {bno}: test rows={idx.sum()}, preds={len(preds)}"
#     preds_full[idx] = preds
#     if not np.isnan(sm):
#         val_smapes.append(sm)

# # ìµœì¢… ê¸¸ì´/ì •ë ¬ ì •í•©ì„±
# assert len(preds_full) == len(samplesub), f"final preds:{len(preds_full)}, sample:{len(samplesub)}"

# samplesub["answer"] = preds_full
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
# filename = f"submission_stack_FIXED_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")