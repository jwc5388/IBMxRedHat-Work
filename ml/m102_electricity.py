
import os
import pandas as pd
import numpy as np
import random
import datetime
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf
import warnings
from joblib import Parallel, delayed
from sklearn.model_selection import KFold
from optuna.samplers import TPESampler
import json
warnings.filterwarnings(action='ignore')

# Seed ê³ ì •
seed = 6054
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# ê²½ë¡œ ì„¤ì •
BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
buildinginfo = pd.read_csv(path + 'building_info.csv')
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
samplesub = pd.read_csv(path + 'sample_submission.csv')

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
    buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# Feature Engineering
def feature_engineering(df):
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
    df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
        if col in df.columns:
            df[col] = df[col].fillna(0)
    temp = df['ê¸°ì˜¨(Â°C)']
    humidity = df['ìŠµë„(%)']
    df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
    return df

train = feature_engineering(train)
test = feature_engineering(test)
train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# ðŸ’¡ [ê°œì„  ì „ëžµ] 'ì¼ì‚¬ëŸ‰' ëŒ€ë¦¬ ë³€ìˆ˜ ìƒì„±
# 1. í›ˆë ¨ ë°ì´í„°ì—ì„œ ì›”(month)ê³¼ ì‹œê°„(hour)ë³„ í‰ê·  ì¼ì‚¬ëŸ‰ ê³„ì‚°
solar_proxy = train.groupby(['month', 'hour'])['ì¼ì‚¬(MJ/m2)'].mean().reset_index()
solar_proxy.rename(columns={'ì¼ì‚¬(MJ/m2)': 'expected_solar'}, inplace=True)

# 2. trainê³¼ test ë°ì´í„°ì— 'expected_solar' í”¼ì²˜ë¥¼ ë³‘í•©
train = train.merge(solar_proxy, on=['month', 'hour'], how='left')
test = test.merge(solar_proxy, on=['month', 'hour'], how='left')

# ë³‘í•© ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìžˆëŠ” ì†ŒëŸ‰ì˜ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
train['expected_solar'] = train['expected_solar'].fillna(0)
test['expected_solar'] = test['expected_solar'].fillna(0)


train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

features = [
    'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
    'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
    'hour', 'dayofweek', 'month', 'day', 'is_weekend',
    'is_working_hours', 'sin_hour', 'cos_hour', 'DI', 'expected_solar'
]

target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# Optuna íŠœë‹ í•¨ìˆ˜ë“¤
def tune_xgb(trial, x_train, y_train, x_val, y_val):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'early_stopping_rounds': 50,
        'eval_metric': 'mae',
        'random_state': seed,
        'objective': 'reg:squarederror'
    }
    model = XGBRegressor(**params)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
    pred = model.predict(x_val)
    smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
                    (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
    return smape

def tune_lgb(trial, x_train, y_train, x_val, y_val):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': seed,
        'objective': 'mae'
    }
    model = LGBMRegressor(**params)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
              callbacks=[lgb.early_stopping(50, verbose=False)])
    pred = model.predict(x_val)
    smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
                    (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
    return smape

def tune_cat(trial, x_train, y_train, x_val, y_val):
    params = {
        'iterations': trial.suggest_int('iterations', 300, 1000),
        'depth': trial.suggest_int('depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
        'random_seed': seed,
        'loss_function': 'MAE',
        'verbose': 0
    }
    model = CatBoostRegressor(**params)
    model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
    pred = model.predict(x_val)
    smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
                    (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
    return smape

def objective(trial, oof_train, oof_val, y_train, y_val):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    ridge.fit(oof_train, y_train)
    preds = ridge.predict(oof_val)
    smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
                    (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
    return smape


def process_building_kfold(bno):
    print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} KFold ì²˜ë¦¬ ì¤‘...")
    param_dir = os.path.join(path, "optuna_params")  # âœ… ì¶”ê°€
    os.makedirs(param_dir, exist_ok=True)   
    train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
    test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
    x = train_b[features].values
    y = np.log1p(train_b[target].values)
    x_test = test_b[features].values

    kf = KFold(n_splits=5, shuffle=True, random_state=seed)
    
    test_preds = []
    val_smapes = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(x)):
        print(f" - Fold {fold+1}")
        x_train, x_val = x[train_idx], x[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        scaler = StandardScaler()
        x_train_scaled = scaler.fit_transform(x_train)
        x_val_scaled = scaler.transform(x_val)
        x_test_scaled = scaler.transform(x_test)

        # ê¸°ì¡´ ì½”ë“œ ì§€ìš°ê³  ì•„ëž˜ë¡œ êµì²´
        model_key = f"{bno}_fold{fold+1}_xgb"
        xgb_param_path = os.path.join(param_dir, f"{model_key}.json")

        if os.path.exists(xgb_param_path):
            with open(xgb_param_path, "r") as f:
                xgb_params = json.load(f)
        else:
            xgb_study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
            xgb_params = xgb_study.best_params
            with open(xgb_param_path, "w") as f:
                json.dump(xgb_params, f)

        best_xgb = XGBRegressor(**xgb_params)
        best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

        model_key = f"{bno}_fold{fold+1}_lgb"
        lgb_param_path = os.path.join(param_dir, f"{model_key}.json")

        if os.path.exists(lgb_param_path):
            with open(lgb_param_path, "r") as f:
                lgb_params = json.load(f)
        else:
            lgb_study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
            lgb_params = lgb_study.best_params
            with open(lgb_param_path, "w") as f:
                json.dump(lgb_params, f)

        best_lgb = LGBMRegressor(**lgb_params)
        best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
                    callbacks=[lgb.early_stopping(50, verbose=False)])

        model_key = f"{bno}_fold{fold+1}_cat"
        cat_param_path = os.path.join(param_dir, f"{model_key}.json")

        if os.path.exists(cat_param_path):
            with open(cat_param_path, "r") as f:
                cat_params = json.load(f)
        else:
            cat_study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
            cat_params = cat_study.best_params
            with open(cat_param_path, "w") as f:
                json.dump(cat_params, f)

        best_cat = CatBoostRegressor(**cat_params)
        best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

        # Stackingìš© ì˜ˆì¸¡ê°’ ìƒì„±
        oof_train = np.vstack([
            best_xgb.predict(x_train_scaled),
            best_lgb.predict(x_train_scaled),
            best_cat.predict(x_train_scaled)
        ]).T
        oof_val = np.vstack([
            best_xgb.predict(x_val_scaled),
            best_lgb.predict(x_val_scaled),
            best_cat.predict(x_val_scaled)
        ]).T
        oof_test = np.vstack([
            best_xgb.predict(x_test_scaled),
            best_lgb.predict(x_test_scaled),
            best_cat.predict(x_test_scaled)
        ]).T

        # Ridge íŠœë‹ ë° í•™ìŠµ
        model_key = f"{bno}_fold{fold+1}_ridge"
        ridge_param_path = os.path.join(param_dir, f"{model_key}.json")

        if os.path.exists(ridge_param_path):
            with open(ridge_param_path, "r") as f:
                ridge_params = json.load(f)
        else:
            ridge_study = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
            ridge_params = ridge_study.best_params
            with open(ridge_param_path, "w") as f:
                json.dump(ridge_params, f)

        meta = Ridge(alpha=ridge_params['alpha'])
        meta.fit(oof_train, y_train)

        val_pred = meta.predict(oof_val)
        test_pred = meta.predict(oof_test)

        smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
                        (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))

        val_smapes.append(smape)
        test_preds.append(np.expm1(test_pred))

    # í‰ê·  ì˜ˆì¸¡ê°’ê³¼ SMAPE
    avg_test_pred = np.mean(test_preds, axis=0)
    avg_smape = np.mean(val_smapes)

    return avg_test_pred.tolist(), avg_smape


# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (KFold ì ìš©)
results = Parallel(n_jobs=-1, backend='loky')(
    delayed(process_building_kfold)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
)

# ê²°ê³¼ í•©ì¹˜ê¸°
final_preds = []
val_smapes = []
for preds, smape in results:
    final_preds.extend(preds)
    val_smapes.append(smape)

samplesub['answer'] = final_preds
today = datetime.datetime.now().strftime('%Y%m%d')
avg_smape = np.mean(val_smapes)
filename = f"submission_stack_optuna_{today}_SMAPE_ì¼ì‚¬_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")




#  ------------------------------í˜„ìž¬ ë² ìŠ¤íŠ¸!!!!!!!!!!!!!!!!!--------------------------------------------

# #ì•„ëž˜êº¼ì— ì¼ì‚¬ ì¶”ê°€. ìž˜ ë´ë¼ 


# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# from joblib import Parallel, delayed
# from sklearn.model_selection import KFold

# warnings.filterwarnings(action='ignore')

# # Seed ê³ ì •
# seed = 707
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
#     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
#     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#         if col in df.columns:
#             df[col] = df[col].fillna(0)
#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# train = feature_engineering(train)
# test = feature_engineering(test)
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # ðŸ’¡ [ê°œì„  ì „ëžµ] 'ì¼ì‚¬ëŸ‰' ëŒ€ë¦¬ ë³€ìˆ˜ ìƒì„±
# # 1. í›ˆë ¨ ë°ì´í„°ì—ì„œ ì›”(month)ê³¼ ì‹œê°„(hour)ë³„ í‰ê·  ì¼ì‚¬ëŸ‰ ê³„ì‚°
# solar_proxy = train.groupby(['month', 'hour'])['ì¼ì‚¬(MJ/m2)'].mean().reset_index()
# solar_proxy.rename(columns={'ì¼ì‚¬(MJ/m2)': 'expected_solar'}, inplace=True)

# # 2. trainê³¼ test ë°ì´í„°ì— 'expected_solar' í”¼ì²˜ë¥¼ ë³‘í•©
# train = train.merge(solar_proxy, on=['month', 'hour'], how='left')
# test = test.merge(solar_proxy, on=['month', 'hour'], how='left')

# # ë³‘í•© ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìžˆëŠ” ì†ŒëŸ‰ì˜ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar'] = test['expected_solar'].fillna(0)


# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI', 'expected_solar'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # Optuna íŠœë‹ í•¨ìˆ˜ë“¤
# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'early_stopping_rounds': 50,
#         'eval_metric': 'mae',
#         'random_state': seed,
#         'objective': 'reg:squarederror'
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'random_state': seed,
#         'objective': 'mae'
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
#               callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'iterations': trial.suggest_int('iterations', 300, 1000),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
#         'random_seed': seed,
#         'loss_function': 'MAE',
#         'verbose': 0
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def objective(trial, oof_train, oof_val, y_train, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_train, y_train)
#     preds = ridge.predict(oof_val)
#     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape


# def process_building_kfold(bno):
#     print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} KFold ì²˜ë¦¬ ì¤‘...")
#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features].values
#     y = np.log1p(train_b[target].values)
#     x_test = test_b[features].values

#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)
    
#     test_preds = []
#     val_smapes = []

#     for fold, (train_idx, val_idx) in enumerate(kf.split(x)):
#         print(f" - Fold {fold+1}")
#         x_train, x_val = x[train_idx], x[val_idx]
#         y_train, y_val = y[train_idx], y[val_idx]

#         scaler = StandardScaler()
#         x_train_scaled = scaler.fit_transform(x_train)
#         x_val_scaled = scaler.transform(x_val)
#         x_test_scaled = scaler.transform(x_test)

#         # ê° ëª¨ë¸ íŠœë‹ ë° í›ˆë ¨
#         xgb_study = optuna.create_study(direction="minimize")
#         xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#         best_xgb = XGBRegressor(**xgb_study.best_params)
#         best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#         lgb_study = optuna.create_study(direction="minimize")
#         lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#         best_lgb = LGBMRegressor(**lgb_study.best_params)
#         best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat_study = optuna.create_study(direction="minimize")
#         cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#         best_cat = CatBoostRegressor(**cat_study.best_params)
#         best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

#         # Stackingìš© ì˜ˆì¸¡ê°’ ìƒì„±
#         oof_train = np.vstack([
#             best_xgb.predict(x_train_scaled),
#             best_lgb.predict(x_train_scaled),
#             best_cat.predict(x_train_scaled)
#         ]).T
#         oof_val = np.vstack([
#             best_xgb.predict(x_val_scaled),
#             best_lgb.predict(x_val_scaled),
#             best_cat.predict(x_val_scaled)
#         ]).T
#         oof_test = np.vstack([
#             best_xgb.predict(x_test_scaled),
#             best_lgb.predict(x_test_scaled),
#             best_cat.predict(x_test_scaled)
#         ]).T

#         # Ridge íŠœë‹ ë° í•™ìŠµ
#         ridge_study = optuna.create_study(direction="minimize")
#         ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
#         meta = Ridge(alpha=ridge_study.best_params['alpha'])
#         meta.fit(oof_train, y_train)

#         val_pred = meta.predict(oof_val)
#         test_pred = meta.predict(oof_test)

#         smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                         (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))

#         val_smapes.append(smape)
#         test_preds.append(np.expm1(test_pred))

#     # í‰ê·  ì˜ˆì¸¡ê°’ê³¼ SMAPE
#     avg_test_pred = np.mean(test_preds, axis=0)
#     avg_smape = np.mean(val_smapes)

#     return avg_test_pred.tolist(), avg_smape


# # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (KFold ì ìš©)
# results = Parallel(n_jobs=-1, backend='loky')(
#     delayed(process_building_kfold)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# )

# # ê²°ê³¼ í•©ì¹˜ê¸°
# final_preds = []
# val_smapes = []
# for preds, smape in results:
#     final_preds.extend(preds)
#     val_smapes.append(smape)

# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# filename = f"submission_stack_optuna_{today}_SMAPE_ì¼ì‚¬_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")






















#ì–˜ëŠ” ëª¨ë‘ ì˜µíŠœë‚˜ í•œê±° !! ì„±ëŠ¥ ë§¤ìš° ì¢‹ìŒ

# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# from joblib import Parallel, delayed

# warnings.filterwarnings(action='ignore')

# # Seed ê³ ì •
# seed = 4464
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
#     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
#     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#         if col in df.columns:
#             df[col] = df[col].fillna(0)
#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# train = feature_engineering(train)
# test = feature_engineering(test)
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # Optuna íŠœë‹ í•¨ìˆ˜ë“¤
# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'early_stopping_rounds': 50,
#         'eval_metric': 'mae',
#         'random_state': seed,
#         'objective': 'reg:squarederror'
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'random_state': seed,
#         'objective': 'mae'
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
#               callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'iterations': trial.suggest_int('iterations', 300, 1000),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
#         'random_seed': seed,
#         'loss_function': 'MAE',
#         'verbose': 0
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def objective(trial, oof_train, oof_val, y_train, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_train, y_train)
#     preds = ridge.predict(oof_val)
#     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def process_building(bno):
#     print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...") 
#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features]
#     y = np.log1p(train_b[target])
#     x_test = test_b[features]

#     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)
#     scaler = StandardScaler()
#     x_train_scaled = scaler.fit_transform(x_train)
#     x_val_scaled = scaler.transform(x_val)
#     x_test_scaled = scaler.transform(x_test)

#     xgb_study = optuna.create_study(direction="minimize")
#     xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_xgb = XGBRegressor(**xgb_study.best_params)
#     best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#     lgb_study = optuna.create_study(direction="minimize")
#     lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_lgb = LGBMRegressor(**lgb_study.best_params)
#     best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

#     cat_study = optuna.create_study(direction="minimize")
#     cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_cat = CatBoostRegressor(**cat_study.best_params)
#     best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

#     oof_train = np.vstack([
#         best_xgb.predict(x_train_scaled),
#         best_lgb.predict(x_train_scaled),
#         best_cat.predict(x_train_scaled)
#     ]).T
#     oof_val = np.vstack([
#         best_xgb.predict(x_val_scaled),
#         best_lgb.predict(x_val_scaled),
#         best_cat.predict(x_val_scaled)
#     ]).T
#     oof_test = np.vstack([
#         best_xgb.predict(x_test_scaled),
#         best_lgb.predict(x_test_scaled),
#         best_cat.predict(x_test_scaled)
#     ]).T

#     ridge_study = optuna.create_study(direction="minimize")
#     ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
#     meta = Ridge(alpha=ridge_study.best_params['alpha'])
#     meta.fit(oof_train, y_train)

#     val_pred = meta.predict(oof_val)
#     test_pred = meta.predict(oof_test)

#     smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))

#     return np.expm1(test_pred).tolist(), smape

# # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
# results = Parallel(n_jobs=-1, backend='loky')(
#     delayed(process_building)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# )

# # ê²°ê³¼ í•©ì¹˜ê¸°
# final_preds = []
# val_smapes = []
# for preds, smape in results:
#     final_preds.extend(preds)
#     val_smapes.append(smape)

# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# filename = f"submission_stack_optuna_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")
























# ìµœì¢…ëª¨ë¸ë§Œ optuna - ëª¨ë‘ ì˜µíŠœë‚˜ í•œê²Œ ì„±ëŠ¥ ë” ì¢‹ìŒ. ì´ê±´ í˜¹ì‹œë‚˜ í•´ì„œ ê°–ê³ ìžˆëŠ”



# # import os
# # import pandas as pd
# # import numpy as np
# # import random
# # import datetime
# # import optuna
# # from sklearn.model_selection import train_test_split
# # from sklearn.preprocessing import StandardScaler
# # from sklearn.linear_model import Ridge
# # from sklearn.metrics import mean_absolute_error
# # from xgboost import XGBRegressor
# # from lightgbm import LGBMRegressor
# # from catboost import CatBoostRegressor
# # import lightgbm as lgb
# # import tensorflow as tf
# # import warnings
# # warnings.filterwarnings(action='ignore')


# # #best 707
# # # Seed ê³ ì •
# # seed = 6054
# # random.seed(seed)
# # np.random.seed(seed)
# # tf.random.set_seed(seed)

# # # ê²½ë¡œ ì„¤ì •
# # if os.path.exists('/workspace/TensorJae/Study25/'):
# #     BASE_PATH = '/workspace/TensorJae/Study25/'
# # else:
# #     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# # buildinginfo = pd.read_csv(path + 'building_info.csv')
# # train = pd.read_csv(path + 'train.csv')
# # test = pd.read_csv(path + 'test.csv')
# # samplesub = pd.read_csv(path + 'sample_submission.csv')

# # # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # # Feature Engineering
# # def feature_engineering(df):
# #     df = df.copy()
# #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
# #     df['month'] = df['ì¼ì‹œ'].dt.month
# #     df['day'] = df['ì¼ì‹œ'].dt.day
# #     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
# #     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
# #     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
# #     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
# #     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
# #         if col in df.columns:
# #             df[col] = df[col].fillna(0)
# #     temp = df['ê¸°ì˜¨(Â°C)']
# #     humidity = df['ìŠµë„(%)']
# #     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
# #     return df

# # train = feature_engineering(train)
# # test = feature_engineering(test)
# # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # features = [
# #     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
# #     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
# #     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
# #     'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# # ]

# # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# # final_preds = []
# # val_smapes = []

# # # Optuna íŠœë‹ í•¨ìˆ˜
# # def objective(trial, oof_train, oof_val, y_train, y_val):
# #     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
# #     ridge = Ridge(alpha=alpha)
# #     ridge.fit(oof_train, y_train)
# #     preds = ridge.predict(oof_val)
# #     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
# #                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
# #     return smape

# # # ê±´ë¬¼ë³„ ëª¨ë¸ í•™ìŠµ
# # building_ids = train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# # for bno in building_ids:
# #     print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...")

# #     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
# #     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
# #     x = train_b[features]
# #     y = np.log1p(train_b[target])
# #     x_test_final = test_b[features]

# #     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# #     scaler = StandardScaler()
# #     x_train_scaled = scaler.fit_transform(x_train)
# #     x_val_scaled = scaler.transform(x_val)
# #     x_test_final_scaled = scaler.transform(x_test_final)

# #     # Base ëª¨ë¸ í•™ìŠµ
# #     xgb = XGBRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
# #                        random_state=seed, early_stopping_rounds=50, objective='reg:squarederror')
# #     xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

# #     lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
# #                               random_state=seed, objective='mae')
# #     lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
# #                   callbacks=[lgb.early_stopping(50, verbose=False)])

# #     cat = CatBoostRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
# #                             random_seed=seed, verbose=0, loss_function='MAE')
# #     cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

# #     # ìŠ¤íƒœí‚¹
# #     oof_train_lvl1 = np.vstack([
# #         xgb.predict(x_train_scaled),
# #         lgb_model.predict(x_train_scaled),
# #         cat.predict(x_train_scaled)
# #     ]).T
# #     oof_val_lvl1 = np.vstack([
# #         xgb.predict(x_val_scaled),
# #         lgb_model.predict(x_val_scaled),
# #         cat.predict(x_val_scaled)
# #     ]).T
# #     oof_test_lvl1 = np.vstack([
# #         xgb.predict(x_test_final_scaled),
# #         lgb_model.predict(x_test_final_scaled),
# #         cat.predict(x_test_final_scaled)
# #     ]).T

# #     # Optunaë¡œ Ridge íŠœë‹
# #     study = optuna.create_study(direction="minimize")
# #     study.optimize(lambda trial: objective(trial, oof_train_lvl1, oof_val_lvl1, y_train, y_val), n_trials=30)
# #     best_alpha = study.best_params['alpha']
# #     meta_model = Ridge(alpha=best_alpha)
# #     meta_model.fit(oof_train_lvl1, y_train)

# #     val_pred = meta_model.predict(oof_val_lvl1)
# #     test_pred = meta_model.predict(oof_test_lvl1)

# #     val_smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
# #                         (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))
# #     val_smapes.append(val_smape)

# #     pred = np.expm1(test_pred)
# #     final_preds.extend(pred)

# # # ê²°ê³¼ ì €ìž¥
# # samplesub['answer'] = final_preds
# # today = datetime.datetime.now().strftime('%Y%m%d')
# # avg_smape = np.mean(val_smapes)
# # score_str = f"{avg_smape:.4f}".replace('.', '_')
# # filename = f"submission_stack_optuna_{today}_SMAPE_{score_str}_{seed}.csv"
# # samplesub.to_csv(os.path.join(path, filename), index=False)

# # print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# # print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")



