# # # import os
# # # import pandas as pd
# # # import numpy as np
# # # from sklearn.preprocessing import StandardScaler
# # # from sklearn.model_selection import train_test_split
# # # from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor
# # # from sklearn.metrics import mean_absolute_error
# # # from xgboost import XGBRegressor
# # # from catboost import CatBoostRegressor
# # # from lightgbm import LGBMRegressor
# # # import random
# # # import tensorflow as tf

# # # # ê³ ì • ì‹œë“œ
# # # seed = 42
# # # random.seed(seed)
# # # np.random.seed(seed)
# # # tf.random.set_seed(seed)

# # # # ê²½ë¡œ ì„¤ì •
# # # if os.path.exists('/workspace/TensorJae/Study25/'):
# # #     BASE_PATH = '/workspace/TensorJae/Study25/'
# # # else:
# # #     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# # # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# # # save_path = os.path.join(path, '_save/')

# # # # ë°ì´í„° ë¡œë“œ
# # # buildinginfo = pd.read_csv(path + 'building_info.csv')
# # # train = pd.read_csv(path + 'train.csv')
# # # test = pd.read_csv(path + 'test.csv')
# # # samplesubmission = pd.read_csv(path + 'sample_submission.csv')

# # # # ë‚ ì§œ íŒŒì‹± ë° ì‹œê°„/ìš”ì¼ íŒŒìƒë³€ìˆ˜ ì¶”ê°€
# # # for df in [train, test]:
# # #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# # #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# # #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek

# # # # building_infoì˜ '-' ê°’ì„ ê²°ì¸¡ì¹˜ë¡œ ì²˜ë¦¬ í›„ float ë³€í™˜
# # # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# # #     buildinginfo[col] = pd.to_numeric(buildinginfo[col].replace('-', np.nan))

# # # # train/testì— building_info ë³‘í•©
# # # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # # # ë²”ì£¼í˜• -> ìˆ«ìí˜• ë³€í™˜
# # # for df in [train, test]:
# # #     df['ê±´ë¬¼ìœ í˜•'] = df['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # # # ê³µí†µ feature ì •ì˜ (test ê¸°ì¤€)
# # # test_features = ['ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
# # #                  'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
# # #                  'ìŠµë„(%)', 'hour', 'dayofweek']

# # # # train featureëŠ” 'ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)' í¬í•¨ë˜ë¯€ë¡œ ë”°ë¡œ ì •ì˜
# # # train_features = test_features + ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']
# # # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # # x = train[train_features]
# # # y = train[target]
# # # x_test_final = test[test_features]

# # # # ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
# # # x = x.fillna(0)
# # # x_test_final = x_test_final.fillna(0)

# # # # í•™ìŠµ/ê²€ì¦ ë¶„ë¦¬
# # # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# # # # ìŠ¤ì¼€ì¼ë§ (test featureì— ë§ì¶°ì„œë§Œ ì ìš©)
# # # scaler = StandardScaler()
# # # x_train_scaled = scaler.fit_transform(x_train[test_features])
# # # x_val_scaled = scaler.transform(x_val[test_features])
# # # x_test_final_scaled = scaler.transform(x_test_final)

# # # # ë¶€ìŠ¤íŒ… ëª¨ë¸ ì •ì˜
# # # xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=seed)
# # # lgb_model = LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=seed)
# # # cat_model = CatBoostRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=seed, verbose=0)

# # # # ìŠ¤íƒœí‚¹ ì•™ìƒë¸” ì •ì˜
# # # stack_model = StackingRegressor(
# # #     estimators=[
# # #         ('xgb', xgb_model),
# # #         ('lgb', lgb_model),
# # #         ('cat', cat_model),
# # #     ],
# # #     final_estimator=GradientBoostingRegressor(n_estimators=100, random_state=seed),
# # #     n_jobs=-1
# # # )

# # # # ëª¨ë¸ í•™ìŠµ
# # # stack_model.fit(x_train_scaled, y_train)

# # # # ê²€ì¦ ì˜ˆì¸¡ ë° SMAPE
# # # val_pred = stack_model.predict(x_val_scaled)
# # # def smape(y_true, y_pred):
# # #     denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0
# # #     diff = np.abs(y_true - y_pred) / denominator
# # #     diff[denominator == 0] = 0.0
# # #     return np.mean(diff) * 100

# # # print(f"âœ… ê²€ì¦ SMAPE: {smape(y_val, val_pred):.4f}")

# # # # í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ í›„ ì œì¶œ
# # # final_pred = stack_model.predict(x_test_final_scaled)
# # # samplesubmission['answer'] = final_pred
# # # filename = f"submission_stack_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv"
# # # samplesubmission.to_csv(os.path.join(path, filename), index=False)
# # # print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")


# # # import os
# # # import pandas as pd
# # # import numpy as np
# # # from sklearn.preprocessing import StandardScaler
# # # from sklearn.model_selection import train_test_split
# # # from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor
# # # from sklearn.metrics import mean_absolute_error
# # # from xgboost import XGBRegressor
# # # from catboost import CatBoostRegressor
# # # from lightgbm import LGBMRegressor
# # # import random
# # # import tensorflow as tf
# # # import datetime

# # # # Seed ê³ ì •
# # # seed = 42
# # # random.seed(seed)
# # # np.random.seed(seed)
# # # tf.random.set_seed(seed)

# # # # ê²½ë¡œ ì„¤ì •
# # # if os.path.exists('/workspace/TensorJae/Study25/'):
# # #     BASE_PATH = '/workspace/TensorJae/Study25/'
# # # else:
# # #     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# # # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# # # save_path = os.path.join(path, '_save/')

# # # # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# # # buildinginfo = pd.read_csv(path + 'building_info.csv')
# # # train = pd.read_csv(path + 'train.csv')
# # # test = pd.read_csv(path + 'test.csv')
# # # samplesub = pd.read_csv(path + 'sample_submission.csv')

# # # # ê²°ì¸¡ì¹˜ ì²˜ë¦¬: '-' â†’ 0, float ë³€í™˜
# # # cols_to_clean = ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']
# # # for col in cols_to_clean:
# # #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # # # ë‚ ì§œ íŒŒì‹± ë° íŒŒìƒë³€ìˆ˜
# # # for df in [train, test]:
# # #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# # #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# # #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek

# # # # ê±´ë¬¼ ì •ë³´ ë³‘í•©
# # # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # # # ë²”ì£¼í˜• ì²˜ë¦¬
# # # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # # # í”¼ì²˜ ì„¤ì •
# # # test_features = ['ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
# # #                  'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
# # #                  'ìŠµë„(%)', 'hour', 'dayofweek']
# # # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # # # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
# # # x = train[test_features]
# # # y = np.log1p(train[target])
# # # x_test_final = test[test_features]

# # # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# # # # ìŠ¤ì¼€ì¼ë§
# # # scaler = StandardScaler()
# # # x_train_scaled = scaler.fit_transform(x_train)
# # # x_val_scaled = scaler.transform(x_val)
# # # x_test_final_scaled = scaler.transform(x_test_final)

# # # # ëª¨ë¸ ì •ì˜
# # # xgb = XGBRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed)
# # # lgb = LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed)
# # # cat = CatBoostRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed, verbose=0)

# # # stack_model = StackingRegressor(
# # #     estimators=[
# # #         ('xgb', xgb),
# # #         ('lgb', lgb),
# # #         ('cat', cat),
# # #     ],
# # #     final_estimator=GradientBoostingRegressor(n_estimators=350, random_state=seed),
# # #     n_jobs=-1
# # # )

# # # # í•™ìŠµ
# # # stack_model.fit(x_train_scaled, y_train)

# # # # ì˜ˆì¸¡ (ë¡œê·¸ ë³µì›)
# # # y_pred = np.expm1(stack_model.predict(x_val_scaled))
# # # y_true = np.expm1(y_val)

# # # # SMAPE ê³„ì‚°
# # # def smape(y_true, y_pred):
# # #     return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))

# # # print(f"\nâœ… ê²€ì¦ SMAPE: {smape(y_true, y_pred):.4f}")

# # # # ì œì¶œ
# # # # ì˜ˆì¸¡ ë° ë¡œê·¸ ë³µì›
# # # final_pred = np.expm1(stack_model.predict(x_test_final_scaled))
# # # samplesub['answer'] = final_pred

# # # # ì˜¤ëŠ˜ ë‚ ì§œ
# # # today = datetime.datetime.now().strftime('%Y%m%d')

# # # # ê²€ì¦ SMAPE ì ìˆ˜ ê³„ì‚°
# # # val_smape = smape(y_true, y_pred)
# # # score_str = f"{val_smape:.4f}".replace('.', '_')

# # # # íŒŒì¼ëª… ìƒì„±
# # # filename = f"submission_{today}_SMAPE_{score_str}.csv"
# # # file_path = os.path.join(path, filename)

# # # # ì €ì¥
# # # samplesub.to_csv(file_path, index=False)
# # # print(f"ğŸ“ {filename} ì €ì¥ ì™„ë£Œ!")

























# # import os
# # import pandas as pd
# # import numpy as np
# # from sklearn.model_selection import train_test_split
# # from sklearn.preprocessing import StandardScaler
# # from sklearn.ensemble import GradientBoostingRegressor
# # from sklearn.linear_model import RidgeCV
# # from sklearn.metrics import mean_absolute_error
# # from xgboost import XGBRegressor
# # from lightgbm import LGBMRegressor
# # from catboost import CatBoostRegressor
# # import random
# # import tensorflow as tf
# # import datetime

# # # Seed ê³ ì •
# # seed = 42
# # random.seed(seed)
# # np.random.seed(seed)
# # tf.random.set_seed(seed)

# # # ê²½ë¡œ ì„¤ì •
# # if os.path.exists('/workspace/TensorJae/Study25/'):
# #     BASE_PATH = '/workspace/TensorJae/Study25/'
# # else:
# #     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# # save_path = os.path.join(path, '_save/')

# # # ë°ì´í„° ë¡œë“œ
# # buildinginfo = pd.read_csv(path + 'building_info.csv')
# # train = pd.read_csv(path + 'train.csv')
# # test = pd.read_csv(path + 'test.csv')
# # samplesub = pd.read_csv(path + 'sample_submission.csv')

# # # '-' ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° float ë³€í™˜
# # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # # ë‚ ì§œ íŒŒì‹± ë° íŒŒìƒë³€ìˆ˜
# # for df in [train, test]:
# #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek

# # # ë³‘í•©
# # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # # ë²”ì£¼í˜• â†’ ìˆ˜ì¹˜í˜•
# # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # # Feature ì •ì˜
# # test_features = ['ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
# #                  'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
# #                  'ìŠµë„(%)', 'hour', 'dayofweek']
# # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # # ë°ì´í„° ë¶„í•  ë° ì „ì²˜ë¦¬
# # x = train[test_features]
# # y = np.log1p(train[target])
# # x_test_final = test[test_features]

# # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# # scaler = StandardScaler()
# # x_train_scaled = scaler.fit_transform(x_train)
# # x_val_scaled = scaler.transform(x_val)
# # x_test_final_scaled = scaler.transform(x_test_final)

# # # âœ… ë² ì´ìŠ¤ ëª¨ë¸ ì •ì˜ + ì¡°ê¸° ì¢…ë£Œ
# # xgb_model = XGBRegressor(
# #     n_estimators=1000,
# #     learning_rate=0.05,
# #     max_depth=6,
# #     random_state=seed,
# #     early_stopping_rounds=50,
# #     objective='reg:squarederror'
# # )
# # xgb_model.fit(
# #     x_train_scaled, y_train,
# #     eval_set=[(x_val_scaled, y_val)],
# #     verbose=False
# # )

# # lgb_model = LGBMRegressor(
# #     n_estimators=1000,
# #     learning_rate=0.05,
# #     max_depth=6,
# #     random_state=seed,
# #     objective='mae'
# # )
# # lgb_model.fit(
# #     x_train_scaled, y_train,
# #     eval_set=[(x_val_scaled, y_val)],
# #     early_stopping_rounds=50,
# #     verbose=-1
# # )

# # cat_model = CatBoostRegressor(
# #     n_estimators=1000,
# #     learning_rate=0.05,
# #     max_depth=6,
# #     random_seed=seed,
# #     verbose=0,
# #     loss_function='MAE'
# # )
# # cat_model.fit(
# #     x_train_scaled, y_train,
# #     eval_set=(x_val_scaled, y_val),
# #     early_stopping_rounds=50
# # )

# # # âœ… ìŠ¤íƒœí‚¹ìš© ì˜ˆì¸¡ ê²°ê³¼
# # oof_train = np.vstack([
# #     xgb_model.predict(x_train_scaled),
# #     lgb_model.predict(x_train_scaled),
# #     cat_model.predict(x_train_scaled)
# # ]).T

# # oof_val = np.vstack([
# #     xgb_model.predict(x_val_scaled),
# #     lgb_model.predict(x_val_scaled),
# #     cat_model.predict(x_val_scaled)
# # ]).T

# # oof_test = np.vstack([
# #     xgb_model.predict(x_test_final_scaled),
# #     lgb_model.predict(x_test_final_scaled),
# #     cat_model.predict(x_test_final_scaled)
# # ]).T

# # # âœ… ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
# # from sklearn.ensemble import GradientBoostingRegressor

# # meta_model = GradientBoostingRegressor(
# #     n_estimators=700, learning_rate=0.05, max_depth=3, random_state=seed
# # )
# # meta_model.fit(oof_train, y_train)

# # # ê²€ì¦ ì˜ˆì¸¡
# # val_pred = meta_model.predict(oof_val)
# # y_val_exp = np.expm1(y_val)
# # val_pred_exp = np.expm1(val_pred)

# # # SMAPE ì •ì˜
# # def smape(y_true, y_pred):
# #     return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))

# # val_smape = smape(y_val_exp, val_pred_exp)
# # print(f"\nâœ… ìŠ¤íƒœí‚¹ SMAPE: {val_smape:.4f}")

# # # ìµœì¢… ì˜ˆì¸¡
# # final_pred_log = meta_model.predict(oof_test)
# # final_pred = np.expm1(final_pred_log)

# # # ì €ì¥
# # samplesub['answer'] = final_pred
# # today = datetime.datetime.now().strftime('%Y%m%d')
# # score_str = f"{val_smape:.4f}".replace('.', '_')
# # filename = f"submission_{today}_SMAPE_{score_str}.csv"
# # samplesub.to_csv(os.path.join(path, filename), index=False)
# # print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")



# # # GradientRegressor nestimator 700> 1000 























# # import os
# # import pandas as pd
# # import numpy as np
# # from sklearn.model_selection import train_test_split
# # from sklearn.preprocessing import StandardScaler
# # from xgboost import XGBRegressor
# # from lightgbm import LGBMRegressor
# # from catboost import CatBoostRegressor
# # from sklearn.ensemble import GradientBoostingRegressor
# # import lightgbm as lgb
# # import random
# # import tensorflow as tf
# # import datetime

# # # Seed ê³ ì •

# # # seed best so far !!!!!! 42
# # seed = 707
# # random.seed(seed)
# # np.random.seed(seed)
# # tf.random.set_seed(seed)

# # # ê²½ë¡œ ì„¤ì •
# # if os.path.exists('/workspace/TensorJae/Study25/'):
# #     BASE_PATH = '/workspace/TensorJae/Study25/'
# # else:
# #     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# # save_path = os.path.join(path, '_save/')

# # # ë°ì´í„° ë¡œë“œ
# # buildinginfo = pd.read_csv(path + 'building_info.csv')
# # train = pd.read_csv(path + 'train.csv')
# # test = pd.read_csv(path + 'test.csv')
# # samplesub = pd.read_csv(path + 'sample_submission.csv')

# # # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # def feature_engineering(df):
# #     df = df.copy()
# #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
# #     df['month'] = df['ì¼ì‹œ'].dt.month
# #     df['day'] = df['ì¼ì‹œ'].dt.day
# #     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
# #     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
# #     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
# #     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
# #     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
# #         if col in df.columns:
# #             df[col] = df[col].fillna(0)
# #     temp = df['ê¸°ì˜¨(Â°C)']
# #     humidity = df['ìŠµë„(%)']
# #     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
# #     return df

# # print("ğŸ“Š Feature Engineering...")
# # train = feature_engineering(train)
# # test = feature_engineering(test)

# # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # features = [
# #     'ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
# #     'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
# #     'ìŠµë„(%)', 'hour', 'dayofweek', 'month', 'day',
# #     'is_weekend', 'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# # ]
# # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # x = train[features] 
# # y = np.log1p(train[target])
# # x_test_final = test[features]

# # x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# # scaler = StandardScaler()
# # x_train_scaled = scaler.fit_transform(x_train)
# # x_val_scaled = scaler.transform(x_val)
# # x_test_final_scaled = scaler.transform(x_test_final)

# # # ëª¨ë¸ í•™ìŠµ
# # print("ğŸš€ Training base models...")
# # xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=seed,
# #                          early_stopping_rounds=50, objective='reg:squarederror')
# # xgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

# # lgb_model = LGBMRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6, random_state=seed, objective='mae')
# # lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

# # cat_model = CatBoostRegressor(n_estimators=1000, learning_rate=0.05, max_depth=6,
# #                               random_seed=seed, verbose=0, loss_function='MAE')
# # cat_model.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

# # # OOF ìƒì„±
# # oof_train = np.vstack([
# #     xgb_model.predict(x_train_scaled),
# #     lgb_model.predict(x_train_scaled),
# #     cat_model.predict(x_train_scaled)
# # ]).T

# # oof_val = np.vstack([
# #     xgb_model.predict(x_val_scaled),
# #     lgb_model.predict(x_val_scaled),
# #     cat_model.predict(x_val_scaled)
# # ]).T

# # oof_test = np.vstack([
# #     xgb_model.predict(x_test_final_scaled),
# #     lgb_model.predict(x_test_final_scaled),
# #     cat_model.predict(x_test_final_scaled)
# # ]).T

# # # ë©”íƒ€ ëª¨ë¸
# # print("ğŸ” Meta model training...")
# # meta_model = GradientBoostingRegressor(n_estimators=700, learning_rate=0.05, max_depth=3, random_state=seed)
# # meta_model.fit(oof_train, y_train)

# # val_pred = meta_model.predict(oof_val)
# # y_val_exp = np.expm1(y_val)
# # val_pred_exp = np.expm1(val_pred)

# # # âœ… ì •í™•í•œ SMAPE êµ¬í˜„
# # def smape(y_true, y_pred):
# #     epsilon = 1e-6  # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
# #     denominator = (np.abs(y_true) + np.abs(y_pred)) + epsilon
# #     return 100 * np.mean(2.0 * np.abs(y_pred - y_true) / denominator)

# # val_smape = smape(y_val_exp, val_pred_exp)
# # print(f"\nâœ… Stacking SMAPE: {val_smape:.4f}")

# # # ìµœì¢… ì˜ˆì¸¡
# # final_pred_log = meta_model.predict(oof_test)
# # final_pred = np.expm1(final_pred_log)

# # samplesub['answer'] = final_pred
# # today = datetime.datetime.now().strftime('%Y%m%d')
# # score_str = f"{val_smape:.4f}".replace('.', '_')
# # filename = f"submission_{today}_SMAPE_{score_str}.csv"
# # samplesub.to_csv(os.path.join(path, filename), index=False)
# # print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")

# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.ensemble import GradientBoostingRegressor
# from sklearn.linear_model import RidgeCV
# from sklearn.metrics import mean_absolute_error

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# # Seed ê³ ì •
# seed = 43


# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# if os.path.exists('/workspace/TensorJae/Study25/'):
#     BASE_PATH = '/workspace/TensorJae/Study25/'
# else:
#     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')

# # ë°ì´í„° ë¡œë“œ
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
#     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
#     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#         if col in df.columns:
#             df[col] = df[col].fillna(0)
#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# # ì „ì²˜ë¦¬
# train = feature_engineering(train)
# test = feature_engineering(test)
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
# final_preds = []
# val_smapes = []

# # ê±´ë¬¼ë³„ë¡œ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
# building_ids = train['ê±´ë¬¼ë²ˆí˜¸'].unique()

# for bno in building_ids:
#     print(f"ğŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ëª¨ë¸ë§ ì¤‘...")

#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()

#     x = train_b[features]
#     y = np.log1p(train_b[target])
#     x_test_final = test_b[features]

#     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

#     scaler = StandardScaler()
#     x_train_scaled = scaler.fit_transform(x_train)
#     x_val_scaled = scaler.transform(x_val)
#     x_test_final_scaled = scaler.transform(x_test_final)

#     # Base models
#     xgb_model = XGBRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                              random_state=seed, early_stopping_rounds=50, objective='reg:squarederror')
#     xgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#     lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                               random_state=seed, objective='mae')
#     lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
#                   callbacks=[lgb.early_stopping(50, verbose=False)])

#     cat_model = CatBoostRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                                   random_seed=seed, verbose=0, loss_function='MAE')
#     cat_model.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

#     # Level 1 predictions
#     oof_train_lvl1 = np.vstack([
#         xgb_model.predict(x_train_scaled),
#         lgb_model.predict(x_train_scaled),
#         cat_model.predict(x_train_scaled)
#     ]).T
#     oof_val_lvl1 = np.vstack([
#         xgb_model.predict(x_val_scaled),
#         lgb_model.predict(x_val_scaled),
#         cat_model.predict(x_val_scaled)
#     ]).T
#     oof_test_lvl1 = np.vstack([
#         xgb_model.predict(x_test_final_scaled),
#         lgb_model.predict(x_test_final_scaled),
#         cat_model.predict(x_test_final_scaled)
#     ]).T

#     # Level 2 Meta model
#     meta_model = RidgeCV()
#     meta_model.fit(oof_train_lvl1, y_train)
#     val_pred_lvl2 = meta_model.predict(oof_val_lvl1)
#     test_pred_lvl2 = meta_model.predict(oof_test_lvl1)

#     # Level 3 Final model
#     final_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=seed)
#     final_model.fit(val_pred_lvl2.reshape(-1, 1), y_val)

#     val_final = final_model.predict(val_pred_lvl2.reshape(-1, 1))
#     val_smape = np.mean(200 * np.abs(np.expm1(val_final) - np.expm1(y_val)) /
#                         (np.abs(np.expm1(val_final)) + np.abs(np.expm1(y_val)) + 1e-6))
#     val_smapes.append(val_smape)

#     pred = np.expm1(final_model.predict(test_pred_lvl2.reshape(-1, 1)))
#     final_preds.extend(pred)

# # ê²°ê³¼ ì €ì¥
# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# score_str = f"{avg_smape:.4f}".replace('.', '_')
# filename = f"submission_groupwise_{today}_SMAPE_{score_str}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)
# print(f"\nğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")
# print(f"âœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")


# # import os
# # import pandas as pd
# # import numpy as np
# # from sklearn.preprocessing import StandardScaler
# # from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor
# # from xgboost import XGBRegressor
# # from catboost import CatBoostRegressor
# # from lightgbm import LGBMRegressor
# # import random
# # import tensorflow as tf
# # import datetime

# # # Seed ê³ ì •
# # seed = 42
# # random.seed(seed)
# # np.random.seed(seed)
# # tf.random.set_seed(seed)

# # # ê²½ë¡œ ì„¤ì •
# # BASE_PATH = './' # ì‚¬ìš©ìì˜ í™˜ê²½ì— ë§ê²Œ ì¡°ì ˆí•˜ì„¸ìš”.
# # path = os.path.join(BASE_PATH, '_data/dacon/electricity/')

# # # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# # buildinginfo = pd.read_csv(path + 'building_info.csv', encoding='cp949')
# # train = pd.read_csv(path + 'train.csv', encoding='cp949')
# # test = pd.read_csv(path + 'test.csv', encoding='cp949')
# # samplesub = pd.read_csv(path + 'sample_submission.csv', encoding='cp949')

# # # --- 1. ê¸°ë³¸ ì „ì²˜ë¦¬ ---
# # # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° íƒ€ì… ë³€í™˜
# # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # # ë‚ ì§œ íŒŒì‹± ë° ê¸°ë³¸ ì‹œê°„ íŠ¹ì„± ìƒì„±
# # for df in [train, test]:
# #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])

# # # ê±´ë¬¼ ì •ë³´ ë³‘í•©
# # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # # ë²”ì£¼í˜• ì²˜ë¦¬
# # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # # --- 2. [ê³ ë„í™”] ì‹œê³„ì—´ íŠ¹ì„± ìƒì„± (Lag & Rolling) ---
# # # í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•©ì³ì„œ íŠ¹ì„± ìƒì„± (ë°ì´í„° ì¼ê´€ì„± ìœ ì§€)
# # combined_df = pd.concat([train, test], ignore_index=True)
# # combined_df = combined_df.sort_values(by=['ê±´ë¬¼ë²ˆí˜¸', 'ì¼ì‹œ']).reset_index(drop=True)

# # # ë¡œê·¸ ë³€í™˜ëœ íƒ€ê²Ÿ ìƒì„± (ì´í›„ ì´ë™í‰ê· /ì‹œì°¨ íŠ¹ì„±ì— ì‚¬ìš©)
# # combined_df['log_target'] = np.log1p(combined_df['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'])

# # # ì‹œì°¨(Lag) íŠ¹ì„± ìƒì„±
# # lags = [24, 48, 168] # 1ì¼, 2ì¼, 1ì£¼ì¼ ì „
# # for lag in lags:
# #     combined_df[f'lag_{lag}'] = combined_df.groupby('ê±´ë¬¼ë²ˆí˜¸')['log_target'].shift(lag)

# # # ì´ë™ í‰ê· (Rolling) íŠ¹ì„± ìƒì„±
# # windows = [24, 48, 168]
# # for window in windows:
# #     combined_df[f'rolling_mean_{window}'] = combined_df.groupby('ê±´ë¬¼ë²ˆí˜¸')['log_target'].transform(
# #         lambda x: x.shift(24).rolling(window=window, min_periods=1).mean() # shift(24)ë¡œ 24ì‹œê°„ ì „ë¶€í„°ì˜ í‰ê·  ê³„ì‚°
# #     )
# #     combined_df[f'rolling_max_{window}'] = combined_df.groupby('ê±´ë¬¼ë²ˆí˜¸')['log_target'].transform(
# #         lambda x: x.shift(24).rolling(window=window, min_periods=1).max()
# #     )

# # # Featuretoolsì—ì„œ ì˜ê°ì„ ë°›ì€ ì‹œê°„ ê´€ë ¨ íŠ¹ì„± ì¶”ê°€
# # combined_df['hour'] = combined_df['ì¼ì‹œ'].dt.hour
# # combined_df['dayofweek'] = combined_df['ì¼ì‹œ'].dt.dayofweek
# # combined_df['month'] = combined_df['ì¼ì‹œ'].dt.month
# # combined_df['dayofyear'] = combined_df['ì¼ì‹œ'].dt.dayofyear

# # # íŠ¹ì„± ìƒì„± í›„ ë‹¤ì‹œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„ë¦¬
# # train = combined_df[combined_df['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].notna()].copy()
# # test = combined_df[combined_df['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'].isna()].copy()

# # # ìƒì„±ëœ íŠ¹ì„±ì—ì„œ ë°œìƒí•œ NaN ê°’ ì²˜ë¦¬ (ê³¼ê±° ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
# # # bfill: ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ì›€. í›ˆë ¨ ì´ˆë°˜ ë°ì´í„°ì—ë§Œ í•´ë‹¹.
# # train = train.fillna(method='bfill')

# # # --- 3. í”¼ì²˜ ë° íƒ€ê²Ÿ ì„¤ì • ---
# # features = [
# #     'ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
# #     'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
# #     'ìŠµë„(%)', 'hour', 'dayofweek', 'month', 'dayofyear',
# # ]
# # # ìƒì„±í•œ ì‹œê³„ì—´ íŠ¹ì„± ì¶”ê°€
# # features.extend([col for col in train.columns if 'lag_' in col or 'rolling_' in col])
# # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # x = train[features]
# # y = np.log1p(train[target])
# # x_test_final = test[features]

# # # --- 4. [ê³ ë„í™”] ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ë¶„í•  ---
# # split_date = pd.to_datetime('2022-08-18') # í›ˆë ¨ ê¸°ê°„ì˜ ë§ˆì§€ë§‰ ì£¼ ì‹œì‘ì¼
# # val_indices = train[train['ì¼ì‹œ'] >= split_date].index

# # x_train, x_val = x.drop(val_indices), x.loc[val_indices]
# # y_train, y_val = y.drop(val_indices), y.loc[val_indices]

# # # --- 5. ìŠ¤ì¼€ì¼ë§ ë° ëª¨ë¸ í•™ìŠµ ---
# # scaler = StandardScaler()
# # x_train_scaled = scaler.fit_transform(x_train)
# # x_val_scaled = scaler.transform(x_val)
# # x_test_final_scaled = scaler.transform(x_test_final)

# # # ëª¨ë¸ ì •ì˜
# # xgb = XGBRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed, n_jobs=-1)
# # lgb = LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed, n_jobs=-1)
# # cat = CatBoostRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed, verbose=0)

# # stack_model = StackingRegressor(
# #     estimators=[('xgb', xgb), ('lgb', lgb), ('cat', cat)],
# #     final_estimator=GradientBoostingRegressor(n_estimators=350, random_state=seed),
# #     n_jobs=-1,
# #     cv='passthrough' # í›ˆë ¨ ë°ì´í„°ë¥¼ final_estimatorì— ê·¸ëŒ€ë¡œ ì‚¬ìš©
# # )

# # print("Starting model training...")
# # stack_model.fit(x_train_scaled, y_train)

# # # --- 6. í‰ê°€ ë° ì œì¶œ ---
# # def smape(y_true, y_pred):
# #     return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))

# # y_pred_log = stack_model.predict(x_val_scaled)
# # y_pred_exp = np.expm1(y_pred_log)
# # y_true_exp = np.expm1(y_val)

# # val_smape = smape(y_true_exp, y_pred_exp)
# # print(f"\nâœ… Validation SMAPE: {val_smape:.4f}")

# # final_pred_log = stack_model.predict(x_test_final_scaled)
# # final_pred_exp = np.expm1(final_pred_log)

# # samplesub['answer'] = final_pred_exp

# # # íŒŒì¼ ì €ì¥
# # today = datetime.datetime.now().strftime('%Y%m%d')
# # score_str = f"{val_smape:.4f}".replace('.', '_')
# # filename = f"submission_{today}_SMAPE_{score_str}.csv"
# # file_path = os.path.join('./', filename) # ì €ì¥ ê²½ë¡œ
# # samplesub.to_csv(file_path, index=False)
# # print(f"ğŸ“ {filename} saved successfully!")


import os
import pandas as pd
import numpy as np
import random
import datetime
import optuna
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf
import warnings
warnings.filterwarnings(action='ignore')

# Seed ê³ ì •
seed = 707
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# ê²½ë¡œ ì„¤ì •
if os.path.exists('/workspace/TensorJae/Study25/'):
    BASE_PATH = '/workspace/TensorJae/Study25/'
else:
    BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
buildinginfo = pd.read_csv(path + 'building_info.csv')
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
samplesub = pd.read_csv(path + 'sample_submission.csv')

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
    buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# Feature Engineering
def feature_engineering(df):
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
    df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
        if col in df.columns:
            df[col] = df[col].fillna(0)
    temp = df['ê¸°ì˜¨(Â°C)']
    humidity = df['ìŠµë„(%)']
    df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
    return df

train = feature_engineering(train)
test = feature_engineering(test)
train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

all_features = [
    'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
    'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
    'hour', 'dayofweek', 'month', 'day', 'is_weekend',
    'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
]

target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
final_preds = []
val_smapes = []

# ì¤‘ìš”ë„ ê¸°ë°˜ feature filtering (ê¸°ë³¸ xgb ê¸°ì¤€)
xgb_temp = XGBRegressor()
x = train[all_features]
y = np.log1p(train[target])
xgb_temp.fit(x, y)
importances = xgb_temp.feature_importances_
feature_importance_dict = dict(zip(all_features, importances))
selected_features = [f for f, score in feature_importance_dict.items() if score >= 0.01]
print(f"\nâœ… ì„ íƒëœ feature ({len(selected_features)}ê°œ): {selected_features}")

# Optuna íŠœë‹ í•¨ìˆ˜
def objective(trial, oof_train, oof_val, y_train, y_val):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    ridge.fit(oof_train, y_train)
    preds = ridge.predict(oof_val)
    smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
                    (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
    return smape

# ê±´ë¬¼ë³„ ëª¨ë¸ í•™ìŠµ
building_ids = train['ê±´ë¬¼ë²ˆí˜¸'].unique()
for bno in building_ids:
    print(f"\nğŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...")

    train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
    test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
    x = train_b[selected_features]
    y = np.log1p(train_b[target])
    x_test_final = test_b[selected_features]

    x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_val_scaled = scaler.transform(x_val)
    x_test_final_scaled = scaler.transform(x_test_final)

    # Base ëª¨ë¸ í•™ìŠµ
    xgb = XGBRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                       random_state=seed, early_stopping_rounds=50, objective='reg:squarederror')
    xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

    lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                              random_state=seed, objective='mae')
    lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
                  callbacks=[lgb.early_stopping(50, verbose=False)])

    cat = CatBoostRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                            random_seed=seed, verbose=0, loss_function='MAE')
    cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

    # ìŠ¤íƒœí‚¹
    oof_train_lvl1 = np.vstack([
        xgb.predict(x_train_scaled),
        lgb_model.predict(x_train_scaled),
        cat.predict(x_train_scaled)
    ]).T
    oof_val_lvl1 = np.vstack([
        xgb.predict(x_val_scaled),
        lgb_model.predict(x_val_scaled),
        cat.predict(x_val_scaled)
    ]).T
    oof_test_lvl1 = np.vstack([
        xgb.predict(x_test_final_scaled),
        lgb_model.predict(x_test_final_scaled),
        cat.predict(x_test_final_scaled)
    ]).T

    # Optunaë¡œ Ridge íŠœë‹
    study = optuna.create_study(direction="minimize")
    study.optimize(lambda trial: objective(trial, oof_train_lvl1, oof_val_lvl1, y_train, y_val), n_trials=30)
    best_alpha = study.best_params['alpha']
    meta_model = Ridge(alpha=best_alpha)
    meta_model.fit(oof_train_lvl1, y_train)

    val_pred = meta_model.predict(oof_val_lvl1)
    test_pred = meta_model.predict(oof_test_lvl1)

    val_smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
                        (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))
    val_smapes.append(val_smape)

    pred = np.expm1(test_pred)
    final_preds.extend(pred)

# ê²°ê³¼ ì €ì¥
samplesub['answer'] = final_preds
today = datetime.datetime.now().strftime('%Y%m%d')
avg_smape = np.mean(val_smapes)
score_str = f"{avg_smape:.4f}".replace('.', '_')
filename = f"submission_stack_filtered_{today}_SMAPE_{score_str}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")