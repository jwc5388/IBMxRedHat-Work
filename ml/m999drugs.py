import os
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import Ridge, LassoCV
from sklearn.feature_selection import SelectFromModel
from sklearn.impute import SimpleImputer
import xgboost as xgb
import lightgbm as lgb
import catboost as cat
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski, MACCSkeys, AllChem
from datetime import datetime
import random
import warnings
import copy

# Seed ê³ ì •
seed = 42
random.seed(seed)
np.random.seed(seed)
warnings.filterwarnings('ignore')

# ê²½ë¡œ ë° ë°ì´í„° ë¡œë“œ
BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
path = os.path.join(BASE_PATH, '_data/dacon/drugs/')
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
submission = pd.read_csv(path + 'sample_submission.csv')
# from rdkit.Chem import Draw
# smi = "COC(=O)[C@@H](OC(C)(C)C)c1c(C)nc2C(=O)N(Cc3ccc(OC)cc3)CCc2c1Cl"
# mol = Chem.MolFromSmiles(smi)
# img = Draw.MolToImage(mol)
# img.show() 

# exit()

# # âœ… ì—¬ê¸° ë¶™ì—¬
# from rdkit import Chem

# failures = []
# for i, s in enumerate(test['Canonical_Smiles']):
#     mol = Chem.MolFromSmiles(s)
#     if mol is None:
#         failures.append((i, s))

# print(f"âš ï¸ ì‹¤íŒ¨í•œ SMILES ê°œìˆ˜: {len(failures)}")
# for idx, smile in failures:
#     print(f"  - index: {idx}, SMILES: {smile}")

# exit()

# Feature Extraction (NaN ë°˜í™˜ ìœ ì§€)
# ì‹¤íŒ¨ ë¡œê·¸ ì €ì¥ìš©
train_nan_info = []
test_nan_info = []

def get_molecule_descriptors(smiles, index=None, is_test=False):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            if is_test:
                test_nan_info.append((index, smiles, "MolFromSmiles=None"))
            else:
                train_nan_info.append((index, smiles, "MolFromSmiles=None"))
            return [np.nan] * 2233

        basic = [ Descriptors.MolWt(mol), Descriptors.MolLogP(mol), Descriptors.NumHAcceptors(mol), Descriptors.NumHDonors(mol),
                 Descriptors.TPSA(mol), Descriptors.NumRotatableBonds(mol), Descriptors.NumAromaticRings(mol), Descriptors.NumHeteroatoms(mol), 
                 Descriptors.FractionCSP3(mol), Descriptors.NumAliphaticRings(mol), Lipinski.NumAromaticHeterocycles(mol),
                 Lipinski.NumSaturatedHeterocycles(mol), Lipinski.NumAliphaticHeterocycles(mol), Descriptors.HeavyAtomCount(mol),
                 Descriptors.RingCount(mol), Descriptors.NOCount(mol), Descriptors.NHOHCount(mol), Descriptors.NumRadicalElectrons(mol) ]
        morgan = [int(b) for b in AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=2048).ToBitString()]
        maccs = [int(b) for b in MACCSkeys.GenMACCSKeys(mol).ToBitString()]
        features = basic + morgan + maccs

        if np.any(pd.isna(features)):
            if is_test:
                test_nan_info.append((index, smiles, "NaN in descriptor"))
            else:
                train_nan_info.append((index, smiles, "NaN in descriptor"))
        return features

    except Exception as e:
        if is_test:
            test_nan_info.append((index, smiles, f"Exception: {e}"))
        else:
            train_nan_info.append((index, smiles, f"Exception: {e}"))
        return [np.nan] * 2233
    


# ê°ê° ì¸ë±ìŠ¤ë¥¼ ì „ë‹¬í•˜ë©° ê²°ì¸¡ì¹˜ ì¶”ì 
train['features'] = [get_molecule_descriptors(smi, idx, is_test=False) for idx, smi in enumerate(train['Canonical_Smiles'])]
test['features'] = [get_molecule_descriptors(smi, idx, is_test=True) for idx, smi in enumerate(test['Canonical_Smiles'])]

x_raw = np.array(train['features'].tolist())
x_test_raw = np.array(test['features'].tolist())

# ğŸ“¢ ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
print(f"âœ… Train ê²°ì¸¡ì¹˜ ì´í•©: {np.isnan(x_raw).sum()}")
print(f"âœ… Test  ê²°ì¸¡ì¹˜ ì´í•©: {np.isnan(x_test_raw).sum()}")
print(f"âš ï¸ Train NaN í¬í•¨ ìƒ˜í”Œ ìˆ˜: {len(train_nan_info)}")
print(f"âš ï¸ Test  NaN í¬í•¨ ìƒ˜í”Œ ìˆ˜: {len(test_nan_info)}")

print("\nğŸ“Œ Train NaN í¬í•¨ëœ SMILES:")
for idx, smi, reason in train_nan_info[:10]:  # ë§ìœ¼ë©´ ìë¥´ê¸°
    print(f" - index: {idx}, SMILES: {smi}, reason: {reason}")

print("\nğŸ“Œ Test NaN í¬í•¨ëœ SMILES:")
for idx, smi, reason in test_nan_info[:10]:
    print(f" - index: {idx}, SMILES: {smi}, reason: {reason}")
    
    
    
y = train['Inhibition'].values


# [ìˆ˜ì •] ì œì•ˆëœ ë°©ì‹ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
print("ê²°ì¸¡ì¹˜ë¥¼ 'ì •ë³´'ë¡œ í™œìš©í•˜ì—¬ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
# 1. ê²°ì¸¡ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 'is_missing' íŠ¹ì§• ì¶”ê°€
train['is_missing'] = np.isnan(x_raw).any(axis=1).astype(int)
test['is_missing'] = np.isnan(x_test_raw).any(axis=1).astype(int)
x_raw = np.c_[x_raw, train['is_missing']]
x_test_raw = np.c_[x_test_raw, test['is_missing']]

# 2. ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
imputer = SimpleImputer(strategy='constant', fill_value=0)
x_raw = imputer.fit_transform(x_raw)
x_test_raw = imputer.transform(x_test_raw)



# í‰ê°€ ì§€í‘œ ì •ì˜
def rmse(y_true, y_pred): return np.sqrt(mean_squared_error(y_true, y_pred))
def normalized_rmse(y_true, y_pred): return rmse(y_true, y_pred) / (np.max(y_true) - np.min(y_true))
def pearson_correlation(y_true, y_pred):
    if np.std(y_true) == 0 or np.std(y_pred) == 0: return 0.0
    return np.clip(np.corrcoef(y_true, y_pred)[0, 1], 0, 1)
def competition_score(y_true, y_pred): return 0.5 * (1 - min(normalized_rmse(y_true, y_pred), 1)) + 0.5 * pearson_correlation(y_true, y_pred)
def create_xgb_model():
    return xgb.XGBRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1,
        random_state=seed,
        tree_method='hist',
        predictor='auto',
        objective='reg:squarederror',
        early_stopping_rounds=50,     # âœ… XGBoostë§Œ ëª¨ë¸ ìƒì„±ìì— ì„¤ì •
        eval_metric='rmse'
    )

def create_lgb_model():
    return lgb.LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.05,
        num_leaves=31,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.1,
        reg_lambda=1,
        random_state=seed,
        objective='regression'
    )

def create_cat_model():
    return cat.CatBoostRegressor(
        iterations=1000,
        learning_rate=0.05,
        depth=6,
        l2_leaf_reg=3,
        random_seed=seed,
        loss_function='RMSE',
        verbose=0
    )

# [ì¶”ê°€] LassoCVë¥¼ ì´ìš©í•œ ì•ˆì •ì ì¸ íŠ¹ì§• ì„ íƒ
print("\nLassoCVë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì„ íƒí•©ë‹ˆë‹¤...")
scaler_fs = RobustScaler()
x_scaled_fs = scaler_fs.fit_transform(x_raw)
lasso = LassoCV(cv=5, random_state=seed, n_jobs=-1).fit(x_scaled_fs, y)
selector = SelectFromModel(lasso, prefit=True)
x_selected = selector.transform(x_raw)
x_test_selected = selector.transform(x_test_raw)
print(f"Lasso selected {x_selected.shape[1]} features.")

# [ìˆ˜ì •] K-Fold êµì°¨ ê²€ì¦ì„ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
kf = KFold(n_splits=5, shuffle=True, random_state=seed)

# OOF(Out-of-Fold) ì˜ˆì¸¡ ë° í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì„ ì €ì¥í•  ë°°ì—´ ì´ˆê¸°í™”
oof_preds = np.zeros((len(x_selected), 3)) # 3ê°œ ëª¨ë¸
test_preds = np.zeros((len(x_test_selected), 3))

base_models = {
    "XGBoost": create_xgb_model(),
    "LightGBM": create_lgb_model(),
    "CatBoost": create_cat_model()
}

print("\nK-Fold êµì°¨ ê²€ì¦ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
for i, (name, model) in enumerate(base_models.items()):
    print(f"  Training {name} model...")
    for fold, (train_idx, val_idx) in enumerate(kf.split(x_selected, y)):
        # ë°ì´í„° ë¶„í• 
        x_train_fold, x_val_fold = x_selected[train_idx], x_selected[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]
        
        # ìŠ¤ì¼€ì¼ë§ (Fold ë‚´ë¶€ì—ì„œ ìˆ˜í–‰í•˜ì—¬ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
        scaler = RobustScaler()
        x_train_fold_scaled = scaler.fit_transform(x_train_fold)
        x_val_fold_scaled = scaler.transform(x_val_fold)

        # ëª¨ë¸ í•™ìŠµ
        m = copy.deepcopy(model)
        if name == "LightGBM":
            m.fit(x_train_fold_scaled, y_train_fold, eval_set=[(x_val_fold_scaled, y_val_fold)], callbacks=[lgb.early_stopping(50, verbose=False)])
        elif name == "CatBoost":
            m.fit(x_train_fold_scaled, y_train_fold, eval_set=[(x_val_fold_scaled, y_val_fold)], early_stopping_rounds=50, verbose=0)
        elif name == "XGBoost": m.fit(x_train_fold_scaled, y_train_fold,
              eval_set=[(x_val_fold_scaled, y_val_fold)],
              verbose=0)
        # OOF ì˜ˆì¸¡ê°’ ì €ì¥
        oof_preds[val_idx, i] = m.predict(x_val_fold_scaled)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ëˆ„ì 
        x_test_fold_scaled = scaler.transform(x_test_selected)
        test_preds[:, i] += m.predict(x_test_fold_scaled) / kf.n_splits

# --- ë©”íƒ€ ëª¨ë¸ í•™ìŠµ ë° ìµœì¢… ì˜ˆì¸¡ ---
print("\në©”íƒ€ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...")
meta_model = Ridge()
# OOF ì˜ˆì¸¡ê°’ìœ¼ë¡œ ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
meta_model.fit(oof_preds, y)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ìµœì¢… ì˜ˆì¸¡
y_pred_test = meta_model.predict(test_preds)

# OOF ì ìˆ˜ ê³„ì‚° (ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²€ì¦ ì ìˆ˜)
# âœ… OOF ì ìˆ˜ ê³„ì‚° ë° ì¶œë ¥
y_oof_pred = meta_model.predict(oof_preds)
from sklearn.metrics import r2_score
oof_r2 = r2_score(y, y_oof_pred)

oof_rmse = rmse(y, y_oof_pred)
oof_nrmse = normalized_rmse(y, y_oof_pred)
oof_pearson = pearson_correlation(y, y_oof_pred)
oof_score = competition_score(y, y_oof_pred)

print("\nğŸ“Š ìµœì¢… OOF ì„±ëŠ¥")
print(f"  - RMSE       : {oof_rmse:.4f}")
print(f"  - NRMSE      : {oof_nrmse:.4f}")
print(f"  - R2 Score   : {oof_r2:.4f}")
print(f"  - Pearson    : {oof_pearson:.4f}")
print(f"  - Comp Score : {oof_score:.4f}")

# ì˜ˆì¸¡ ì €ì¥
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
filename = f"submission_kfold_final_{timestamp}.csv"
submission['Inhibition'] = y_pred_test
submission.to_csv(os.path.join(path, filename), index=False)
print(f"âœ… ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {filename}")

