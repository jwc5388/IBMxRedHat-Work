
#0
# -*- coding: utf-8 -*-
# Optuna íŒŒë¼ë¯¸í„° ì €ìž¥/ë¡œë“œ ìœ ì§€, ì˜µíŠœë‚˜ 1íšŒ, fold ë°–ì—ì„œ 
# ì „ì²˜ë¦¬ ê°•í™” 6.5x BEST VERSION

import os
import json
import random
import warnings
import datetime
import numpy as np
import pandas as pd
import optuna

from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from joblib import Parallel, delayed
from optuna.samplers import TPESampler

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf

warnings.filterwarnings("ignore")

# ==============================
# 0) ì‹œë“œ / ê²½ë¡œ
# ==============================
seed = 6054
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
    else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
train = pd.read_csv(os.path.join(path, "train.csv"))
test = pd.read_csv(os.path.join(path, "test.csv"))
samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))

# === 0) ì˜µì…˜: building_info ë³‘í•© (ìžˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
if 'buildinginfo' in globals():
    bi = buildinginfo.copy()
else:
    bi = None

if bi is not None:
    # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìžë¡œ ìºìŠ¤íŒ…
    for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
        if col in bi.columns:
            bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
    # ì„¤ë¹„ ìœ ë¬´ í”Œëž˜ê·¸
    bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
    bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ìž¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ìž¥ìš©ëŸ‰(kWh)' in bi.columns else 0

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
    keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
    for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ìž¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
        if c in bi.columns: keep_cols.append(c)
    bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

    train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['hour']      = df['ì¼ì‹œ'].dt.hour
    df['day']       = df['ì¼ì‹œ'].dt.day
    df['month']     = df['ì¼ì‹œ'].dt.month
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
    df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
    df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
    df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
    df['sin_month'] = np.sin(2*np.pi*df['month']/12)
    df['cos_month'] = np.cos(2*np.pi*df['month']/12)
    df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
    df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['DI'] = 0.0
    return df

train = add_time_features_kor(train)
test  = add_time_features_kor(test)

# === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
if 'ì¼ì‚¬(MJ/m2)' in train.columns:
    solar_proxy = (
        train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
             .mean().reset_index()
             .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
    )
    train = train.merge(solar_proxy, on=['month','hour'], how='left')
    test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
else:
    train['expected_solar'] = 0.0
    test['expected_solar']  = 0.0

train['expected_solar'] = train['expected_solar'].fillna(0)
test['expected_solar']  = test['expected_solar'].fillna(0)

# === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
            df[c] = 0.0
        return df
    grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
    stats = grp.agg(day_max_temperature='max',
                    day_mean_temperature='mean',
                    day_min_temperature='min').reset_index()
    df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
    return df

train = add_daily_temp_stats_kor(train)
test  = add_daily_temp_stats_kor(test)

# === 4) CDH / THI / WCT (train/test ë™ì¼)
def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        df['CDH'] = 0.0
        return df
    def _cdh_1d(x):
        cs = np.cumsum(x - 26)
        return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
    parts = []
    for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
        arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
        cdh = _cdh_1d(arr)
        parts.append(pd.Series(cdh, index=g.index))
    df['CDH'] = pd.concat(parts).sort_index()
    return df

def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['THI'] = 0.0
    if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
        df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
    else:
        df['WCT'] = 0.0
    return df

train = add_CDH_kor(train)
test  = add_CDH_kor(test)
train = add_THI_WCT_kor(train)
test  = add_THI_WCT_kor(test)

# === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    pm = (train
          .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
          .agg(['mean','std'])
          .reset_index()
          .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
    train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
    test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
else:
    train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
    test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìžˆì„ ë•Œë§Œ)
if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
    both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
    cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
    train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
    test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)

# 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìžˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
feature_candidates = [
    'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ìž¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
    'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
    'hour','day','month','dayofweek','is_weekend','is_working_hours',
    'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
    'DI','expected_solar',
    'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
    'CDH','THI','WCT',
    'day_hour_mean','day_hour_std'
]
features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# 2) target ëª…ì‹œ
target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
if target not in train.columns:
    raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# 3) ìµœì¢… ìž…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
X = train[features].values
y = np.log1p(train[target].values.astype(float))
X_test_raw = test[features].values
ts = train['ì¼ì‹œ']  # ë‚´ë¶€ CVì—ì„œ ì •ë ¬/ì°¸ì¡°ìš© ê°€ëŠ¥

print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
print(f"[í™•ì¸] target: {target}")
print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y shape: {y.shape}")

# ------------------------------
# SMAPE
# ------------------------------
def smape_exp(y_true_log, y_pred_log):
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# ------------------------------
# [ë³€ê²½] Optuna: ë‚´ë¶€ CV(KFold 3, shuffle=True)ë¡œ "ê±´ë¬¼ë‹¹ 1íšŒ" íŠœë‹
# ------------------------------
def tune_xgb_cv(trial, X_full, y_full, seed=seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "eval_metric": "mae",
        "random_state": seed,
        "objective": "reg:squarederror",
        "early_stopping_rounds": 50,
    }
    kf_in = KFold(n_splits=3, shuffle=True, random_state=seed)
    scores = []
    for tr_idx, va_idx in kf_in.split(X_full):
        X_tr, X_va = X_full[tr_idx], X_full[va_idx]
        y_tr, y_va = y_full[tr_idx], y_full[va_idx]
        # ìŠ¤ì¼€ì¼ëŸ¬(ì™¸ë¶€ KFoldì™€ ì •í•©ì„±)
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = XGBRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def tune_lgb_cv(trial, X_full, y_full, seed=seed):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "random_state": seed,
        "objective": "mae",
    }
    kf_in = KFold(n_splits=3, shuffle=True, random_state=seed)
    scores = []
    for tr_idx, va_idx in kf_in.split(X_full):
        X_tr, X_va = X_full[tr_idx], X_full[va_idx]
        y_tr, y_va = y_full[tr_idx], y_full[va_idx]
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = LGBMRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def tune_cat_cv(trial, X_full, y_full, seed=seed):
    params = {
        "iterations": trial.suggest_int("iterations", 300, 1000),
        "depth": trial.suggest_int("depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
        "random_seed": seed,
        "loss_function": "MAE",
        "verbose": 0,
    }
    kf_in = KFold(n_splits=3, shuffle=True, random_state=seed)
    scores = []
    for tr_idx, va_idx in kf_in.split(X_full):
        X_tr, X_va = X_full[tr_idx], X_full[va_idx]
        y_tr, y_va = y_full[tr_idx], y_full[va_idx]
        sc = StandardScaler().fit(X_tr)
        X_tr_s = sc.transform(X_tr); X_va_s = sc.transform(X_va)
        model = CatBoostRegressor(**params)
        model.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)
        pred = model.predict(X_va_s)
        scores.append(smape_exp(y_va, pred))
    return float(np.mean(scores))

def get_or_tune_params_once(bno, X_full, y_full, param_dir):
    """[ë³€ê²½] ê±´ë¬¼ë‹¹ 1íšŒë§Œ íŠœë‹í•˜ê³  JSON ì €ìž¥/ë¡œë“œ"""
    os.makedirs(param_dir, exist_ok=True)
    paths = {
        "xgb": os.path.join(param_dir, f"{bno}_xgb.json"),
        "lgb": os.path.join(param_dir, f"{bno}_lgb.json"),
        "cat": os.path.join(param_dir, f"{bno}_cat.json"),
    }
    params = {}
    # XGB
    if os.path.exists(paths["xgb"]):
        with open(paths["xgb"], "r") as f: params["xgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_xgb_cv(t, X_full, y_full), n_trials=30)
        params["xgb"] = st.best_params
        with open(paths["xgb"], "w") as f: json.dump(params["xgb"], f)
    # LGB
    if os.path.exists(paths["lgb"]):
        with open(paths["lgb"], "r") as f: params["lgb"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_lgb_cv(t, X_full, y_full), n_trials=30)
        params["lgb"] = st.best_params
        with open(paths["lgb"], "w") as f: json.dump(params["lgb"], f)
    # CAT
    if os.path.exists(paths["cat"]):
        with open(paths["cat"], "r") as f: params["cat"] = json.load(f)
    else:
        st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
        st.optimize(lambda t: tune_cat_cv(t, X_full, y_full), n_trials=30)
        params["cat"] = st.best_params
        with open(paths["cat"], "w") as f: json.dump(params["cat"], f)
    return params

def objective_ridge(trial, oof_tr, oof_val, y_tr, y_val):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    ridge.fit(oof_tr, y_tr)
    preds = ridge.predict(oof_val)
    return smape_exp(y_val, preds)

def process_building_kfold(bno):
    print(f"ðŸ¢ building {bno} KFold...")
    param_dir = os.path.join(path, "optuna_params")
    os.makedirs(param_dir, exist_ok=True)

    tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
    te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

    X_full = tr_b[features].values
    y_full = np.log1p(tr_b[target].values.astype(float))
    X_test = te_b[features].values

    # [ë³€ê²½] ê±´ë¬¼ë‹¹ 1íšŒë§Œ Optuna íŠœë‹ í›„ íŒŒë¼ë¯¸í„° ë¡œë“œ
    best_params = get_or_tune_params_once(bno, X_full, y_full, param_dir)

    kf = KFold(n_splits=5, shuffle=True, random_state=seed)  # ì™¸ë¶€ KFoldëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    test_preds, val_smapes = [], []

    # [ë³€ê²½] Ridgeë„ ê±´ë¬¼ë‹¹ 1íšŒë§Œ: ì²« foldì—ì„œ íŠœë‹ â†’ ì €ìž¥, ì´í›„ ë¡œë“œ
    ridge_key = f"{bno}_ridge"
    ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
    ridge_params_cached = None
    if os.path.exists(ridge_path):
        with open(ridge_path, "r") as f:
            ridge_params_cached = json.load(f)

    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
        print(f" - fold {fold}")
        X_tr, X_va = X_full[tr_idx], X_full[va_idx]
        y_tr, y_va = y_full[tr_idx], y_full[va_idx]

        sc = StandardScaler()
        X_tr_s = sc.fit_transform(X_tr)
        X_va_s = sc.transform(X_va)
        X_te_s = sc.transform(X_test)

        # [ë³€ê²½] Optuna íŠœë‹ ì œê±° â†’ ì €ìž¥ëœ best_params ì‚¬ìš©í•´ í•™ìŠµë§Œ
        xgb = XGBRegressor(**best_params["xgb"])
        xgb.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)

        lgbm = LGBMRegressor(**best_params["lgb"])
        lgbm.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])

        cat = CatBoostRegressor(**best_params["cat"])
        cat.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)

        # ìŠ¤íƒœí‚¹ì„ ìœ„í•œ oof
        oof_tr = np.vstack([
            xgb.predict(X_tr_s),
            lgbm.predict(X_tr_s),
            cat.predict(X_tr_s)
        ]).T
        oof_va = np.vstack([
            xgb.predict(X_va_s),
            lgbm.predict(X_va_s),
            cat.predict(X_va_s)
        ]).T
        oof_te = np.vstack([
            xgb.predict(X_te_s),
            lgbm.predict(X_te_s),
            cat.predict(X_te_s)
        ]).T

        # [ë³€ê²½] Ridge ë©”íƒ€: ê±´ë¬¼ë‹¹ 1íšŒë§Œ íŠœë‹/ì €ìž¥
        if ridge_params_cached is None and fold == 1:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: objective_ridge(t, oof_tr, oof_va, y_tr, y_va), n_trials=30)
            ridge_params_cached = st.best_params
            with open(ridge_path, "w") as f:
                json.dump(ridge_params_cached, f)

        meta = Ridge(alpha=ridge_params_cached["alpha"])
        meta.fit(oof_tr, y_tr)

        va_pred = meta.predict(oof_va)
        te_pred = meta.predict(oof_te)

        fold_smape = smape_exp(y_va, va_pred)
        val_smapes.append(fold_smape)
        test_preds.append(np.expm1(te_pred))  # ì—­ë¡œê·¸

    avg_test_pred = np.mean(test_preds, axis=0)
    avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
    return avg_test_pred.tolist(), avg_smape

# ==============================
# 12) ë³‘ë ¬ ì‹¤í–‰
# ==============================
results = Parallel(n_jobs=-1, backend="loky")(
    delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
)

final_preds, val_smapes = [], []
for preds, sm in results:
    final_preds.extend(preds)
    val_smapes.append(sm)

samplesub["answer"] = final_preds
today = datetime.datetime.now().strftime("%Y%m%d")
avg_smape = float(np.mean(val_smapes))
filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")






# -*- coding: utf-8 -*-
# Optuna íŒŒë¼ë¯¸í„° ì €ìž¥/ë¡œë“œ ìœ ì§€, í‘œì¤€ ì»¬ëŸ¼ ê¸°ë°˜ ì „ì²˜ë¦¬ ì¼ì›í™” ë²„ì „
#ì˜µíŠœë‚˜ fold ì•ˆì—ì„œ
#ì „ì²˜ë¦¬ ê°•í™” 6.5x BEST VERSION

# import os
# import json
# import random
# import warnings
# import datetime
# import numpy as np
# import pandas as pd
# import optuna

# from sklearn.model_selection import train_test_split, KFold
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from joblib import Parallel, delayed
# from optuna.samplers import TPESampler

# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf

# warnings.filterwarnings("ignore")

# # ==============================
# # 0) ì‹œë“œ / ê²½ë¡œ
# # ==============================
# seed = 6054
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
#     else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
# path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

# buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
# train = pd.read_csv(os.path.join(path, "train.csv"))
# test = pd.read_csv(os.path.join(path, "test.csv"))
# samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))


# # === 0) ì˜µì…˜: building_info ë³‘í•© (ìžˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
# have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
# if 'buildinginfo' in globals():
#     bi = buildinginfo.copy()
# else:
#     bi = None

# if bi is not None:
#     # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìžë¡œ ìºìŠ¤íŒ…
#     for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
#         if col in bi.columns:
#             bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
#     # ì„¤ë¹„ ìœ ë¬´ í”Œëž˜ê·¸
#     bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
#     bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ìž¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ìž¥ìš©ëŸ‰(kWh)' in bi.columns else 0

#     # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
#     keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
#     for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ìž¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
#         if c in bi.columns: keep_cols.append(c)
#     bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

#     train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
#     test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# # === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
# def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     # ì¼ì‹œ íŒŒì‹± (í˜•ì‹: 'YYYYMMDD HH')
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
#     df['hour']      = df['ì¼ì‹œ'].dt.hour
#     df['day']       = df['ì¼ì‹œ'].dt.day
#     df['month']     = df['ì¼ì‹œ'].dt.month
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
#     # ì£¼ê¸° ì¸ì½”ë”©
#     df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
#     df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
#     df['sin_month'] = np.sin(2*np.pi*df['month']/12)
#     df['cos_month'] = np.cos(2*np.pi*df['month']/12)
#     df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
#     df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
#     # DI (ìžˆì„ ë•Œë§Œ)
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']
#         h = df['ìŠµë„(%)']
#         df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['DI'] = 0.0
#     return df

# train = add_time_features_kor(train)
# test  = add_time_features_kor(test)

# # === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
# if 'ì¼ì‚¬(MJ/m2)' in train.columns:
#     solar_proxy = (
#         train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
#              .mean().reset_index()
#              .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
#     )
#     train = train.merge(solar_proxy, on=['month','hour'], how='left')
#     test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
# else:
#     # trainì— ì¼ì‚¬ê°€ ì—†ìœ¼ë©´ 0
#     train['expected_solar'] = 0.0
#     test['expected_solar']  = 0.0

# train['expected_solar'] = train['expected_solar'].fillna(0)
# test['expected_solar']  = test['expected_solar'].fillna(0)

# # === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
# def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
#             df[c] = 0.0
#         return df
#     grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
#     stats = grp.agg(day_max_temperature='max',
#                     day_mean_temperature='mean',
#                     day_min_temperature='min').reset_index()
#     df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
#     df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
#     return df

# train = add_daily_temp_stats_kor(train)
# test  = add_daily_temp_stats_kor(test)

# # === 4) CDH / THI / WCT (train/test ë™ì¼)
# def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if 'ê¸°ì˜¨(Â°C)' not in df.columns:
#         df['CDH'] = 0.0
#         return df
#     def _cdh_1d(x):
#         cs = np.cumsum(x - 26)
#         return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
#     parts = []
#     # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ê±´ë¬¼ë³„ ì²˜ë¦¬
#     for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
#         arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
#         cdh = _cdh_1d(arr)
#         parts.append(pd.Series(cdh, index=g.index))
#     df['CDH'] = pd.concat(parts).sort_index()
#     return df

# def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
#     df = df.copy()
#     if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
#         df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
#     else:
#         df['THI'] = 0.0
#     if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
#         t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
#         df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
#     else:
#         df['WCT'] = 0.0
#     return df

# train = add_CDH_kor(train)
# test  = add_CDH_kor(test)
# train = add_THI_WCT_kor(train)
# test  = add_THI_WCT_kor(test)

# # === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     pm = (train
#           .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
#           .agg(['mean','std'])
#           .reset_index()
#           .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
#     train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
#     test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
# else:
#     train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
#     test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# # === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
# if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
#     train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# # === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìžˆì„ ë•Œë§Œ)
# if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
#     both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
#     cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
#     train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
#     test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
# # 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìžˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
# feature_candidates = [
#     # building_info ê´€ë ¨
#     'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ìž¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
#     # ê¸°ìƒ/í™˜ê²½ ë°ì´í„°
#     'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
#     # ì‹œê°„ ê¸°ë°˜
#     'hour','day','month','dayofweek','is_weekend','is_working_hours',
#     'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
#     # íŒŒìƒì§€í‘œ
#     'DI','expected_solar',
#     'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
#     'CDH','THI','WCT',
#     'day_hour_mean','day_hour_std'
# ]

# features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# # 2) target ëª…ì‹œ
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# if target not in train.columns:
#     raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# # 3) ìµœì¢… ìž…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
# X = train[features].copy()
# y = np.log1p(train[target].values)   # ì•ˆì •í™”ë¥¼ ìœ„í•´ log1p ë³€í™˜
# X_test = test[features].copy()

# print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
# print(f"[í™•ì¸] target: {target}")
# print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test.shape}, y shape: {y.shape}")

# # ------------------------------
# # ì´í•˜: Optuna ê³ ì •, ì €ìž¥/ë¡œë“œ ìœ ì§€(ì›ëž˜ ë¡œì§ ìµœëŒ€í•œ ë³´ì¡´)
# # ------------------------------

# # íŠœë‹ ëª©ì í•¨ìˆ˜ë“¤ (í‘œì¤€ ì»¬ëŸ¼ ê¸°ì¤€ yëŠ” ë¡œê·¸ ë³€í™˜ ì‚¬ìš©)
# def smape_exp(y_true_log, y_pred_log):
#     y_true = np.expm1(y_true_log)
#     y_pred = np.expm1(y_pred_log)
#     return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "early_stopping_rounds": 50,
#         "eval_metric": "mae",
#         "random_state": seed,
#         "objective": "reg:squarederror",
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     return smape_exp(y_val, pred)

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
#         "max_depth": trial.suggest_int("max_depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "subsample": trial.suggest_float("subsample", 0.6, 1.0),
#         "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
#         "random_state": seed,
#         "objective": "mae",
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     return smape_exp(y_val, pred)

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         "iterations": trial.suggest_int("iterations", 300, 1000),
#         "depth": trial.suggest_int("depth", 3, 10),
#         "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
#         "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
#         "random_seed": seed,
#         "loss_function": "MAE",
#         "verbose": 0,
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     return smape_exp(y_val, pred)

# def objective_ridge(trial, oof_tr, oof_val, y_tr, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_tr, y_tr)
#     preds = ridge.predict(oof_val)
#     return smape_exp(y_val, preds)

# def process_building_kfold(bno):
#     print(f"ðŸ¢ building {bno} KFold...")
#     param_dir = os.path.join(path, "optuna_params")
#     os.makedirs(param_dir, exist_ok=True)

#     tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
#     te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

#     X = tr_b[features].values
#     y = np.log1p(tr_b[target].values.astype(float))
#     X_test = te_b[features].values

#     kf = KFold(n_splits=7, shuffle=True, random_state=seed)
#     test_preds, val_smapes = [], []

#     for fold, (tr_idx, va_idx) in enumerate(kf.split(X), 1):
#         print(f" - fold {fold}")
#         X_tr, X_va = X[tr_idx], X[va_idx]
#         y_tr, y_va = y[tr_idx], y[va_idx]

#         sc = StandardScaler()
#         X_tr_s = sc.fit_transform(X_tr)
#         X_va_s = sc.transform(X_va)
#         X_te_s = sc.transform(X_test)

#         # ë³€ê²½
#         # X_tr_s, X_va_s, X_te_s = X_tr, X_va, X_test

#         # XGB
#         xgb_key = f"{bno}_fold{fold}_xgb"
#         xgb_path = os.path.join(param_dir, f"{xgb_key}.json")
#         if os.path.exists(xgb_path):
#             with open(xgb_path, "r") as f:
#                 xgb_params = json.load(f)
#         else:
#             st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#             st.optimize(lambda t: tune_xgb(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
#             xgb_params = st.best_params
#             with open(xgb_path, "w") as f:
#                 json.dump(xgb_params, f)
#         xgb = XGBRegressor(**xgb_params)
#         xgb.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)

#         # LGB
#         lgb_key = f"{bno}_fold{fold}_lgb"
#         lgb_path = os.path.join(param_dir, f"{lgb_key}.json")
#         if os.path.exists(lgb_path):
#             with open(lgb_path, "r") as f:
#                 lgb_params = json.load(f)
#         else:
#             st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#             st.optimize(lambda t: tune_lgb(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
#             lgb_params = st.best_params
#             with open(lgb_path, "w") as f:
#                 json.dump(lgb_params, f)
#         lgbm = LGBMRegressor(**lgb_params)
#         lgbm.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         # CAT
#         cat_key = f"{bno}_fold{fold}_cat"
#         cat_path = os.path.join(param_dir, f"{cat_key}.json")
#         if os.path.exists(cat_path):
#             with open(cat_path, "r") as f:
#                 cat_params = json.load(f)
#         else:
#             st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#             st.optimize(lambda t: tune_cat(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
#             cat_params = st.best_params
#             with open(cat_path, "w") as f:
#                 json.dump(cat_params, f)
#         cat = CatBoostRegressor(**cat_params)
#         cat.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)

#         # ìŠ¤íƒœí‚¹ì„ ìœ„í•œ oof
#         oof_tr = np.vstack([
#             xgb.predict(X_tr_s),
#             lgbm.predict(X_tr_s),
#             cat.predict(X_tr_s)
#         ]).T
#         oof_va = np.vstack([
#             xgb.predict(X_va_s),
#             lgbm.predict(X_va_s),
#             cat.predict(X_va_s)
#         ]).T
#         oof_te = np.vstack([
#             xgb.predict(X_te_s),
#             lgbm.predict(X_te_s),
#             cat.predict(X_te_s)
#         ]).T

#         # Ridge ë©”íƒ€
#         ridge_key = f"{bno}_fold{fold}_ridge"
#         ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
#         if os.path.exists(ridge_path):
#             with open(ridge_path, "r") as f:
#                 ridge_params = json.load(f)
#         else:
#             st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
#             st.optimize(lambda t: objective_ridge(t, oof_tr, oof_va, y_tr, y_va), n_trials=30)
#             ridge_params = st.best_params
#             with open(ridge_path, "w") as f:
#                 json.dump(ridge_params, f)

#         meta = Ridge(alpha=ridge_params["alpha"])
#         meta.fit(oof_tr, y_tr)

#         va_pred = meta.predict(oof_va)
#         te_pred = meta.predict(oof_te)

#         fold_smape = smape_exp(y_va, va_pred)
#         val_smapes.append(fold_smape)
#         test_preds.append(np.expm1(te_pred))  # ì—­ë¡œê·¸

#     avg_test_pred = np.mean(test_preds, axis=0)
#     avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
#     return avg_test_pred.tolist(), avg_smape


# # ==============================
# # 12) ë³‘ë ¬ ì‹¤í–‰
# # ==============================
# results = Parallel(n_jobs=-1, backend="loky")(
#     delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
# )

# final_preds, val_smapes = [], []
# for preds, sm in results:
#     final_preds.extend(preds)
#     val_smapes.append(sm)

# samplesub["answer"] = final_preds
# today = datetime.datetime.now().strftime("%Y%m%d")
# avg_smape = float(np.mean(val_smapes))
# filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")





















#2ì–˜ëŠ” ëª¨ë‘ ì˜µíŠœë‚˜ í•œê±° !! ì„±ëŠ¥ ë§¤ìš° ì¢‹ìŒ

# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# from joblib import Parallel, delayed

# warnings.filterwarnings(action='ignore')

# # Seed ê³ ì •
# seed = 4464
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
#     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
#     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#         if col in df.columns:
#             df[col] = df[col].fillna(0)
#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# train = feature_engineering(train)
# test = feature_engineering(test)
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # Optuna íŠœë‹ í•¨ìˆ˜ë“¤
# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'early_stopping_rounds': 50,
#         'eval_metric': 'mae',
#         'random_state': seed,
#         'objective': 'reg:squarederror'
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'random_state': seed,
#         'objective': 'mae'
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
#               callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'iterations': trial.suggest_int('iterations', 300, 1000),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
#         'random_seed': seed,
#         'loss_function': 'MAE',
#         'verbose': 0
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def objective(trial, oof_train, oof_val, y_train, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_train, y_train)
#     preds = ridge.predict(oof_val)
#     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def process_building(bno):
#     print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...") 
#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features]
#     y = np.log1p(train_b[target])
#     x_test = test_b[features]

#     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)
#     scaler = StandardScaler()
#     x_train_scaled = scaler.fit_transform(x_train)
#     x_val_scaled = scaler.transform(x_val)
#     x_test_scaled = scaler.transform(x_test)

#     xgb_study = optuna.create_study(direction="minimize")
#     xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_xgb = XGBRegressor(**xgb_study.best_params)
#     best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#     lgb_study = optuna.create_study(direction="minimize")
#     lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_lgb = LGBMRegressor(**lgb_study.best_params)
#     best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

#     cat_study = optuna.create_study(direction="minimize")
#     cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_cat = CatBoostRegressor(**cat_study.best_params)
#     best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

#     oof_train = np.vstack([
#         best_xgb.predict(x_train_scaled),
#         best_lgb.predict(x_train_scaled),
#         best_cat.predict(x_train_scaled)
#     ]).T
#     oof_val = np.vstack([
#         best_xgb.predict(x_val_scaled),
#         best_lgb.predict(x_val_scaled),
#         best_cat.predict(x_val_scaled)
#     ]).T
#     oof_test = np.vstack([
#         best_xgb.predict(x_test_scaled),
#         best_lgb.predict(x_test_scaled),
#         best_cat.predict(x_test_scaled)
#     ]).T

#     ridge_study = optuna.create_study(direction="minimize")
#     ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
#     meta = Ridge(alpha=ridge_study.best_params['alpha'])
#     meta.fit(oof_train, y_train)

#     val_pred = meta.predict(oof_val)
#     test_pred = meta.predict(oof_test)

#     smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))

#     return np.expm1(test_pred).tolist(), smape

# # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
# results = Parallel(n_jobs=-1, backend='loky')(
#     delayed(process_building)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# )

# # ê²°ê³¼ í•©ì¹˜ê¸°
# final_preds = []
# val_smapes = []
# for preds, smape in results:
#     final_preds.extend(preds)
#     val_smapes.append(smape)

# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# filename = f"submission_stack_optuna_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")















