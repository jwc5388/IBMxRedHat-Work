

# # #0  ------------------------------í˜„ì¬ ë² ìŠ¤íŠ¸!!!!!!!!!!!!!!!!!--------------------------------------------
# # #ëª¨ë‘ ì˜µíŠœë‚˜. + ì¼ì‚¬. (ì™”ë‹¤ê°”ë‹¤ í•˜ê³  ìˆìŒ) trials ë°”ê¿ˆ ì¶œë ¥ ì§€í‘œ ì¶”ê°€ 
# # #ì•„ë˜êº¼ì— ì¼ì‚¬ ì¶”ê°€. trials 20 ì„¤ì •ì´ í˜„ì¬ ìµœê³ . ì´ê±° ë’¤ë°”ê¾¸ê¸°


# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# from joblib import Parallel, delayed
# from sklearn.model_selection import KFold
# from sklearn.metrics import mean_absolute_error, r2_score

# warnings.filterwarnings(action='ignore')

# # Seed ê³ ì •
# seed = 707
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')



# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df, is_train=True):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = df['hour'].between(9, 18).astype(int)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    
#     if is_train:
#         for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#             df[col] = df[col].fillna(0)
#     else:
#         df['ì¼ì¡°(hr)'] = 0
#         df['ì¼ì‚¬(MJ/m2)'] = 0

#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# # ì „ì²˜ë¦¬ ì ìš©
# train = feature_engineering(train, is_train=True)
# test = feature_engineering(test, is_train=False)

# # ë©”íƒ€ ì •ë³´ ë³‘í•©
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # ë²”ì£¼í˜• ì¸ì½”ë”©
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
# for df in [train, test]:
#     df['cooling_area_ratio'] = df['ëƒ‰ë°©ë©´ì (m2)'] / df['ì—°ë©´ì (m2)']
#     df['has_pv'] = (df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] > 0).astype(int)
#     df['has_ess'] = (df['ESSì €ì¥ìš©ëŸ‰(kWh)'] > 0).astype(int)
    
    
# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 
#     'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI',
#     'cooling_area_ratio', 'has_pv', 'has_ess'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# final_preds = []
# val_smapes = []



# # # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# # for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
# #     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # # Feature Engineering
# # def feature_engineering(df):
# #     df = df.copy()
# #     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
# #     df['hour'] = df['ì¼ì‹œ'].dt.hour
# #     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
# #     df['month'] = df['ì¼ì‹œ'].dt.month
# #     df['day'] = df['ì¼ì‹œ'].dt.day
# #     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
# #     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
# #     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
# #     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
# #     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
# #         if col in df.columns:
# #             df[col] = df[col].fillna(0)
# #     temp = df['ê¸°ì˜¨(Â°C)']
# #     humidity = df['ìŠµë„(%)']
# #     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
# #     return df

# # train = feature_engineering(train)
# # test = feature_engineering(test)
# # train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# # test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # # # ğŸ’¡ [ê°œì„  ì „ëµ] 'ì¼ì‚¬ëŸ‰' ëŒ€ë¦¬ ë³€ìˆ˜ ìƒì„±
# # # # 1. í›ˆë ¨ ë°ì´í„°ì—ì„œ ì›”(month)ê³¼ ì‹œê°„(hour)ë³„ í‰ê·  ì¼ì‚¬ëŸ‰ ê³„ì‚°
# # # solar_proxy = train.groupby(['month', 'hour'])['ì¼ì‚¬(MJ/m2)'].mean().reset_index()
# # # solar_proxy.rename(columns={'ì¼ì‚¬(MJ/m2)': 'expected_solar'}, inplace=True)

# # # # 2. trainê³¼ test ë°ì´í„°ì— 'expected_solar' í”¼ì²˜ë¥¼ ë³‘í•©
# # # train = train.merge(solar_proxy, on=['month', 'hour'], how='left')
# # # test = test.merge(solar_proxy, on=['month', 'hour'], how='left')

# # # # ë³‘í•© ê³¼ì •ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì†ŒëŸ‰ì˜ ê²°ì¸¡ì¹˜ëŠ” 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
# # # train['expected_solar'] = train['expected_solar'].fillna(0)
# # # test['expected_solar'] = test['expected_solar'].fillna(0)


# # train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# # test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # features = [
# #     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
# #     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
# #     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
# #     'is_working_hours', 'sin_hour', 'cos_hour', 'DI', 
# #     # 'expected_solar'
# # ]

# # target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # Optuna íŠœë‹ í•¨ìˆ˜ë“¤
# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'early_stopping_rounds': 50,
#         'eval_metric': 'mae',
#         'random_state': seed,
#         'objective': 'reg:squarederror'
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'random_state': seed,
#         'objective': 'mae'
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
#               callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'iterations': trial.suggest_int('iterations', 300, 1000),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
#         'random_seed': seed,
#         'loss_function': 'MAE',
#         'verbose': 0
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def objective(trial, oof_train, oof_val, y_train, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_train, y_train)
#     preds = ridge.predict(oof_val)
#     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape


# def process_building_kfold(bno):
#     print(f"ğŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} KFold ì²˜ë¦¬ ì¤‘...")
#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features].values
#     y = np.log1p(train_b[target].values)
#     x_test = test_b[features].values

#     kf = KFold(n_splits=5, shuffle=True, random_state=seed)

#     test_preds = []
#     val_smapes = []
#     maes, r2s, stds = [], [], []   # âœ… ì§€í‘œ ìˆ˜ì§‘ìš© ë¦¬ìŠ¤íŠ¸

#     for fold, (train_idx, val_idx) in enumerate(kf.split(x)):
#         print(f" - Fold {fold+1}")
#         x_train, x_val = x[train_idx], x[val_idx]
#         y_train, y_val = y[train_idx], y[val_idx]

#         scaler = StandardScaler()
#         x_train_scaled = scaler.fit_transform(x_train)
#         x_val_scaled = scaler.transform(x_val)
#         x_test_scaled = scaler.transform(x_test)

#         # ëª¨ë¸ íŠœë‹ ë° í•™ìŠµ
#         xgb_study = optuna.create_study(direction="minimize")
#         xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=30)
#         best_xgb = XGBRegressor(**xgb_study.best_params)
#         best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#         lgb_study = optuna.create_study(direction="minimize")
#         lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=30)
#         best_lgb = LGBMRegressor(**lgb_study.best_params)
#         best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

#         cat_study = optuna.create_study(direction="minimize")
#         cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=30)
#         best_cat = CatBoostRegressor(**cat_study.best_params)
#         best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

#         # Stackingìš© ì˜ˆì¸¡ê°’
#         oof_train = np.vstack([
#             best_xgb.predict(x_train_scaled),
#             best_lgb.predict(x_train_scaled),
#             best_cat.predict(x_train_scaled)
#         ]).T
#         oof_val = np.vstack([
#             best_xgb.predict(x_val_scaled),
#             best_lgb.predict(x_val_scaled),
#             best_cat.predict(x_val_scaled)
#         ]).T
#         oof_test = np.vstack([
#             best_xgb.predict(x_test_scaled),
#             best_lgb.predict(x_test_scaled),
#             best_cat.predict(x_test_scaled)
#         ]).T

#         # Ridge
#         ridge_study = optuna.create_study(direction="minimize")
#         ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
#         meta = Ridge(alpha=ridge_study.best_params['alpha'])
#         meta.fit(oof_train, y_train)

#         val_pred = meta.predict(oof_val)
#         test_pred = meta.predict(oof_test)

#         smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                         (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#         mae = mean_absolute_error(np.expm1(y_val), np.expm1(val_pred))
#         r2 = r2_score(np.expm1(y_val), np.expm1(val_pred))
#         test_std = np.std(test_pred)

#         val_smapes.append(smape)
#         maes.append(mae)
#         r2s.append(r2)
#         stds.append(test_std)
#         test_preds.append(np.expm1(test_pred))

#     # í‰ê·  ê³„ì‚°
#     avg_test_pred = np.mean(test_preds, axis=0)
#     avg_smape = np.mean(val_smapes)
#     avg_mae = np.mean(maes)
#     avg_r2 = np.mean(r2s)
#     avg_test_std = np.mean(stds)

#     return avg_test_pred.tolist(), avg_smape, avg_mae, avg_r2, avg_test_std


# # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰ (KFold ì ìš©)
# results = Parallel(n_jobs=-1, backend='loky')(
#     delayed(process_building_kfold)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# )

# # ê²°ê³¼ í•©ì¹˜ê¸°
# final_preds = []
# val_smapes = []
# maes, r2s, test_stds = [], [], []

# for preds, smape, mae, r2, std in results:
#     final_preds.extend(preds)
#     val_smapes.append(smape)
#     maes.append(mae)
#     r2s.append(r2)
#     test_stds.append(std)

# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')

# # âœ… í‰ê·  ì§€í‘œ ê³„ì‚°
# avg_smape = np.mean(val_smapes)
# smape_std = np.std(val_smapes)
# avg_mae = np.mean(maes)
# mae_std = np.std(maes)
# avg_r2 = np.mean(r2s)
# r2_std = np.std(r2s)
# avg_std = np.mean(test_stds)

# # âœ… ì €ì¥
# filename = f"submission_stack_optuna_{today}_SMAPE_ì¼ì‚¬_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# # âœ… ìµœì¢… ì„±ëŠ¥ ì¶œë ¥
# print("\nğŸ“Š ìµœì¢… í‰ê°€ ì§€í‘œ ìš”ì•½")
# print(f"âœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f} Â± {smape_std:.4f}")
# print(f"ğŸ“ í‰ê·  MAE: {avg_mae:.4f} Â± {mae_std:.4f}")
# print(f"ğŸ“ í‰ê·  RÂ²: {avg_r2:.4f} Â± {r2_std:.4f}")
# print(f"ğŸ“ ì˜ˆì¸¡ê°’ í‘œì¤€í¸ì°¨ (test ê¸°ì¤€): {avg_std:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")
























# -*- coding: utf-8 -*-
# Optuna íŒŒë¼ë¯¸í„° ì €ì¥/ë¡œë“œ ìœ ì§€, í‘œì¤€ ì»¬ëŸ¼ ê¸°ë°˜ ì „ì²˜ë¦¬ ì¼ì›í™” ë²„ì „

import os
import json
import random
import warnings
import datetime
import numpy as np
import pandas as pd
import optuna

from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_absolute_error
from joblib import Parallel, delayed
from optuna.samplers import TPESampler

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf

warnings.filterwarnings("ignore")

# ==============================
# 0) ì‹œë“œ / ê²½ë¡œ
# ==============================
seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

BASE_PATH = "/workspace/TensorJae/Study25/" if os.path.exists("/workspace/TensorJae/Study25/") \
    else os.path.expanduser("~/Desktop/IBM:RedHat/Study25/")
path = os.path.join(BASE_PATH, "_data/dacon/electricity/")

buildinginfo = pd.read_csv(os.path.join(path, "building_info.csv"))
train = pd.read_csv(os.path.join(path, "train.csv"))
test = pd.read_csv(os.path.join(path, "test.csv"))
samplesub = pd.read_csv(os.path.join(path, "sample_submission.csv"))


# === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
if 'buildinginfo' in globals():
    bi = buildinginfo.copy()
else:
    bi = None

if bi is not None:
    # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìë¡œ ìºìŠ¤íŒ…
    for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
        if col in bi.columns:
            bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
    # ì„¤ë¹„ ìœ ë¬´ í”Œë˜ê·¸
    bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
    bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
    keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
    for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
        if c in bi.columns: keep_cols.append(c)
    bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

    train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # ì¼ì‹œ íŒŒì‹± (í˜•ì‹: 'YYYYMMDD HH')
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['hour']      = df['ì¼ì‹œ'].dt.hour
    df['day']       = df['ì¼ì‹œ'].dt.day
    df['month']     = df['ì¼ì‹œ'].dt.month
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend']       = (df['dayofweek'] >= 5).astype(int)
    df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
    # ì£¼ê¸° ì¸ì½”ë”©
    df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
    df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
    df['sin_month'] = np.sin(2*np.pi*df['month']/12)
    df['cos_month'] = np.cos(2*np.pi*df['month']/12)
    df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
    df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
    # DI (ìˆì„ ë•Œë§Œ)
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']
        h = df['ìŠµë„(%)']
        df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['DI'] = 0.0
    return df

train = add_time_features_kor(train)
test  = add_time_features_kor(test)

# === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
if 'ì¼ì‚¬(MJ/m2)' in train.columns:
    solar_proxy = (
        train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
             .mean().reset_index()
             .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
    )
    train = train.merge(solar_proxy, on=['month','hour'], how='left')
    test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
else:
    # trainì— ì¼ì‚¬ê°€ ì—†ìœ¼ë©´ 0
    train['expected_solar'] = 0.0
    test['expected_solar']  = 0.0

train['expected_solar'] = train['expected_solar'].fillna(0)
test['expected_solar']  = test['expected_solar'].fillna(0)

# === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
            df[c] = 0.0
        return df
    grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
    stats = grp.agg(day_max_temperature='max',
                    day_mean_temperature='mean',
                    day_min_temperature='min').reset_index()
    df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
    return df

train = add_daily_temp_stats_kor(train)
test  = add_daily_temp_stats_kor(test)

# === 4) CDH / THI / WCT (train/test ë™ì¼)
def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        df['CDH'] = 0.0
        return df
    def _cdh_1d(x):
        cs = np.cumsum(x - 26)
        return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
    parts = []
    # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ê±´ë¬¼ë³„ ì²˜ë¦¬
    for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
        arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
        cdh = _cdh_1d(arr)
        parts.append(pd.Series(cdh, index=g.index))
    df['CDH'] = pd.concat(parts).sort_index()
    return df

def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['THI'] = 0.0
    if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
        df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
    else:
        df['WCT'] = 0.0
    return df

train = add_CDH_kor(train)
test  = add_CDH_kor(test)
train = add_THI_WCT_kor(train)
test  = add_THI_WCT_kor(test)

# === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    pm = (train
          .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
          .agg(['mean','std'])
          .reset_index()
          .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
    train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
    test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
else:
    train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
    test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
    both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
    cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
    train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
    test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
# 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
feature_candidates = [
    # building_info ê´€ë ¨
    'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
    # ê¸°ìƒ/í™˜ê²½ ë°ì´í„°
    'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
    # ì‹œê°„ ê¸°ë°˜
    'hour','day','month','dayofweek','is_weekend','is_working_hours',
    'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
    # íŒŒìƒì§€í‘œ
    'DI','expected_solar',
    'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
    'CDH','THI','WCT',
    'day_hour_mean','day_hour_std'
]

features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# 2) target ëª…ì‹œ
target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
if target not in train.columns:
    raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# 3) ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
X = train[features].copy()
y = np.log1p(train[target].values)   # ì•ˆì •í™”ë¥¼ ìœ„í•´ log1p ë³€í™˜
X_test = test[features].copy()

print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
print(f"[í™•ì¸] target: {target}")
print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test.shape}, y shape: {y.shape}")

# ------------------------------
# ì´í•˜: Optuna ê³ ì •, ì €ì¥/ë¡œë“œ ìœ ì§€(ì›ë˜ ë¡œì§ ìµœëŒ€í•œ ë³´ì¡´)
# ------------------------------

# íŠœë‹ ëª©ì í•¨ìˆ˜ë“¤ (í‘œì¤€ ì»¬ëŸ¼ ê¸°ì¤€ yëŠ” ë¡œê·¸ ë³€í™˜ ì‚¬ìš©)
def smape_exp(y_true_log, y_pred_log):
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

def tune_xgb(trial, x_train, y_train, x_val, y_val):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "early_stopping_rounds": 50,
        "eval_metric": "mae",
        "random_state": seed,
        "objective": "reg:squarederror",
    }
    model = XGBRegressor(**params)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
    pred = model.predict(x_val)
    return smape_exp(y_val, pred)

def tune_lgb(trial, x_train, y_train, x_val, y_val):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 300, 1000),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.6, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
        "random_state": seed,
        "objective": "mae",
    }
    model = LGBMRegressor(**params)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])
    pred = model.predict(x_val)
    return smape_exp(y_val, pred)

def tune_cat(trial, x_train, y_train, x_val, y_val):
    params = {
        "iterations": trial.suggest_int("iterations", 300, 1000),
        "depth": trial.suggest_int("depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
        "random_seed": seed,
        "loss_function": "MAE",
        "verbose": 0,
    }
    model = CatBoostRegressor(**params)
    model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
    pred = model.predict(x_val)
    return smape_exp(y_val, pred)

def objective_ridge(trial, oof_tr, oof_val, y_tr, y_val):
    alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
    ridge = Ridge(alpha=alpha)
    ridge.fit(oof_tr, y_tr)
    preds = ridge.predict(oof_val)
    return smape_exp(y_val, preds)

def process_building_kfold(bno):
    print(f"ğŸ¢ building {bno} KFold...")
    param_dir = os.path.join(path, "optuna_params")
    os.makedirs(param_dir, exist_ok=True)

    tr_b = train[train["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()
    te_b = test[test["ê±´ë¬¼ë²ˆí˜¸"] == bno].copy()

    X = tr_b[features].values
    y = np.log1p(tr_b[target].values.astype(float))
    X_test = te_b[features].values

    kf = KFold(n_splits=7, shuffle=True, random_state=seed)
    test_preds, val_smapes = [], []

    for fold, (tr_idx, va_idx) in enumerate(kf.split(X), 1):
        print(f" - fold {fold}")
        X_tr, X_va = X[tr_idx], X[va_idx]
        y_tr, y_va = y[tr_idx], y[va_idx]

        sc = StandardScaler()
        X_tr_s = sc.fit_transform(X_tr)
        X_va_s = sc.transform(X_va)
        X_te_s = sc.transform(X_test)

        # XGB
        xgb_key = f"{bno}_fold{fold}_xgb"
        xgb_path = os.path.join(param_dir, f"{xgb_key}.json")
        if os.path.exists(xgb_path):
            with open(xgb_path, "r") as f:
                xgb_params = json.load(f)
        else:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: tune_xgb(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
            xgb_params = st.best_params
            with open(xgb_path, "w") as f:
                json.dump(xgb_params, f)
        xgb = XGBRegressor(**xgb_params)
        xgb.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], verbose=False)

        # LGB
        lgb_key = f"{bno}_fold{fold}_lgb"
        lgb_path = os.path.join(param_dir, f"{lgb_key}.json")
        if os.path.exists(lgb_path):
            with open(lgb_path, "r") as f:
                lgb_params = json.load(f)
        else:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: tune_lgb(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
            lgb_params = st.best_params
            with open(lgb_path, "w") as f:
                json.dump(lgb_params, f)
        lgbm = LGBMRegressor(**lgb_params)
        lgbm.fit(X_tr_s, y_tr, eval_set=[(X_va_s, y_va)], callbacks=[lgb.early_stopping(50, verbose=False)])

        # CAT
        cat_key = f"{bno}_fold{fold}_cat"
        cat_path = os.path.join(param_dir, f"{cat_key}.json")
        if os.path.exists(cat_path):
            with open(cat_path, "r") as f:
                cat_params = json.load(f)
        else:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: tune_cat(t, X_tr_s, y_tr, X_va_s, y_va), n_trials=20)
            cat_params = st.best_params
            with open(cat_path, "w") as f:
                json.dump(cat_params, f)
        cat = CatBoostRegressor(**cat_params)
        cat.fit(X_tr_s, y_tr, eval_set=(X_va_s, y_va), early_stopping_rounds=50, verbose=0)

        # ìŠ¤íƒœí‚¹ì„ ìœ„í•œ oof
        oof_tr = np.vstack([
            xgb.predict(X_tr_s),
            lgbm.predict(X_tr_s),
            cat.predict(X_tr_s)
        ]).T
        oof_va = np.vstack([
            xgb.predict(X_va_s),
            lgbm.predict(X_va_s),
            cat.predict(X_va_s)
        ]).T
        oof_te = np.vstack([
            xgb.predict(X_te_s),
            lgbm.predict(X_te_s),
            cat.predict(X_te_s)
        ]).T

        # Ridge ë©”íƒ€
        ridge_key = f"{bno}_fold{fold}_ridge"
        ridge_path = os.path.join(param_dir, f"{ridge_key}.json")
        if os.path.exists(ridge_path):
            with open(ridge_path, "r") as f:
                ridge_params = json.load(f)
        else:
            st = optuna.create_study(direction="minimize", sampler=TPESampler(seed=seed))
            st.optimize(lambda t: objective_ridge(t, oof_tr, oof_va, y_tr, y_va), n_trials=30)
            ridge_params = st.best_params
            with open(ridge_path, "w") as f:
                json.dump(ridge_params, f)

        meta = Ridge(alpha=ridge_params["alpha"])
        meta.fit(oof_tr, y_tr)

        va_pred = meta.predict(oof_va)
        te_pred = meta.predict(oof_te)

        fold_smape = smape_exp(y_va, va_pred)
        val_smapes.append(fold_smape)
        test_preds.append(np.expm1(te_pred))  # ì—­ë¡œê·¸

    avg_test_pred = np.mean(test_preds, axis=0)
    avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
    return avg_test_pred.tolist(), avg_smape


# ==============================
# 12) ë³‘ë ¬ ì‹¤í–‰
# ==============================
results = Parallel(n_jobs=-1, backend="loky")(
    delayed(process_building_kfold)(bno) for bno in train["ê±´ë¬¼ë²ˆí˜¸"].unique()
)

final_preds, val_smapes = [], []
for preds, sm in results:
    final_preds.extend(preds)
    val_smapes.append(sm)

samplesub["answer"] = final_preds
today = datetime.datetime.now().strftime("%Y%m%d")
avg_smape = float(np.mean(val_smapes))
filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")

























#2ì–˜ëŠ” ëª¨ë‘ ì˜µíŠœë‚˜ í•œê±° !! ì„±ëŠ¥ ë§¤ìš° ì¢‹ìŒ

# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# from joblib import Parallel, delayed

# warnings.filterwarnings(action='ignore')

# # Seed ê³ ì •
# seed = 4464
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# BASE_PATH = '/workspace/TensorJae/Study25/' if os.path.exists('/workspace/TensorJae/Study25/') else os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')
# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
#     df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
#     for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#         if col in df.columns:
#             df[col] = df[col].fillna(0)
#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# train = feature_engineering(train)
# test = feature_engineering(test)
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # Optuna íŠœë‹ í•¨ìˆ˜ë“¤
# def tune_xgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'early_stopping_rounds': 50,
#         'eval_metric': 'mae',
#         'random_state': seed,
#         'objective': 'reg:squarederror'
#     }
#     model = XGBRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_lgb(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'n_estimators': trial.suggest_int('n_estimators', 300, 1000),
#         'max_depth': trial.suggest_int('max_depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'random_state': seed,
#         'objective': 'mae'
#     }
#     model = LGBMRegressor(**params)
#     model.fit(x_train, y_train, eval_set=[(x_val, y_val)],
#               callbacks=[lgb.early_stopping(50, verbose=False)])
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def tune_cat(trial, x_train, y_train, x_val, y_val):
#     params = {
#         'iterations': trial.suggest_int('iterations', 300, 1000),
#         'depth': trial.suggest_int('depth', 3, 10),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
#         'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),
#         'random_seed': seed,
#         'loss_function': 'MAE',
#         'verbose': 0
#     }
#     model = CatBoostRegressor(**params)
#     model.fit(x_train, y_train, eval_set=(x_val, y_val), early_stopping_rounds=50, verbose=0)
#     pred = model.predict(x_val)
#     smape = np.mean(200 * np.abs(np.expm1(pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def objective(trial, oof_train, oof_val, y_train, y_val):
#     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
#     ridge = Ridge(alpha=alpha)
#     ridge.fit(oof_train, y_train)
#     preds = ridge.predict(oof_val)
#     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
#     return smape

# def process_building(bno):
#     print(f"ğŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...") 
#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features]
#     y = np.log1p(train_b[target])
#     x_test = test_b[features]

#     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)
#     scaler = StandardScaler()
#     x_train_scaled = scaler.fit_transform(x_train)
#     x_val_scaled = scaler.transform(x_val)
#     x_test_scaled = scaler.transform(x_test)

#     xgb_study = optuna.create_study(direction="minimize")
#     xgb_study.optimize(lambda trial: tune_xgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_xgb = XGBRegressor(**xgb_study.best_params)
#     best_xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#     lgb_study = optuna.create_study(direction="minimize")
#     lgb_study.optimize(lambda trial: tune_lgb(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_lgb = LGBMRegressor(**lgb_study.best_params)
#     best_lgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])

#     cat_study = optuna.create_study(direction="minimize")
#     cat_study.optimize(lambda trial: tune_cat(trial, x_train_scaled, y_train, x_val_scaled, y_val), n_trials=20)
#     best_cat = CatBoostRegressor(**cat_study.best_params)
#     best_cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50, verbose=0)

#     oof_train = np.vstack([
#         best_xgb.predict(x_train_scaled),
#         best_lgb.predict(x_train_scaled),
#         best_cat.predict(x_train_scaled)
#     ]).T
#     oof_val = np.vstack([
#         best_xgb.predict(x_val_scaled),
#         best_lgb.predict(x_val_scaled),
#         best_cat.predict(x_val_scaled)
#     ]).T
#     oof_test = np.vstack([
#         best_xgb.predict(x_test_scaled),
#         best_lgb.predict(x_test_scaled),
#         best_cat.predict(x_test_scaled)
#     ]).T

#     ridge_study = optuna.create_study(direction="minimize")
#     ridge_study.optimize(lambda trial: objective(trial, oof_train, oof_val, y_train, y_val), n_trials=30)
#     meta = Ridge(alpha=ridge_study.best_params['alpha'])
#     meta.fit(oof_train, y_train)

#     val_pred = meta.predict(oof_val)
#     test_pred = meta.predict(oof_test)

#     smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                     (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))

#     return np.expm1(test_pred).tolist(), smape

# # ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
# results = Parallel(n_jobs=-1, backend='loky')(
#     delayed(process_building)(bno) for bno in train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# )

# # ê²°ê³¼ í•©ì¹˜ê¸°
# final_preds = []
# val_smapes = []
# for preds, smape in results:
#     final_preds.extend(preds)
#     val_smapes.append(smape)

# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# filename = f"submission_stack_optuna_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")
























# # 3 ìµœì¢…ëª¨ë¸ë§Œ optuna - ëª¨ë‘ ì˜µíŠœë‚˜ í•œê²Œ ì„±ëŠ¥ ë” ì¢‹ìŒ. ì´ê±´ í˜¹ì‹œë‚˜ í•´ì„œ ê°–ê³ ìˆëŠ”



# import os
# import pandas as pd
# import numpy as np
# import random
# import datetime
# import optuna
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# from sklearn.linear_model import Ridge
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from lightgbm import LGBMRegressor
# from catboost import CatBoostRegressor
# import lightgbm as lgb
# import tensorflow as tf
# import warnings
# warnings.filterwarnings(action='ignore')


# #best 707
# # Seed ê³ ì •
# seed = 6054
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# if os.path.exists('/workspace/TensorJae/Study25/'):
#     BASE_PATH = '/workspace/TensorJae/Study25/'
# else:
#     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')


# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
# for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # Feature Engineering
# def feature_engineering(df, is_train=True):
#     df = df.copy()
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
#     df['month'] = df['ì¼ì‹œ'].dt.month
#     df['day'] = df['ì¼ì‹œ'].dt.day
#     df['is_weekend'] = (df['dayofweek'] >= 5).astype(int)
#     df['is_working_hours'] = df['hour'].between(9, 18).astype(int)
#     df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
#     df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    
#     if is_train:
#         for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
#             df[col] = df[col].fillna(0)
#     else:
#         df['ì¼ì¡°(hr)'] = 0
#         df['ì¼ì‚¬(MJ/m2)'] = 0

#     temp = df['ê¸°ì˜¨(Â°C)']
#     humidity = df['ìŠµë„(%)']
#     df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
#     return df

# # ì „ì²˜ë¦¬ ì ìš©
# train = feature_engineering(train, is_train=True)
# test = feature_engineering(test, is_train=False)

# # ë©”íƒ€ ì •ë³´ ë³‘í•©
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # ë²”ì£¼í˜• ì¸ì½”ë”©
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # íŒŒìƒ ë³€ìˆ˜ ìƒì„±
# for df in [train, test]:
#     df['cooling_area_ratio'] = df['ëƒ‰ë°©ë©´ì (m2)'] / df['ì—°ë©´ì (m2)']
#     df['has_pv'] = (df['íƒœì–‘ê´‘ìš©ëŸ‰(kW)'] > 0).astype(int)
#     df['has_ess'] = (df['ESSì €ì¥ìš©ëŸ‰(kWh)'] > 0).astype(int)
    
    
# features = [
#     'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 
#     'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
#     'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
#     'hour', 'dayofweek', 'month', 'day', 'is_weekend',
#     'is_working_hours', 'sin_hour', 'cos_hour', 'DI',
#     'cooling_area_ratio', 'has_pv', 'has_ess'
# ]

# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
# final_preds = []
# val_smapes = []

# # Optuna íŠœë‹ í•¨ìˆ˜
# # def objective(trial, oof_train, oof_val, y_train, y_val):
# #     alpha = trial.suggest_float("alpha", 1e-4, 10.0, log=True)
# #     ridge = Ridge(alpha=alpha)
# #     ridge.fit(oof_train, y_train)
# #     preds = ridge.predict(oof_val)
# #     smape = np.mean(200 * np.abs(np.expm1(preds) - np.expm1(y_val)) /
# #                     (np.abs(np.expm1(preds)) + np.abs(np.expm1(y_val)) + 1e-6))
# #     return smape

# # ê±´ë¬¼ë³„ ëª¨ë¸ í•™ìŠµ
# building_ids = train['ê±´ë¬¼ë²ˆí˜¸'].unique()
# for bno in building_ids:
#     print(f"ğŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ì²˜ë¦¬ ì¤‘...")

#     train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
#     x = train_b[features]
#     y = np.log1p(train_b[target])
#     x_test_final = test_b[features]

#     x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

#     scaler = StandardScaler()
#     x_train_scaled = scaler.fit_transform(x_train)
#     x_val_scaled = scaler.transform(x_val)
#     x_test_final_scaled = scaler.transform(x_test_final)

#     # Base ëª¨ë¸ í•™ìŠµ
#     xgb = XGBRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                        random_state=seed, early_stopping_rounds=50, objective='reg:squarederror')
#     xgb.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

#     lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                               random_state=seed, objective='mae')
#     lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
#                   callbacks=[lgb.early_stopping(50, verbose=False)])

#     cat = CatBoostRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
#                             random_seed=seed, verbose=0, loss_function='MAE')
#     cat.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

#     # ìŠ¤íƒœí‚¹
#     oof_train_lvl1 = np.vstack([
#         xgb.predict(x_train_scaled),
#         lgb_model.predict(x_train_scaled),
#         cat.predict(x_train_scaled)
#     ]).T
#     oof_val_lvl1 = np.vstack([
#         xgb.predict(x_val_scaled),
#         lgb_model.predict(x_val_scaled),
#         cat.predict(x_val_scaled)
#     ]).T
#     oof_test_lvl1 = np.vstack([
#         xgb.predict(x_test_final_scaled),
#         lgb_model.predict(x_test_final_scaled),
#         cat.predict(x_test_final_scaled)
#     ]).T

#     # Optunaë¡œ Ridge íŠœë‹
#     # study = optuna.create_study(direction="minimize")
#     # study.optimize(lambda trial: objective(trial, oof_train_lvl1, oof_val_lvl1, y_train, y_val), n_trials=30)
#     # best_alpha = study.best_params['alpha']
#     # meta_model = Ridge(alpha=best_alpha)
#     # meta_model.fit(oof_train_lvl1, y_train)
    
#     meta_model = Ridge(alpha=1.0)
#     meta_model.fit(oof_train_lvl1, y_train)
    
#     #=============================================

#     val_pred = meta_model.predict(oof_val_lvl1)
#     test_pred = meta_model.predict(oof_test_lvl1)

#     val_smape = np.mean(200 * np.abs(np.expm1(val_pred) - np.expm1(y_val)) /
#                         (np.abs(np.expm1(val_pred)) + np.abs(np.expm1(y_val)) + 1e-6))
#     val_smapes.append(val_smape)

#     pred = np.expm1(test_pred)
#     final_preds.extend(pred)

# # ê²°ê³¼ ì €ì¥
# samplesub['answer'] = final_preds
# today = datetime.datetime.now().strftime('%Y%m%d')
# avg_smape = np.mean(val_smapes)
# score_str = f"{avg_smape:.4f}".replace('.', '_')
# filename = f"submission_stack_optuna_{today}_SMAPE_{score_str}_{seed}.csv"
# samplesub.to_csv(os.path.join(path, filename), index=False)

# print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
# print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")



