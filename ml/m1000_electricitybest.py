


# First Best COULD BE GOOD BASE
# import os
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# from sklearn.model_selection import train_test_split
# from sklearn.ensemble import StackingRegressor, GradientBoostingRegressor
# from sklearn.metrics import mean_absolute_error
# from xgboost import XGBRegressor
# from catboost import CatBoostRegressor
# from lightgbm import LGBMRegressor
# import random
# import tensorflow as tf
# import datetime

# # Seed ê³ ì •
# seed = 33
# random.seed(seed)
# np.random.seed(seed)
# tf.random.set_seed(seed)

# # ê²½ë¡œ ì„¤ì •
# if os.path.exists('/workspace/TensorJae/Study25/'):
#     BASE_PATH = '/workspace/TensorJae/Study25/'
# else:
#     BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

# path = os.path.join(BASE_PATH, '_data/dacon/electricity/')
# save_path = os.path.join(path, '_save/')

# # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
# buildinginfo = pd.read_csv(path + 'building_info.csv')
# train = pd.read_csv(path + 'train.csv')
# test = pd.read_csv(path + 'test.csv')
# samplesub = pd.read_csv(path + 'sample_submission.csv')

# # ê²°ì¸¡ì¹˜ ì²˜ë¦¬: '-' â†’ 0, float ë³€í™˜
# cols_to_clean = ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']
# for col in cols_to_clean:
#     buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# # ë‚ ì§œ íŒŒì‹± ë° íŒŒìƒë³€ìˆ˜
# for df in [train, test]:
#     df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
#     df['hour'] = df['ì¼ì‹œ'].dt.hour
#     df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek

# # ê±´ë¬¼ ì •ë³´ ë³‘í•©
# train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
# test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')

# # ë²”ì£¼í˜• ì²˜ë¦¬
# train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
# test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

# # í”¼ì²˜ ì„¤ì •
# test_features = ['ê±´ë¬¼ë²ˆí˜¸', 'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)',
#                  'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)',
#                  'ìŠµë„(%)', 'hour', 'dayofweek']
# target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„í• 
# x = train[test_features]
# y = np.log1p(train[target])
# x_test_final = test[test_features]

# x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

# # ìŠ¤ì¼€ì¼ë§
# scaler = StandardScaler()
# x_train_scaled = scaler.fit_transform(x_train)
# x_val_scaled = scaler.transform(x_val)
# x_test_final_scaled = scaler.transform(x_test_final)

# # ëª¨ë¸ ì •ì˜
# xgb = XGBRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed)
# lgb = LGBMRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed)
# cat = CatBoostRegressor(n_estimators=600, learning_rate=0.1, max_depth=6, random_state=seed, verbose=0)

# stack_model = StackingRegressor(
#     estimators=[
#         ('xgb', xgb),
#         ('lgb', lgb),
#         ('cat', cat),
#     ],
#     final_estimator=GradientBoostingRegressor(n_estimators=350, random_state=seed),
#     n_jobs=-1
# )

# # í•™ìŠµ
# stack_model.fit(x_train_scaled, y_train)

# # ì˜ˆì¸¡ (ë¡œê·¸ ë³µì›)
# y_pred = np.expm1(stack_model.predict(x_val_scaled))
# y_true = np.expm1(y_val)

# # SMAPE ê³„ì‚°
# def smape(y_true, y_pred):
#     return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))

# print(f"\nâœ… ê²€ì¦ SMAPE: {smape(y_true, y_pred):.4f}")

# # ì œì¶œ
# # ì˜ˆì¸¡ ë° ë¡œê·¸ ë³µì›
# final_pred = np.expm1(stack_model.predict(x_test_final_scaled))
# samplesub['answer'] = final_pred

# # ì˜¤ëŠ˜ ë‚ ì§œ
# today = datetime.datetime.now().strftime('%Y%m%d')

# # ê²€ì¦ SMAPE ì ìˆ˜ ê³„ì‚°
# val_smape = smape(y_true, y_pred)
# score_str = f"{val_smape:.4f}".replace('.', '_')

# # íŒŒì¼ëª… ìƒì„±
# filename = f"submission_{today}_SMAPE_{score_str}.csv"
# file_path = os.path.join(path, filename)

# # ì €ìž¥
# samplesub.to_csv(file_path, index=False)
# print(f"ðŸ“ {filename} ì €ìž¥ ì™„ë£Œ!")





## Current fine working best

import os
import pandas as pd
import numpy as np
import random
import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import RidgeCV
from sklearn.metrics import mean_absolute_error

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
import lightgbm as lgb
import tensorflow as tf

# Seed ê³ ì •
seed = 43


random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

# ê²½ë¡œ ì„¤ì •
if os.path.exists('/workspace/TensorJae/Study25/'):
    BASE_PATH = '/workspace/TensorJae/Study25/'
else:
    BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

path = os.path.join(BASE_PATH, '_data/dacon/electricity/')

# ë°ì´í„° ë¡œë“œ
buildinginfo = pd.read_csv(path + 'building_info.csv')
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
samplesub = pd.read_csv(path + 'sample_submission.csv')

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)']:
    buildinginfo[col] = buildinginfo[col].replace('-', 0).astype(float)

# Feature Engineering
def feature_engineering(df):
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'])
    df['hour'] = df['ì¼ì‹œ'].dt.hour
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['month'] = df['ì¼ì‹œ'].dt.month
    df['day'] = df['ì¼ì‹œ'].dt.day
    df['is_weekend'] = df['dayofweek'].apply(lambda x: 1 if x >= 5 else 0)
    df['is_working_hours'] = df['hour'].apply(lambda x: 1 if 9 <= x <= 18 else 0)
    df['sin_hour'] = np.sin(2 * np.pi * df['hour'] / 24)
    df['cos_hour'] = np.cos(2 * np.pi * df['hour'] / 24)
    for col in ['ì¼ì¡°(hr)', 'ì¼ì‚¬(MJ/m2)']:
        if col in df.columns:
            df[col] = df[col].fillna(0)
    temp = df['ê¸°ì˜¨(Â°C)']
    humidity = df['ìŠµë„(%)']
    df['DI'] = 9/5 * temp - 0.55 * (1 - humidity/100) * (9/5 * temp - 26) + 32
    return df

# ì „ì²˜ë¦¬
train = feature_engineering(train)
test = feature_engineering(test)
train = train.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
test = test.merge(buildinginfo, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes
test['ê±´ë¬¼ìœ í˜•'] = test['ê±´ë¬¼ìœ í˜•'].astype('category').cat.codes

features = [
    'ê±´ë¬¼ìœ í˜•', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)', 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ìž¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)',
    'ê¸°ì˜¨(Â°C)', 'ê°•ìˆ˜ëŸ‰(mm)', 'í’ì†(m/s)', 'ìŠµë„(%)',
    'hour', 'dayofweek', 'month', 'day', 'is_weekend',
    'is_working_hours', 'sin_hour', 'cos_hour', 'DI'
]

target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'

# ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ ì €ìž¥ìš©
final_preds = []
val_smapes = []

# ê±´ë¬¼ë³„ë¡œ ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
building_ids = train['ê±´ë¬¼ë²ˆí˜¸'].unique()

for bno in building_ids:
    print(f"ðŸ¢ ê±´ë¬¼ë²ˆí˜¸ {bno} ëª¨ë¸ë§ ì¤‘...")

    train_b = train[train['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()
    test_b = test[test['ê±´ë¬¼ë²ˆí˜¸'] == bno].copy()

    x = train_b[features]
    y = np.log1p(train_b[target])
    x_test_final = test_b[features]

    x_train, x_val, y_train, y_val = train_test_split(x, y, train_size=0.8, random_state=seed)

    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)
    x_val_scaled = scaler.transform(x_val)
    x_test_final_scaled = scaler.transform(x_test_final)

    # Base models
    xgb_model = XGBRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                             random_state=seed, early_stopping_rounds=50, objective='reg:squarederror')
    xgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)], verbose=False)

    lgb_model = LGBMRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                              random_state=seed, objective='mae')
    lgb_model.fit(x_train_scaled, y_train, eval_set=[(x_val_scaled, y_val)],
                  callbacks=[lgb.early_stopping(50, verbose=False)])

    cat_model = CatBoostRegressor(n_estimators=700, learning_rate=0.05, max_depth=5,
                                  random_seed=seed, verbose=0, loss_function='MAE')
    cat_model.fit(x_train_scaled, y_train, eval_set=(x_val_scaled, y_val), early_stopping_rounds=50)

    # Level 1 predictions
    oof_train_lvl1 = np.vstack([
        xgb_model.predict(x_train_scaled),
        lgb_model.predict(x_train_scaled),
        cat_model.predict(x_train_scaled)
    ]).T
    oof_val_lvl1 = np.vstack([
        xgb_model.predict(x_val_scaled),
        lgb_model.predict(x_val_scaled),
        cat_model.predict(x_val_scaled)
    ]).T
    oof_test_lvl1 = np.vstack([
        xgb_model.predict(x_test_final_scaled),
        lgb_model.predict(x_test_final_scaled),
        cat_model.predict(x_test_final_scaled)
    ]).T

    # Level 2 Meta model
    meta_model = RidgeCV()
    meta_model.fit(oof_train_lvl1, y_train)
    val_pred_lvl2 = meta_model.predict(oof_val_lvl1)
    test_pred_lvl2 = meta_model.predict(oof_test_lvl1)

    # Level 3 Final model
    final_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=3, random_state=seed)
    final_model.fit(val_pred_lvl2.reshape(-1, 1), y_val)

    val_final = final_model.predict(val_pred_lvl2.reshape(-1, 1))
    val_smape = np.mean(200 * np.abs(np.expm1(val_final) - np.expm1(y_val)) /
                        (np.abs(np.expm1(val_final)) + np.abs(np.expm1(y_val)) + 1e-6))
    val_smapes.append(val_smape)

    pred = np.expm1(final_model.predict(test_pred_lvl2.reshape(-1, 1)))
    final_preds.extend(pred)

# ê²°ê³¼ ì €ìž¥
samplesub['answer'] = final_preds
today = datetime.datetime.now().strftime('%Y%m%d')
avg_smape = np.mean(val_smapes)
score_str = f"{avg_smape:.4f}".replace('.', '_')
filename = f"submission_groupwise_{today}_SMAPE_{score_str}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)
print(f"\nðŸ“ ì €ìž¥ ì™„ë£Œ â†’ {filename}")
print(f"âœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")