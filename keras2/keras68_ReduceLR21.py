import os
import numpy as np
import pandas as pd
from datetime import datetime

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

# ==============================
# 0) ê²½ë¡œ
# ==============================
if os.path.exists('/workspace/TensorJae/Study25/'):
    BASE_PATH = '/workspace/TensorJae/Study25/'
else:
    BASE_PATH = os.path.expanduser('~/Desktop/IBM:RedHat/Study25/')

DATA_PATH = os.path.join(BASE_PATH, '_data/jena/jena_climate_2009_2016.csv')
TIMESTEPS = 144      # 1ì¼(10ë¶„ ê°„ê²© x 144)
STRIDE = 1           # í•„ìš”ì‹œ 6ìœ¼ë¡œ ë°”ê¾¸ë©´ 1ì‹œê°„ ê°„ê²© ìƒ˜í”Œë§

# ==============================
# 1) ë°ì´í„° ë¡œë”© & ë‚ ì§œ íŒŒì‹± ê³ ì •
# ==============================
df = pd.read_csv(DATA_PATH)
# ì•ˆì „ íŒŒì‹±(í˜•ì‹ ê³ ì • + ì‹¤íŒ¨ í—ˆìš©)
dt_raw = df['Date Time'].astype(str).str.strip()
dt = pd.to_datetime(dt_raw, format="%d.%m.%Y %H:%M:%S", errors='coerce', dayfirst=True)
if dt.isna().any():
    # í¬ë§·ì´ ì„ì˜€ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ê´€ëŒ€ íŒŒì‹±ìœ¼ë¡œ í•œ ë²ˆ ë”
    dt2 = pd.to_datetime(dt_raw[dt.isna()], errors='coerce', dayfirst=True)
    dt.loc[dt.isna()] = dt2
df['Date Time'] = dt.dt.tz_localize(None)

# íŒŒì‹± ì‹¤íŒ¨í–‰ ì œê±°
bad = df['Date Time'].isna().sum()
if bad > 0:
    print(f"[ê²½ê³ ] Date Time íŒŒì‹± ì‹¤íŒ¨ {bad}í–‰ ë“œë")
    df = df[df['Date Time'].notna()].reset_index(drop=True)

start_pred = pd.to_datetime("31.12.2016 00:10:00", format="%d.%m.%Y %H:%M:%S", dayfirst=True)
end_pred   = pd.to_datetime("01.01.2017 00:00:00",   format="%d.%m.%Y %H:%M:%S", dayfirst=True)

# ëˆ„ìˆ˜ ë°©ì§€ ë¶„í• 
df_train_val  = df[df['Date Time'] < start_pred].copy()
df_to_predict = df[(df['Date Time'] >= start_pred) & (df['Date Time'] <= end_pred)].copy()

# ==============================
# 2) í”¼ì²˜/íƒ€ê¹ƒ ì„¤ì •
# ==============================
target_col = 'wd (deg)'
if target_col not in df_train_val.columns:
    raise KeyError(f"íƒ€ê¹ƒ ì»¬ëŸ¼ '{target_col}' ì´(ê°€) ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")

# ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ ì‚¬ìš©(ë‚ ì§œ/ë¬¸ì ìë™ ë°°ì œ)
feature_columns = df_train_val.select_dtypes(include=[np.number]).columns.tolist()
if target_col in feature_columns:
    feature_columns.remove(target_col)

# íƒ€ê¹ƒ ê°ë„ â†’ (sin, cos)
def deg_to_sin_cos(deg):
    rad = np.deg2rad(deg)
    return np.sin(rad), np.cos(rad)

df_train_val[target_col] = pd.to_numeric(df_train_val[target_col], errors='coerce')
na_tgt = df_train_val[target_col].isna().sum()
if na_tgt > 0:
    print(f"[ê²½ê³ ] íƒ€ê¹ƒ NaN {na_tgt}í–‰ ë“œë")
    df_train_val = df_train_val[df_train_val[target_col].notna()].reset_index(drop=True)

df_train_val['wd_sin'], df_train_val['wd_cos'] = deg_to_sin_cos(df_train_val[target_col])

# ==============================
# 3) ìŠ¤ì¼€ì¼ë§(ìˆ˜ì¹˜í˜•ë§Œ)
# ==============================
scaler = StandardScaler()
# ìŠ¤ì¼€ì¼ ì „ ê²°ì¸¡/ë¬´í•œì¹˜ ì²˜ë¦¬
X_num = df_train_val[feature_columns].replace([np.inf, -np.inf], np.nan)
num_na = X_num.isna().sum().sum()
if num_na > 0:
    print(f"[ê²½ê³ ] ìŠ¤ì¼€ì¼ ì „ ê²°ì¸¡ {num_na}ê°œ â†’ 0 ëŒ€ì²´")
    X_num = X_num.fillna(0.0)

X_scaled = scaler.fit_transform(X_num)
df_train_val_scaled = df_train_val.copy()
df_train_val_scaled[feature_columns] = X_scaled

# ==============================
# 4) ì‹œí€€ìŠ¤ ìƒì„±
# ==============================
def make_sequences(x_arr, y1, y2, timesteps=144, stride=1):
    X, Y = [], []
    N = len(x_arr)
    for start in range(0, N - timesteps, stride):
        end = start + timesteps
        X.append(x_arr[start:end])
        Y.append([y1[end], y2[end]])  # ë‹¤ìŒ ì‹œì  ì˜ˆì¸¡
    if len(X) == 0:
        raise ValueError("ì‹œí€€ìŠ¤ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TIMESTEPS/STRIDE/ë°ì´í„° ê¸¸ì´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    return np.asarray(X, dtype=np.float32), np.asarray(Y, dtype=np.float32)

x_data = df_train_val_scaled[feature_columns].values
y_sin  = df_train_val['wd_sin'].values
y_cos  = df_train_val['wd_cos'].values

X, Y = make_sequences(x_data, y_sin, y_cos, timesteps=TIMESTEPS, stride=STRIDE)

# ì‹œê°„ìˆœ ë¶„í• (90/10)
split_idx = int(len(X) * 0.9)
x_train, x_val = X[:split_idx], X[split_idx:]
y_train, y_val = Y[:split_idx], Y[split_idx:]

print(f"[í™•ì¸] X shape: {X.shape}, train: {x_train.shape}, val: {x_val.shape}")

# ==============================
# 5) ëª¨ë¸
# ==============================
model = Sequential([
    LSTM(64, input_shape=(x_train.shape[1], x_train.shape[2]), return_sequences=True),
    LSTM(64, return_sequences=False),
    BatchNormalization(),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(32, activation='relu'),
    Dense(2, activation='tanh')  # sin, cos
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()

es  = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
rlr = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.9, verbose=1)

model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    epochs=50, batch_size=64,
    callbacks=[es, rlr],
    verbose=1
)

# ==============================
# 6) ì˜ˆì¸¡ ì‹œí€€ìŠ¤ ì¤€ë¹„(í›ˆë ¨ ë§ˆì§€ë§‰ 144 + ì˜ˆì¸¡ êµ¬ê°„)
# ==============================
last_train = df_train_val.tail(TIMESTEPS)
concat_for_pred = pd.concat([last_train, df_to_predict], ignore_index=True)

# í”¼ì²˜ ìŠ¤ì¼€ì¼
X_pred_num = concat_for_pred[feature_columns].replace([np.inf, -np.inf], np.nan).fillna(0.0)
X_pred_scaled = scaler.transform(X_pred_num)
x_pred_data = X_pred_scaled

# ì‹œí€€ìŠ¤ ìƒì„±: ì˜ˆì¸¡ êµ¬ê°„ ê¸¸ì´ë§Œí¼ ìƒì„±
def make_pred_sequences(x_arr, timesteps=144):
    Xp = []
    for i in range(len(x_arr) - timesteps):
        Xp.append(x_arr[i:i+timesteps])
    return np.asarray(Xp, dtype=np.float32)

x_pred = make_pred_sequences(x_pred_data, TIMESTEPS)

# ==============================
# 7) ì˜ˆì¸¡ & í‰ê°€(ì›í˜• ê°ë„ ì§€í‘œ í¬í•¨)
# ==============================
y_pred_sin_cos = model.predict(x_pred, verbose=0)

def sincos_to_deg(sin_vals, cos_vals):
    radians = np.arctan2(sin_vals, cos_vals)
    degrees = np.rad2deg(radians)
    return (degrees + 360) % 360

y_pred_deg = sincos_to_deg(y_pred_sin_cos[:, 0], y_pred_sin_cos[:, 1])

y_true_deg = df_to_predict[target_col].values.astype(float)
mask = ~np.isnan(y_true_deg)
y_true_deg = y_true_deg[mask]
y_pred_deg = y_pred_deg[:len(y_true_deg)]  # ë°©ì–´ì  ìŠ¬ë¼ì´ì‹±

# ì›í˜•(360Â° ì£¼ê¸°) ê¸°ë°˜ ì˜¤ì°¨
def circular_diff(a_deg, b_deg):
    d = np.abs(a_deg - b_deg) % 360
    return np.minimum(d, 360 - d)

c_rmse = np.sqrt(np.mean(circular_diff(y_true_deg, y_pred_deg) ** 2))
c_mae  = np.mean(circular_diff(y_true_deg, y_pred_deg))

# ì¼ë°˜ RMSE/MAE(ì°¸ê³ )
rmse = np.sqrt(mean_squared_error(y_true_deg, y_pred_deg))
mae  = mean_absolute_error(y_true_deg, y_pred_deg)

print("\n" + "="*50)
print("ğŸ“Œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€")
print(f"ğŸ“ˆ Circular RMSE: {c_rmse:.4f}")
print(f"ğŸ“ˆ Circular MAE : {c_mae:.4f}")
print(f"(ref) RMSE      : {rmse:.4f}")
print(f"(ref) MAE       : {mae:.4f}")
print("="*50 + "\n")



# ğŸ“Œ ìµœì¢… ì˜ˆì¸¡ ê²°ê³¼ í‰ê°€
# ğŸ“ˆ Circular RMSE: 54.6024
# ğŸ“ˆ Circular MAE : 40.3133
# (ref) RMSE      : 61.7938
# (ref) MAE       : 42.6384